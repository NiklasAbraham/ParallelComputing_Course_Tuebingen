# Generated by Futhark 0.25.33.
# Compiled with GHC 9.10.3.
import sys
import numpy as np
import ctypes as ct
# Stub code for OpenCL setup.

import pyopencl as cl
import numpy as np
import sys

if cl.version.VERSION < (2015, 2):
    raise Exception(
        "Futhark requires at least PyOpenCL version 2015.2.  Installed version is %s."
        % cl.version.VERSION_TEXT
    )

TR_BLOCK_DIM = 16
TR_TILE_DIM = TR_BLOCK_DIM * 2
TR_ELEMS_PER_THREAD = 8


def parse_preferred_device(s):
    pref_num = 0
    if len(s) > 1 and s[0] == "#":
        i = 1
        while i < len(s):
            if not s[i].isdigit():
                break
            else:
                pref_num = pref_num * 10 + int(s[i])
            i += 1
        while i < len(s) and s[i].isspace():
            i += 1
        return (s[i:], pref_num)
    else:
        return (s, 0)


def get_prefered_context(
    interactive=False, platform_pref=None, device_pref=None
):
    if device_pref != None:
        (device_pref, device_num) = parse_preferred_device(device_pref)
    else:
        device_num = 0

    if interactive:
        return cl.create_some_context(interactive=True)

    def blacklisted(p, d):
        return (
            platform_pref == None
            and device_pref == None
            and p.name == "Apple"
            and d.name.find("Intel(R) Core(TM)") >= 0
        )

    def platform_ok(p):
        return not platform_pref or p.name.find(platform_pref) >= 0

    def device_ok(d):
        return not device_pref or d.name.find(device_pref) >= 0

    device_matches = 0

    for p in cl.get_platforms():
        if not platform_ok(p):
            continue
        for d in p.get_devices():
            if blacklisted(p, d) or not device_ok(d):
                continue
            if device_matches == device_num:
                return cl.Context(devices=[d])
            else:
                device_matches += 1
    raise Exception(
        "No OpenCL platform and device matching constraints found."
    )


def param_assignment(s):
    name, value = s.split("=")
    return (name, int(value))


def check_types(self, required_types):
    if "f64" in required_types:
        if (
            self.device.get_info(cl.device_info.PREFERRED_VECTOR_WIDTH_DOUBLE)
            == 0
        ):
            raise Exception(
                "Program uses double-precision floats, but this is not supported on chosen device: %s"
                % self.device.name
            )


def apply_size_heuristics(self, size_heuristics, sizes):
    for platform_name, device_type, size, valuef in size_heuristics:
        if (
            sizes[size] == None
            and self.platform.name.find(platform_name) >= 0
            and (self.device.type & device_type) == device_type
        ):
            sizes[size] = valuef(self.device)
    return sizes


def to_c_str_rep(x):
    if type(x) is bool or type(x) is np.bool_:
        if x:
            return "true"
        else:
            return "false"
    else:
        return str(x)


def initialise_opencl_object(
    self,
    program_src="",
    build_options=[],
    command_queue=None,
    interactive=False,
    platform_pref=None,
    device_pref=None,
    default_group_size=None,
    default_num_groups=None,
    default_tile_size=None,
    default_reg_tile_size=None,
    default_threshold=None,
    size_heuristics=[],
    required_types=[],
    all_sizes={},
    user_sizes={},
    constants=[],
):
    if command_queue is None:
        self.ctx = get_prefered_context(
            interactive, platform_pref, device_pref
        )
        self.queue = cl.CommandQueue(self.ctx)
    else:
        self.ctx = command_queue.context
        self.queue = command_queue
    self.device = self.queue.device
    self.platform = self.device.platform
    self.pool = cl.tools.MemoryPool(cl.tools.ImmediateAllocator(self.queue))
    device_type = self.device.type

    check_types(self, required_types)

    max_group_size = int(self.device.max_work_group_size)
    max_tile_size = int(np.sqrt(self.device.max_work_group_size))

    self.max_thread_block_size = max_group_size
    self.max_tile_size = max_tile_size
    self.max_threshold = 0
    self.max_grid_size = 0

    self.max_shared_memory = int(self.device.local_mem_size)

    # Futhark reserves 4 bytes of local memory for its own purposes.
    self.max_shared_memory -= 4

    # See comment in rts/c/opencl.h.
    if self.platform.name.find("NVIDIA CUDA") >= 0:
        self.max_shared_memory -= 12
    elif self.platform.name.find("AMD") >= 0:
        self.max_shared_memory -= 16

    self.max_registers = int(2**16)  # Not sure how to query for this.

    self.max_cache = self.device.get_info(cl.device_info.GLOBAL_MEM_CACHE_SIZE)

    if self.max_cache == 0:
        self.max_cache = 1024 * 1024

    self.free_list = {}

    self.global_failure = self.pool.allocate(np.int32().itemsize)
    cl.enqueue_fill_buffer(
        self.queue, self.global_failure, np.int32(-1), 0, np.int32().itemsize
    )
    self.global_failure_args = self.pool.allocate(
        np.int64().itemsize * (self.global_failure_args_max + 1)
    )
    self.failure_is_an_option = np.int32(0)

    if "default_group_size" in sizes:
        default_group_size = sizes["default_group_size"]
        del sizes["default_group_size"]

    if "default_num_groups" in sizes:
        default_num_groups = sizes["default_num_groups"]
        del sizes["default_num_groups"]

    if "default_tile_size" in sizes:
        default_tile_size = sizes["default_tile_size"]
        del sizes["default_tile_size"]

    if "default_reg_tile_size" in sizes:
        default_reg_tile_size = sizes["default_reg_tile_size"]
        del sizes["default_reg_tile_size"]

    if "default_threshold" in sizes:
        default_threshold = sizes["default_threshold"]
        del sizes["default_threshold"]

    default_group_size_set = default_group_size != None
    default_tile_size_set = default_tile_size != None
    default_sizes = apply_size_heuristics(
        self,
        size_heuristics,
        {
            "group_size": default_group_size,
            "tile_size": default_tile_size,
            "reg_tile_size": default_reg_tile_size,
            "num_groups": default_num_groups,
            "lockstep_width": None,
            "threshold": default_threshold,
        },
    )
    default_group_size = default_sizes["group_size"]
    default_num_groups = default_sizes["num_groups"]
    default_threshold = default_sizes["threshold"]
    default_tile_size = default_sizes["tile_size"]
    default_reg_tile_size = default_sizes["reg_tile_size"]
    lockstep_width = default_sizes["lockstep_width"]

    if default_group_size > max_group_size:
        if default_group_size_set:
            sys.stderr.write(
                "Note: Device limits group size to {} (down from {})\n".format(
                    max_tile_size, default_group_size
                )
            )
        default_group_size = max_group_size

    if default_tile_size > max_tile_size:
        if default_tile_size_set:
            sys.stderr.write(
                "Note: Device limits tile size to {} (down from {})\n".format(
                    max_tile_size, default_tile_size
                )
            )
        default_tile_size = max_tile_size

    for k, v in user_sizes.items():
        if k in all_sizes:
            all_sizes[k]["value"] = v
        else:
            raise Exception(
                "Unknown size: {}\nKnown sizes: {}".format(
                    k, " ".join(all_sizes.keys())
                )
            )

    self.sizes = {}
    for k, v in all_sizes.items():
        if v["class"] == "thread_block_size":
            max_value = max_group_size
            default_value = default_group_size
        elif v["class"] == "grid_size":
            max_value = max_group_size  # Intentional!
            default_value = default_num_groups
        elif v["class"] == "tile_size":
            max_value = max_tile_size
            default_value = default_tile_size
        elif v["class"] == "reg_tile_size":
            max_value = None
            default_value = default_reg_tile_size
        elif v["class"].startswith("shared_memory"):
            max_value = self.max_shared_memory
            default_value = self.max_shared_memory
        elif v["class"].startswith("cache"):
            max_value = self.max_cache
            default_value = self.max_cache
        elif v["class"].startswith("threshold"):
            max_value = None
            default_value = default_threshold
        else:
            # Bespoke sizes have no limit or default.
            max_value = None
        if v["value"] == None:
            self.sizes[k] = default_value
        elif max_value != None and v["value"] > max_value:
            sys.stderr.write(
                "Note: Device limits {} to {} (down from {}\n".format(
                    k, max_value, v["value"]
                )
            )
            self.sizes[k] = max_value
        else:
            self.sizes[k] = v["value"]

    # XXX: we perform only a subset of z-encoding here.  Really, the
    # compiler should provide us with the variables to which
    # parameters are mapped.
    if len(program_src) >= 0:
        build_options += ["-DLOCKSTEP_WIDTH={}".format(lockstep_width)]

        build_options += [
            "-D{}={}".format("max_thread_block_size", max_group_size)
        ]

        build_options += [
            "-D{}={}".format(
                s.replace("z", "zz")
                .replace(".", "zi")
                .replace("#", "zh")
                .replace("'", "zq"),
                v,
            )
            for (s, v) in self.sizes.items()
        ]

        build_options += [
            "-D{}={}".format(s, to_c_str_rep(f())) for (s, f) in constants
        ]

        if self.platform.name == "Oclgrind":
            build_options += ["-DEMULATE_F16"]

        build_options += [
            f"-DTR_BLOCK_DIM={TR_BLOCK_DIM}",
            f"-DTR_TILE_DIM={TR_TILE_DIM}",
            f"-DTR_ELEMS_PER_THREAD={TR_ELEMS_PER_THREAD}",
        ]

        program = cl.Program(self.ctx, program_src).build(build_options)

        self.transpose_kernels = {
            1: {
                "default": program.map_transpose_1b,
                "low_height": program.map_transpose_1b_low_height,
                "low_width": program.map_transpose_1b_low_width,
                "small": program.map_transpose_1b_small,
                "large": program.map_transpose_1b_large,
            },
            2: {
                "default": program.map_transpose_2b,
                "low_height": program.map_transpose_2b_low_height,
                "low_width": program.map_transpose_2b_low_width,
                "small": program.map_transpose_2b_small,
                "large": program.map_transpose_2b_large,
            },
            4: {
                "default": program.map_transpose_4b,
                "low_height": program.map_transpose_4b_low_height,
                "low_width": program.map_transpose_4b_low_width,
                "small": program.map_transpose_4b_small,
                "large": program.map_transpose_4b_large,
            },
            8: {
                "default": program.map_transpose_8b,
                "low_height": program.map_transpose_8b_low_height,
                "low_width": program.map_transpose_8b_low_width,
                "small": program.map_transpose_8b_small,
                "large": program.map_transpose_8b_large,
            },
        }

        self.copy_kernels = {
            1: program.lmad_copy_1b,
            2: program.lmad_copy_2b,
            4: program.lmad_copy_4b,
            8: program.lmad_copy_8b,
        }

        return program


def opencl_alloc(self, min_size, tag):
    min_size = 4 if min_size == 0 else min_size
    assert min_size > 0
    # Round up to a multiple of four.
    min_size = ((min_size + 3) // 4) * 4
    return self.pool.allocate(min_size)


def opencl_free_all(self):
    self.pool.free_held()


def sync(self):
    failure = np.empty(1, dtype=np.int32)
    cl.enqueue_copy(self.queue, failure, self.global_failure, is_blocking=True)
    self.failure_is_an_option = np.int32(0)
    if failure[0] >= 0:
        # Reset failure information.
        cl.enqueue_fill_buffer(
            self.queue,
            self.global_failure,
            np.int32(-1),
            0,
            np.int32().itemsize,
        )

        # Read failure args.
        failure_args = np.empty(
            self.global_failure_args_max + 1, dtype=np.int64
        )
        cl.enqueue_copy(
            self.queue,
            failure_args,
            self.global_failure_args,
            is_blocking=True,
        )

        raise Exception(self.failure_msgs[failure[0]].format(*failure_args))


def map_transpose_gpu2gpu(
    self, elem_size, dst, dst_offset, src, src_offset, k, n, m
):
    kernels = self.transpose_kernels[elem_size]
    kernel = kernels["default"]
    mulx = TR_BLOCK_DIM / n
    muly = TR_BLOCK_DIM / m

    group_dims = (TR_TILE_DIM, TR_TILE_DIM // TR_ELEMS_PER_THREAD, 1)
    dims = (
        (m + TR_TILE_DIM - 1) // TR_TILE_DIM * group_dims[0],
        (n + TR_TILE_DIM - 1) // TR_TILE_DIM * group_dims[1],
        k,
    )

    k32 = np.int32(k)
    n32 = np.int32(n)
    m32 = np.int32(m)
    mulx32 = np.int32(mulx)
    muly32 = np.int32(muly)

    kernel.set_args(
        cl.LocalMemory(TR_TILE_DIM * (TR_TILE_DIM + 1) * elem_size),
        dst,
        dst_offset,
        src,
        src_offset,
        k32,
        m32,
        n32,
        mulx32,
        muly32,
        np.int32(0),
        np.int32(0),
    )
    cl.enqueue_nd_range_kernel(self.queue, kernel, dims, group_dims)


def copy_elements_gpu2gpu(
    self,
    elem_size,
    dst,
    dst_offset,
    dst_strides,
    src,
    src_offset,
    src_strides,
    shape,
):
    r = len(shape)
    if r > 8:
        raise Exception(
            "Futhark runtime limitation:\nCannot copy array of greater than rank 8.\n"
        )

    n = np.prod(shape)
    zero = np.int64(0)
    layout_args = [None] * (8 * 3)
    for i in range(8):
        if i < r:
            layout_args[i * 3 + 0] = shape[i]
            layout_args[i * 3 + 1] = dst_strides[i]
            layout_args[i * 3 + 2] = src_strides[i]
        else:
            layout_args[i * 3 + 0] = zero
            layout_args[i * 3 + 1] = zero
            layout_args[i * 3 + 2] = zero

    kernel = self.copy_kernels[elem_size]
    kernel.set_args(
        cl.LocalMemory(1),
        dst,
        dst_offset,
        src,
        src_offset,
        n,
        np.int32(r),
        *layout_args,
    )
    w = 256
    dims = ((n + w - 1) // w * w,)
    group_dims = (w,)
    cl.enqueue_nd_range_kernel(self.queue, kernel, dims, group_dims)


def lmad_copy_gpu2gpu(
    self, pt, dst, dst_offset, dst_strides, src, src_offset, src_strides, shape
):
    elem_size = ct.sizeof(pt)
    nbytes = np.prod(shape) * elem_size
    if nbytes == 0:
        return None
    if lmad_memcpyable(dst_strides, src_strides, shape):
        cl.enqueue_copy(
            self.queue,
            dst,
            src,
            dst_offset=dst_offset * elem_size,
            src_offset=src_offset * elem_size,
            byte_count=nbytes,
        )
    else:
        tr = lmad_map_tr(dst_strides, src_strides, shape)
        if tr is not None:
            (k, n, m) = tr
            map_transpose_gpu2gpu(
                self, elem_size, dst, dst_offset, src, src_offset, k, m, n
            )
        else:
            copy_elements_gpu2gpu(
                self,
                elem_size,
                dst,
                dst_offset,
                dst_strides,
                src,
                src_offset,
                src_strides,
                shape,
            )
import pyopencl.array
import time
import argparse
sizes = {}
synchronous = False
preferred_platform = None
build_options = []
preferred_device = None
default_threshold = None
default_group_size = None
default_num_groups = None
default_tile_size = None
default_reg_tile_size = None
fut_opencl_src = """#define FUTHARK_OPENCL
#define FUTHARK_F64_ENABLED
// Start of prelude.cl

#define SCALAR_FUN_ATTR static inline
#define FUTHARK_FUN_ATTR static

typedef char int8_t;
typedef short int16_t;
typedef int int32_t;
typedef long int64_t;

typedef uchar uint8_t;
typedef ushort uint16_t;
typedef uint uint32_t;
typedef ulong uint64_t;

#define get_tblock_id(d) get_group_id(d)
#define get_num_tblocks(d) get_num_groups(d)

// Clang-based OpenCL implementations need this for 'static' to work.
#ifdef cl_clang_storage_class_specifiers
#pragma OPENCL EXTENSION cl_clang_storage_class_specifiers : enable
#endif
#pragma OPENCL EXTENSION cl_khr_byte_addressable_store : enable

#ifdef FUTHARK_F64_ENABLED
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#endif

#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
#pragma OPENCL EXTENSION cl_khr_int64_extended_atomics : enable

// NVIDIAs OpenCL does not create device-wide memory fences (see #734), so we
// use inline assembly if we detect we are on an NVIDIA GPU.
#ifdef cl_nv_pragma_unroll
static inline void mem_fence_global() {
  asm("membar.gl;");
}
#else
static inline void mem_fence_global() {
  mem_fence(CLK_LOCAL_MEM_FENCE | CLK_GLOBAL_MEM_FENCE);
}
#endif
static inline void mem_fence_local() {
  mem_fence(CLK_LOCAL_MEM_FENCE);
}

static inline void barrier_local() {
  barrier(CLK_LOCAL_MEM_FENCE);
}

// Important for this to be int64_t so it has proper alignment for any type.
#define SHARED_MEM_PARAM __local uint64_t* shared_mem,
#define FUTHARK_KERNEL __kernel
#define FUTHARK_KERNEL_SIZED(a,b,c) __attribute__((reqd_work_group_size(a, b, c))) __kernel

// End of prelude.cl
// Start of half.h.

// Conversion functions are from http://half.sourceforge.net/, but
// translated to C.
//
// Copyright (c) 2012-2021 Christian Rau
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
// THE SOFTWARE.

#ifndef __OPENCL_VERSION__
#define __constant
#endif

__constant static const uint16_t base_table[512] = {
  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000,
  0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0001, 0x0002, 0x0004, 0x0008, 0x0010, 0x0020, 0x0040, 0x0080, 0x0100,
  0x0200, 0x0400, 0x0800, 0x0C00, 0x1000, 0x1400, 0x1800, 0x1C00, 0x2000, 0x2400, 0x2800, 0x2C00, 0x3000, 0x3400, 0x3800, 0x3C00,
  0x4000, 0x4400, 0x4800, 0x4C00, 0x5000, 0x5400, 0x5800, 0x5C00, 0x6000, 0x6400, 0x6800, 0x6C00, 0x7000, 0x7400, 0x7800, 0x7C00,
  0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00,
  0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00,
  0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00,
  0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00,
  0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00,
  0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00,
  0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00, 0x7C00,
  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000,
  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000,
  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000,
  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000,
  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000,
  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000,
  0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8000, 0x8001, 0x8002, 0x8004, 0x8008, 0x8010, 0x8020, 0x8040, 0x8080, 0x8100,
  0x8200, 0x8400, 0x8800, 0x8C00, 0x9000, 0x9400, 0x9800, 0x9C00, 0xA000, 0xA400, 0xA800, 0xAC00, 0xB000, 0xB400, 0xB800, 0xBC00,
  0xC000, 0xC400, 0xC800, 0xCC00, 0xD000, 0xD400, 0xD800, 0xDC00, 0xE000, 0xE400, 0xE800, 0xEC00, 0xF000, 0xF400, 0xF800, 0xFC00,
  0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00,
  0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00,
  0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00,
  0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00,
  0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00,
  0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00,
  0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00, 0xFC00 };

__constant static const unsigned char shift_table[512] = {
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,
  13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 13,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,
  13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 13 };

__constant static const uint32_t mantissa_table[2048] = {
  0x00000000, 0x33800000, 0x34000000, 0x34400000, 0x34800000, 0x34A00000, 0x34C00000, 0x34E00000, 0x35000000, 0x35100000, 0x35200000, 0x35300000, 0x35400000, 0x35500000, 0x35600000, 0x35700000,
  0x35800000, 0x35880000, 0x35900000, 0x35980000, 0x35A00000, 0x35A80000, 0x35B00000, 0x35B80000, 0x35C00000, 0x35C80000, 0x35D00000, 0x35D80000, 0x35E00000, 0x35E80000, 0x35F00000, 0x35F80000,
  0x36000000, 0x36040000, 0x36080000, 0x360C0000, 0x36100000, 0x36140000, 0x36180000, 0x361C0000, 0x36200000, 0x36240000, 0x36280000, 0x362C0000, 0x36300000, 0x36340000, 0x36380000, 0x363C0000,
  0x36400000, 0x36440000, 0x36480000, 0x364C0000, 0x36500000, 0x36540000, 0x36580000, 0x365C0000, 0x36600000, 0x36640000, 0x36680000, 0x366C0000, 0x36700000, 0x36740000, 0x36780000, 0x367C0000,
  0x36800000, 0x36820000, 0x36840000, 0x36860000, 0x36880000, 0x368A0000, 0x368C0000, 0x368E0000, 0x36900000, 0x36920000, 0x36940000, 0x36960000, 0x36980000, 0x369A0000, 0x369C0000, 0x369E0000,
  0x36A00000, 0x36A20000, 0x36A40000, 0x36A60000, 0x36A80000, 0x36AA0000, 0x36AC0000, 0x36AE0000, 0x36B00000, 0x36B20000, 0x36B40000, 0x36B60000, 0x36B80000, 0x36BA0000, 0x36BC0000, 0x36BE0000,
  0x36C00000, 0x36C20000, 0x36C40000, 0x36C60000, 0x36C80000, 0x36CA0000, 0x36CC0000, 0x36CE0000, 0x36D00000, 0x36D20000, 0x36D40000, 0x36D60000, 0x36D80000, 0x36DA0000, 0x36DC0000, 0x36DE0000,
  0x36E00000, 0x36E20000, 0x36E40000, 0x36E60000, 0x36E80000, 0x36EA0000, 0x36EC0000, 0x36EE0000, 0x36F00000, 0x36F20000, 0x36F40000, 0x36F60000, 0x36F80000, 0x36FA0000, 0x36FC0000, 0x36FE0000,
  0x37000000, 0x37010000, 0x37020000, 0x37030000, 0x37040000, 0x37050000, 0x37060000, 0x37070000, 0x37080000, 0x37090000, 0x370A0000, 0x370B0000, 0x370C0000, 0x370D0000, 0x370E0000, 0x370F0000,
  0x37100000, 0x37110000, 0x37120000, 0x37130000, 0x37140000, 0x37150000, 0x37160000, 0x37170000, 0x37180000, 0x37190000, 0x371A0000, 0x371B0000, 0x371C0000, 0x371D0000, 0x371E0000, 0x371F0000,
  0x37200000, 0x37210000, 0x37220000, 0x37230000, 0x37240000, 0x37250000, 0x37260000, 0x37270000, 0x37280000, 0x37290000, 0x372A0000, 0x372B0000, 0x372C0000, 0x372D0000, 0x372E0000, 0x372F0000,
  0x37300000, 0x37310000, 0x37320000, 0x37330000, 0x37340000, 0x37350000, 0x37360000, 0x37370000, 0x37380000, 0x37390000, 0x373A0000, 0x373B0000, 0x373C0000, 0x373D0000, 0x373E0000, 0x373F0000,
  0x37400000, 0x37410000, 0x37420000, 0x37430000, 0x37440000, 0x37450000, 0x37460000, 0x37470000, 0x37480000, 0x37490000, 0x374A0000, 0x374B0000, 0x374C0000, 0x374D0000, 0x374E0000, 0x374F0000,
  0x37500000, 0x37510000, 0x37520000, 0x37530000, 0x37540000, 0x37550000, 0x37560000, 0x37570000, 0x37580000, 0x37590000, 0x375A0000, 0x375B0000, 0x375C0000, 0x375D0000, 0x375E0000, 0x375F0000,
  0x37600000, 0x37610000, 0x37620000, 0x37630000, 0x37640000, 0x37650000, 0x37660000, 0x37670000, 0x37680000, 0x37690000, 0x376A0000, 0x376B0000, 0x376C0000, 0x376D0000, 0x376E0000, 0x376F0000,
  0x37700000, 0x37710000, 0x37720000, 0x37730000, 0x37740000, 0x37750000, 0x37760000, 0x37770000, 0x37780000, 0x37790000, 0x377A0000, 0x377B0000, 0x377C0000, 0x377D0000, 0x377E0000, 0x377F0000,
  0x37800000, 0x37808000, 0x37810000, 0x37818000, 0x37820000, 0x37828000, 0x37830000, 0x37838000, 0x37840000, 0x37848000, 0x37850000, 0x37858000, 0x37860000, 0x37868000, 0x37870000, 0x37878000,
  0x37880000, 0x37888000, 0x37890000, 0x37898000, 0x378A0000, 0x378A8000, 0x378B0000, 0x378B8000, 0x378C0000, 0x378C8000, 0x378D0000, 0x378D8000, 0x378E0000, 0x378E8000, 0x378F0000, 0x378F8000,
  0x37900000, 0x37908000, 0x37910000, 0x37918000, 0x37920000, 0x37928000, 0x37930000, 0x37938000, 0x37940000, 0x37948000, 0x37950000, 0x37958000, 0x37960000, 0x37968000, 0x37970000, 0x37978000,
  0x37980000, 0x37988000, 0x37990000, 0x37998000, 0x379A0000, 0x379A8000, 0x379B0000, 0x379B8000, 0x379C0000, 0x379C8000, 0x379D0000, 0x379D8000, 0x379E0000, 0x379E8000, 0x379F0000, 0x379F8000,
  0x37A00000, 0x37A08000, 0x37A10000, 0x37A18000, 0x37A20000, 0x37A28000, 0x37A30000, 0x37A38000, 0x37A40000, 0x37A48000, 0x37A50000, 0x37A58000, 0x37A60000, 0x37A68000, 0x37A70000, 0x37A78000,
  0x37A80000, 0x37A88000, 0x37A90000, 0x37A98000, 0x37AA0000, 0x37AA8000, 0x37AB0000, 0x37AB8000, 0x37AC0000, 0x37AC8000, 0x37AD0000, 0x37AD8000, 0x37AE0000, 0x37AE8000, 0x37AF0000, 0x37AF8000,
  0x37B00000, 0x37B08000, 0x37B10000, 0x37B18000, 0x37B20000, 0x37B28000, 0x37B30000, 0x37B38000, 0x37B40000, 0x37B48000, 0x37B50000, 0x37B58000, 0x37B60000, 0x37B68000, 0x37B70000, 0x37B78000,
  0x37B80000, 0x37B88000, 0x37B90000, 0x37B98000, 0x37BA0000, 0x37BA8000, 0x37BB0000, 0x37BB8000, 0x37BC0000, 0x37BC8000, 0x37BD0000, 0x37BD8000, 0x37BE0000, 0x37BE8000, 0x37BF0000, 0x37BF8000,
  0x37C00000, 0x37C08000, 0x37C10000, 0x37C18000, 0x37C20000, 0x37C28000, 0x37C30000, 0x37C38000, 0x37C40000, 0x37C48000, 0x37C50000, 0x37C58000, 0x37C60000, 0x37C68000, 0x37C70000, 0x37C78000,
  0x37C80000, 0x37C88000, 0x37C90000, 0x37C98000, 0x37CA0000, 0x37CA8000, 0x37CB0000, 0x37CB8000, 0x37CC0000, 0x37CC8000, 0x37CD0000, 0x37CD8000, 0x37CE0000, 0x37CE8000, 0x37CF0000, 0x37CF8000,
  0x37D00000, 0x37D08000, 0x37D10000, 0x37D18000, 0x37D20000, 0x37D28000, 0x37D30000, 0x37D38000, 0x37D40000, 0x37D48000, 0x37D50000, 0x37D58000, 0x37D60000, 0x37D68000, 0x37D70000, 0x37D78000,
  0x37D80000, 0x37D88000, 0x37D90000, 0x37D98000, 0x37DA0000, 0x37DA8000, 0x37DB0000, 0x37DB8000, 0x37DC0000, 0x37DC8000, 0x37DD0000, 0x37DD8000, 0x37DE0000, 0x37DE8000, 0x37DF0000, 0x37DF8000,
  0x37E00000, 0x37E08000, 0x37E10000, 0x37E18000, 0x37E20000, 0x37E28000, 0x37E30000, 0x37E38000, 0x37E40000, 0x37E48000, 0x37E50000, 0x37E58000, 0x37E60000, 0x37E68000, 0x37E70000, 0x37E78000,
  0x37E80000, 0x37E88000, 0x37E90000, 0x37E98000, 0x37EA0000, 0x37EA8000, 0x37EB0000, 0x37EB8000, 0x37EC0000, 0x37EC8000, 0x37ED0000, 0x37ED8000, 0x37EE0000, 0x37EE8000, 0x37EF0000, 0x37EF8000,
  0x37F00000, 0x37F08000, 0x37F10000, 0x37F18000, 0x37F20000, 0x37F28000, 0x37F30000, 0x37F38000, 0x37F40000, 0x37F48000, 0x37F50000, 0x37F58000, 0x37F60000, 0x37F68000, 0x37F70000, 0x37F78000,
  0x37F80000, 0x37F88000, 0x37F90000, 0x37F98000, 0x37FA0000, 0x37FA8000, 0x37FB0000, 0x37FB8000, 0x37FC0000, 0x37FC8000, 0x37FD0000, 0x37FD8000, 0x37FE0000, 0x37FE8000, 0x37FF0000, 0x37FF8000,
  0x38000000, 0x38004000, 0x38008000, 0x3800C000, 0x38010000, 0x38014000, 0x38018000, 0x3801C000, 0x38020000, 0x38024000, 0x38028000, 0x3802C000, 0x38030000, 0x38034000, 0x38038000, 0x3803C000,
  0x38040000, 0x38044000, 0x38048000, 0x3804C000, 0x38050000, 0x38054000, 0x38058000, 0x3805C000, 0x38060000, 0x38064000, 0x38068000, 0x3806C000, 0x38070000, 0x38074000, 0x38078000, 0x3807C000,
  0x38080000, 0x38084000, 0x38088000, 0x3808C000, 0x38090000, 0x38094000, 0x38098000, 0x3809C000, 0x380A0000, 0x380A4000, 0x380A8000, 0x380AC000, 0x380B0000, 0x380B4000, 0x380B8000, 0x380BC000,
  0x380C0000, 0x380C4000, 0x380C8000, 0x380CC000, 0x380D0000, 0x380D4000, 0x380D8000, 0x380DC000, 0x380E0000, 0x380E4000, 0x380E8000, 0x380EC000, 0x380F0000, 0x380F4000, 0x380F8000, 0x380FC000,
  0x38100000, 0x38104000, 0x38108000, 0x3810C000, 0x38110000, 0x38114000, 0x38118000, 0x3811C000, 0x38120000, 0x38124000, 0x38128000, 0x3812C000, 0x38130000, 0x38134000, 0x38138000, 0x3813C000,
  0x38140000, 0x38144000, 0x38148000, 0x3814C000, 0x38150000, 0x38154000, 0x38158000, 0x3815C000, 0x38160000, 0x38164000, 0x38168000, 0x3816C000, 0x38170000, 0x38174000, 0x38178000, 0x3817C000,
  0x38180000, 0x38184000, 0x38188000, 0x3818C000, 0x38190000, 0x38194000, 0x38198000, 0x3819C000, 0x381A0000, 0x381A4000, 0x381A8000, 0x381AC000, 0x381B0000, 0x381B4000, 0x381B8000, 0x381BC000,
  0x381C0000, 0x381C4000, 0x381C8000, 0x381CC000, 0x381D0000, 0x381D4000, 0x381D8000, 0x381DC000, 0x381E0000, 0x381E4000, 0x381E8000, 0x381EC000, 0x381F0000, 0x381F4000, 0x381F8000, 0x381FC000,
  0x38200000, 0x38204000, 0x38208000, 0x3820C000, 0x38210000, 0x38214000, 0x38218000, 0x3821C000, 0x38220000, 0x38224000, 0x38228000, 0x3822C000, 0x38230000, 0x38234000, 0x38238000, 0x3823C000,
  0x38240000, 0x38244000, 0x38248000, 0x3824C000, 0x38250000, 0x38254000, 0x38258000, 0x3825C000, 0x38260000, 0x38264000, 0x38268000, 0x3826C000, 0x38270000, 0x38274000, 0x38278000, 0x3827C000,
  0x38280000, 0x38284000, 0x38288000, 0x3828C000, 0x38290000, 0x38294000, 0x38298000, 0x3829C000, 0x382A0000, 0x382A4000, 0x382A8000, 0x382AC000, 0x382B0000, 0x382B4000, 0x382B8000, 0x382BC000,
  0x382C0000, 0x382C4000, 0x382C8000, 0x382CC000, 0x382D0000, 0x382D4000, 0x382D8000, 0x382DC000, 0x382E0000, 0x382E4000, 0x382E8000, 0x382EC000, 0x382F0000, 0x382F4000, 0x382F8000, 0x382FC000,
  0x38300000, 0x38304000, 0x38308000, 0x3830C000, 0x38310000, 0x38314000, 0x38318000, 0x3831C000, 0x38320000, 0x38324000, 0x38328000, 0x3832C000, 0x38330000, 0x38334000, 0x38338000, 0x3833C000,
  0x38340000, 0x38344000, 0x38348000, 0x3834C000, 0x38350000, 0x38354000, 0x38358000, 0x3835C000, 0x38360000, 0x38364000, 0x38368000, 0x3836C000, 0x38370000, 0x38374000, 0x38378000, 0x3837C000,
  0x38380000, 0x38384000, 0x38388000, 0x3838C000, 0x38390000, 0x38394000, 0x38398000, 0x3839C000, 0x383A0000, 0x383A4000, 0x383A8000, 0x383AC000, 0x383B0000, 0x383B4000, 0x383B8000, 0x383BC000,
  0x383C0000, 0x383C4000, 0x383C8000, 0x383CC000, 0x383D0000, 0x383D4000, 0x383D8000, 0x383DC000, 0x383E0000, 0x383E4000, 0x383E8000, 0x383EC000, 0x383F0000, 0x383F4000, 0x383F8000, 0x383FC000,
  0x38400000, 0x38404000, 0x38408000, 0x3840C000, 0x38410000, 0x38414000, 0x38418000, 0x3841C000, 0x38420000, 0x38424000, 0x38428000, 0x3842C000, 0x38430000, 0x38434000, 0x38438000, 0x3843C000,
  0x38440000, 0x38444000, 0x38448000, 0x3844C000, 0x38450000, 0x38454000, 0x38458000, 0x3845C000, 0x38460000, 0x38464000, 0x38468000, 0x3846C000, 0x38470000, 0x38474000, 0x38478000, 0x3847C000,
  0x38480000, 0x38484000, 0x38488000, 0x3848C000, 0x38490000, 0x38494000, 0x38498000, 0x3849C000, 0x384A0000, 0x384A4000, 0x384A8000, 0x384AC000, 0x384B0000, 0x384B4000, 0x384B8000, 0x384BC000,
  0x384C0000, 0x384C4000, 0x384C8000, 0x384CC000, 0x384D0000, 0x384D4000, 0x384D8000, 0x384DC000, 0x384E0000, 0x384E4000, 0x384E8000, 0x384EC000, 0x384F0000, 0x384F4000, 0x384F8000, 0x384FC000,
  0x38500000, 0x38504000, 0x38508000, 0x3850C000, 0x38510000, 0x38514000, 0x38518000, 0x3851C000, 0x38520000, 0x38524000, 0x38528000, 0x3852C000, 0x38530000, 0x38534000, 0x38538000, 0x3853C000,
  0x38540000, 0x38544000, 0x38548000, 0x3854C000, 0x38550000, 0x38554000, 0x38558000, 0x3855C000, 0x38560000, 0x38564000, 0x38568000, 0x3856C000, 0x38570000, 0x38574000, 0x38578000, 0x3857C000,
  0x38580000, 0x38584000, 0x38588000, 0x3858C000, 0x38590000, 0x38594000, 0x38598000, 0x3859C000, 0x385A0000, 0x385A4000, 0x385A8000, 0x385AC000, 0x385B0000, 0x385B4000, 0x385B8000, 0x385BC000,
  0x385C0000, 0x385C4000, 0x385C8000, 0x385CC000, 0x385D0000, 0x385D4000, 0x385D8000, 0x385DC000, 0x385E0000, 0x385E4000, 0x385E8000, 0x385EC000, 0x385F0000, 0x385F4000, 0x385F8000, 0x385FC000,
  0x38600000, 0x38604000, 0x38608000, 0x3860C000, 0x38610000, 0x38614000, 0x38618000, 0x3861C000, 0x38620000, 0x38624000, 0x38628000, 0x3862C000, 0x38630000, 0x38634000, 0x38638000, 0x3863C000,
  0x38640000, 0x38644000, 0x38648000, 0x3864C000, 0x38650000, 0x38654000, 0x38658000, 0x3865C000, 0x38660000, 0x38664000, 0x38668000, 0x3866C000, 0x38670000, 0x38674000, 0x38678000, 0x3867C000,
  0x38680000, 0x38684000, 0x38688000, 0x3868C000, 0x38690000, 0x38694000, 0x38698000, 0x3869C000, 0x386A0000, 0x386A4000, 0x386A8000, 0x386AC000, 0x386B0000, 0x386B4000, 0x386B8000, 0x386BC000,
  0x386C0000, 0x386C4000, 0x386C8000, 0x386CC000, 0x386D0000, 0x386D4000, 0x386D8000, 0x386DC000, 0x386E0000, 0x386E4000, 0x386E8000, 0x386EC000, 0x386F0000, 0x386F4000, 0x386F8000, 0x386FC000,
  0x38700000, 0x38704000, 0x38708000, 0x3870C000, 0x38710000, 0x38714000, 0x38718000, 0x3871C000, 0x38720000, 0x38724000, 0x38728000, 0x3872C000, 0x38730000, 0x38734000, 0x38738000, 0x3873C000,
  0x38740000, 0x38744000, 0x38748000, 0x3874C000, 0x38750000, 0x38754000, 0x38758000, 0x3875C000, 0x38760000, 0x38764000, 0x38768000, 0x3876C000, 0x38770000, 0x38774000, 0x38778000, 0x3877C000,
  0x38780000, 0x38784000, 0x38788000, 0x3878C000, 0x38790000, 0x38794000, 0x38798000, 0x3879C000, 0x387A0000, 0x387A4000, 0x387A8000, 0x387AC000, 0x387B0000, 0x387B4000, 0x387B8000, 0x387BC000,
  0x387C0000, 0x387C4000, 0x387C8000, 0x387CC000, 0x387D0000, 0x387D4000, 0x387D8000, 0x387DC000, 0x387E0000, 0x387E4000, 0x387E8000, 0x387EC000, 0x387F0000, 0x387F4000, 0x387F8000, 0x387FC000,
  0x38000000, 0x38002000, 0x38004000, 0x38006000, 0x38008000, 0x3800A000, 0x3800C000, 0x3800E000, 0x38010000, 0x38012000, 0x38014000, 0x38016000, 0x38018000, 0x3801A000, 0x3801C000, 0x3801E000,
  0x38020000, 0x38022000, 0x38024000, 0x38026000, 0x38028000, 0x3802A000, 0x3802C000, 0x3802E000, 0x38030000, 0x38032000, 0x38034000, 0x38036000, 0x38038000, 0x3803A000, 0x3803C000, 0x3803E000,
  0x38040000, 0x38042000, 0x38044000, 0x38046000, 0x38048000, 0x3804A000, 0x3804C000, 0x3804E000, 0x38050000, 0x38052000, 0x38054000, 0x38056000, 0x38058000, 0x3805A000, 0x3805C000, 0x3805E000,
  0x38060000, 0x38062000, 0x38064000, 0x38066000, 0x38068000, 0x3806A000, 0x3806C000, 0x3806E000, 0x38070000, 0x38072000, 0x38074000, 0x38076000, 0x38078000, 0x3807A000, 0x3807C000, 0x3807E000,
  0x38080000, 0x38082000, 0x38084000, 0x38086000, 0x38088000, 0x3808A000, 0x3808C000, 0x3808E000, 0x38090000, 0x38092000, 0x38094000, 0x38096000, 0x38098000, 0x3809A000, 0x3809C000, 0x3809E000,
  0x380A0000, 0x380A2000, 0x380A4000, 0x380A6000, 0x380A8000, 0x380AA000, 0x380AC000, 0x380AE000, 0x380B0000, 0x380B2000, 0x380B4000, 0x380B6000, 0x380B8000, 0x380BA000, 0x380BC000, 0x380BE000,
  0x380C0000, 0x380C2000, 0x380C4000, 0x380C6000, 0x380C8000, 0x380CA000, 0x380CC000, 0x380CE000, 0x380D0000, 0x380D2000, 0x380D4000, 0x380D6000, 0x380D8000, 0x380DA000, 0x380DC000, 0x380DE000,
  0x380E0000, 0x380E2000, 0x380E4000, 0x380E6000, 0x380E8000, 0x380EA000, 0x380EC000, 0x380EE000, 0x380F0000, 0x380F2000, 0x380F4000, 0x380F6000, 0x380F8000, 0x380FA000, 0x380FC000, 0x380FE000,
  0x38100000, 0x38102000, 0x38104000, 0x38106000, 0x38108000, 0x3810A000, 0x3810C000, 0x3810E000, 0x38110000, 0x38112000, 0x38114000, 0x38116000, 0x38118000, 0x3811A000, 0x3811C000, 0x3811E000,
  0x38120000, 0x38122000, 0x38124000, 0x38126000, 0x38128000, 0x3812A000, 0x3812C000, 0x3812E000, 0x38130000, 0x38132000, 0x38134000, 0x38136000, 0x38138000, 0x3813A000, 0x3813C000, 0x3813E000,
  0x38140000, 0x38142000, 0x38144000, 0x38146000, 0x38148000, 0x3814A000, 0x3814C000, 0x3814E000, 0x38150000, 0x38152000, 0x38154000, 0x38156000, 0x38158000, 0x3815A000, 0x3815C000, 0x3815E000,
  0x38160000, 0x38162000, 0x38164000, 0x38166000, 0x38168000, 0x3816A000, 0x3816C000, 0x3816E000, 0x38170000, 0x38172000, 0x38174000, 0x38176000, 0x38178000, 0x3817A000, 0x3817C000, 0x3817E000,
  0x38180000, 0x38182000, 0x38184000, 0x38186000, 0x38188000, 0x3818A000, 0x3818C000, 0x3818E000, 0x38190000, 0x38192000, 0x38194000, 0x38196000, 0x38198000, 0x3819A000, 0x3819C000, 0x3819E000,
  0x381A0000, 0x381A2000, 0x381A4000, 0x381A6000, 0x381A8000, 0x381AA000, 0x381AC000, 0x381AE000, 0x381B0000, 0x381B2000, 0x381B4000, 0x381B6000, 0x381B8000, 0x381BA000, 0x381BC000, 0x381BE000,
  0x381C0000, 0x381C2000, 0x381C4000, 0x381C6000, 0x381C8000, 0x381CA000, 0x381CC000, 0x381CE000, 0x381D0000, 0x381D2000, 0x381D4000, 0x381D6000, 0x381D8000, 0x381DA000, 0x381DC000, 0x381DE000,
  0x381E0000, 0x381E2000, 0x381E4000, 0x381E6000, 0x381E8000, 0x381EA000, 0x381EC000, 0x381EE000, 0x381F0000, 0x381F2000, 0x381F4000, 0x381F6000, 0x381F8000, 0x381FA000, 0x381FC000, 0x381FE000,
  0x38200000, 0x38202000, 0x38204000, 0x38206000, 0x38208000, 0x3820A000, 0x3820C000, 0x3820E000, 0x38210000, 0x38212000, 0x38214000, 0x38216000, 0x38218000, 0x3821A000, 0x3821C000, 0x3821E000,
  0x38220000, 0x38222000, 0x38224000, 0x38226000, 0x38228000, 0x3822A000, 0x3822C000, 0x3822E000, 0x38230000, 0x38232000, 0x38234000, 0x38236000, 0x38238000, 0x3823A000, 0x3823C000, 0x3823E000,
  0x38240000, 0x38242000, 0x38244000, 0x38246000, 0x38248000, 0x3824A000, 0x3824C000, 0x3824E000, 0x38250000, 0x38252000, 0x38254000, 0x38256000, 0x38258000, 0x3825A000, 0x3825C000, 0x3825E000,
  0x38260000, 0x38262000, 0x38264000, 0x38266000, 0x38268000, 0x3826A000, 0x3826C000, 0x3826E000, 0x38270000, 0x38272000, 0x38274000, 0x38276000, 0x38278000, 0x3827A000, 0x3827C000, 0x3827E000,
  0x38280000, 0x38282000, 0x38284000, 0x38286000, 0x38288000, 0x3828A000, 0x3828C000, 0x3828E000, 0x38290000, 0x38292000, 0x38294000, 0x38296000, 0x38298000, 0x3829A000, 0x3829C000, 0x3829E000,
  0x382A0000, 0x382A2000, 0x382A4000, 0x382A6000, 0x382A8000, 0x382AA000, 0x382AC000, 0x382AE000, 0x382B0000, 0x382B2000, 0x382B4000, 0x382B6000, 0x382B8000, 0x382BA000, 0x382BC000, 0x382BE000,
  0x382C0000, 0x382C2000, 0x382C4000, 0x382C6000, 0x382C8000, 0x382CA000, 0x382CC000, 0x382CE000, 0x382D0000, 0x382D2000, 0x382D4000, 0x382D6000, 0x382D8000, 0x382DA000, 0x382DC000, 0x382DE000,
  0x382E0000, 0x382E2000, 0x382E4000, 0x382E6000, 0x382E8000, 0x382EA000, 0x382EC000, 0x382EE000, 0x382F0000, 0x382F2000, 0x382F4000, 0x382F6000, 0x382F8000, 0x382FA000, 0x382FC000, 0x382FE000,
  0x38300000, 0x38302000, 0x38304000, 0x38306000, 0x38308000, 0x3830A000, 0x3830C000, 0x3830E000, 0x38310000, 0x38312000, 0x38314000, 0x38316000, 0x38318000, 0x3831A000, 0x3831C000, 0x3831E000,
  0x38320000, 0x38322000, 0x38324000, 0x38326000, 0x38328000, 0x3832A000, 0x3832C000, 0x3832E000, 0x38330000, 0x38332000, 0x38334000, 0x38336000, 0x38338000, 0x3833A000, 0x3833C000, 0x3833E000,
  0x38340000, 0x38342000, 0x38344000, 0x38346000, 0x38348000, 0x3834A000, 0x3834C000, 0x3834E000, 0x38350000, 0x38352000, 0x38354000, 0x38356000, 0x38358000, 0x3835A000, 0x3835C000, 0x3835E000,
  0x38360000, 0x38362000, 0x38364000, 0x38366000, 0x38368000, 0x3836A000, 0x3836C000, 0x3836E000, 0x38370000, 0x38372000, 0x38374000, 0x38376000, 0x38378000, 0x3837A000, 0x3837C000, 0x3837E000,
  0x38380000, 0x38382000, 0x38384000, 0x38386000, 0x38388000, 0x3838A000, 0x3838C000, 0x3838E000, 0x38390000, 0x38392000, 0x38394000, 0x38396000, 0x38398000, 0x3839A000, 0x3839C000, 0x3839E000,
  0x383A0000, 0x383A2000, 0x383A4000, 0x383A6000, 0x383A8000, 0x383AA000, 0x383AC000, 0x383AE000, 0x383B0000, 0x383B2000, 0x383B4000, 0x383B6000, 0x383B8000, 0x383BA000, 0x383BC000, 0x383BE000,
  0x383C0000, 0x383C2000, 0x383C4000, 0x383C6000, 0x383C8000, 0x383CA000, 0x383CC000, 0x383CE000, 0x383D0000, 0x383D2000, 0x383D4000, 0x383D6000, 0x383D8000, 0x383DA000, 0x383DC000, 0x383DE000,
  0x383E0000, 0x383E2000, 0x383E4000, 0x383E6000, 0x383E8000, 0x383EA000, 0x383EC000, 0x383EE000, 0x383F0000, 0x383F2000, 0x383F4000, 0x383F6000, 0x383F8000, 0x383FA000, 0x383FC000, 0x383FE000,
  0x38400000, 0x38402000, 0x38404000, 0x38406000, 0x38408000, 0x3840A000, 0x3840C000, 0x3840E000, 0x38410000, 0x38412000, 0x38414000, 0x38416000, 0x38418000, 0x3841A000, 0x3841C000, 0x3841E000,
  0x38420000, 0x38422000, 0x38424000, 0x38426000, 0x38428000, 0x3842A000, 0x3842C000, 0x3842E000, 0x38430000, 0x38432000, 0x38434000, 0x38436000, 0x38438000, 0x3843A000, 0x3843C000, 0x3843E000,
  0x38440000, 0x38442000, 0x38444000, 0x38446000, 0x38448000, 0x3844A000, 0x3844C000, 0x3844E000, 0x38450000, 0x38452000, 0x38454000, 0x38456000, 0x38458000, 0x3845A000, 0x3845C000, 0x3845E000,
  0x38460000, 0x38462000, 0x38464000, 0x38466000, 0x38468000, 0x3846A000, 0x3846C000, 0x3846E000, 0x38470000, 0x38472000, 0x38474000, 0x38476000, 0x38478000, 0x3847A000, 0x3847C000, 0x3847E000,
  0x38480000, 0x38482000, 0x38484000, 0x38486000, 0x38488000, 0x3848A000, 0x3848C000, 0x3848E000, 0x38490000, 0x38492000, 0x38494000, 0x38496000, 0x38498000, 0x3849A000, 0x3849C000, 0x3849E000,
  0x384A0000, 0x384A2000, 0x384A4000, 0x384A6000, 0x384A8000, 0x384AA000, 0x384AC000, 0x384AE000, 0x384B0000, 0x384B2000, 0x384B4000, 0x384B6000, 0x384B8000, 0x384BA000, 0x384BC000, 0x384BE000,
  0x384C0000, 0x384C2000, 0x384C4000, 0x384C6000, 0x384C8000, 0x384CA000, 0x384CC000, 0x384CE000, 0x384D0000, 0x384D2000, 0x384D4000, 0x384D6000, 0x384D8000, 0x384DA000, 0x384DC000, 0x384DE000,
  0x384E0000, 0x384E2000, 0x384E4000, 0x384E6000, 0x384E8000, 0x384EA000, 0x384EC000, 0x384EE000, 0x384F0000, 0x384F2000, 0x384F4000, 0x384F6000, 0x384F8000, 0x384FA000, 0x384FC000, 0x384FE000,
  0x38500000, 0x38502000, 0x38504000, 0x38506000, 0x38508000, 0x3850A000, 0x3850C000, 0x3850E000, 0x38510000, 0x38512000, 0x38514000, 0x38516000, 0x38518000, 0x3851A000, 0x3851C000, 0x3851E000,
  0x38520000, 0x38522000, 0x38524000, 0x38526000, 0x38528000, 0x3852A000, 0x3852C000, 0x3852E000, 0x38530000, 0x38532000, 0x38534000, 0x38536000, 0x38538000, 0x3853A000, 0x3853C000, 0x3853E000,
  0x38540000, 0x38542000, 0x38544000, 0x38546000, 0x38548000, 0x3854A000, 0x3854C000, 0x3854E000, 0x38550000, 0x38552000, 0x38554000, 0x38556000, 0x38558000, 0x3855A000, 0x3855C000, 0x3855E000,
  0x38560000, 0x38562000, 0x38564000, 0x38566000, 0x38568000, 0x3856A000, 0x3856C000, 0x3856E000, 0x38570000, 0x38572000, 0x38574000, 0x38576000, 0x38578000, 0x3857A000, 0x3857C000, 0x3857E000,
  0x38580000, 0x38582000, 0x38584000, 0x38586000, 0x38588000, 0x3858A000, 0x3858C000, 0x3858E000, 0x38590000, 0x38592000, 0x38594000, 0x38596000, 0x38598000, 0x3859A000, 0x3859C000, 0x3859E000,
  0x385A0000, 0x385A2000, 0x385A4000, 0x385A6000, 0x385A8000, 0x385AA000, 0x385AC000, 0x385AE000, 0x385B0000, 0x385B2000, 0x385B4000, 0x385B6000, 0x385B8000, 0x385BA000, 0x385BC000, 0x385BE000,
  0x385C0000, 0x385C2000, 0x385C4000, 0x385C6000, 0x385C8000, 0x385CA000, 0x385CC000, 0x385CE000, 0x385D0000, 0x385D2000, 0x385D4000, 0x385D6000, 0x385D8000, 0x385DA000, 0x385DC000, 0x385DE000,
  0x385E0000, 0x385E2000, 0x385E4000, 0x385E6000, 0x385E8000, 0x385EA000, 0x385EC000, 0x385EE000, 0x385F0000, 0x385F2000, 0x385F4000, 0x385F6000, 0x385F8000, 0x385FA000, 0x385FC000, 0x385FE000,
  0x38600000, 0x38602000, 0x38604000, 0x38606000, 0x38608000, 0x3860A000, 0x3860C000, 0x3860E000, 0x38610000, 0x38612000, 0x38614000, 0x38616000, 0x38618000, 0x3861A000, 0x3861C000, 0x3861E000,
  0x38620000, 0x38622000, 0x38624000, 0x38626000, 0x38628000, 0x3862A000, 0x3862C000, 0x3862E000, 0x38630000, 0x38632000, 0x38634000, 0x38636000, 0x38638000, 0x3863A000, 0x3863C000, 0x3863E000,
  0x38640000, 0x38642000, 0x38644000, 0x38646000, 0x38648000, 0x3864A000, 0x3864C000, 0x3864E000, 0x38650000, 0x38652000, 0x38654000, 0x38656000, 0x38658000, 0x3865A000, 0x3865C000, 0x3865E000,
  0x38660000, 0x38662000, 0x38664000, 0x38666000, 0x38668000, 0x3866A000, 0x3866C000, 0x3866E000, 0x38670000, 0x38672000, 0x38674000, 0x38676000, 0x38678000, 0x3867A000, 0x3867C000, 0x3867E000,
  0x38680000, 0x38682000, 0x38684000, 0x38686000, 0x38688000, 0x3868A000, 0x3868C000, 0x3868E000, 0x38690000, 0x38692000, 0x38694000, 0x38696000, 0x38698000, 0x3869A000, 0x3869C000, 0x3869E000,
  0x386A0000, 0x386A2000, 0x386A4000, 0x386A6000, 0x386A8000, 0x386AA000, 0x386AC000, 0x386AE000, 0x386B0000, 0x386B2000, 0x386B4000, 0x386B6000, 0x386B8000, 0x386BA000, 0x386BC000, 0x386BE000,
  0x386C0000, 0x386C2000, 0x386C4000, 0x386C6000, 0x386C8000, 0x386CA000, 0x386CC000, 0x386CE000, 0x386D0000, 0x386D2000, 0x386D4000, 0x386D6000, 0x386D8000, 0x386DA000, 0x386DC000, 0x386DE000,
  0x386E0000, 0x386E2000, 0x386E4000, 0x386E6000, 0x386E8000, 0x386EA000, 0x386EC000, 0x386EE000, 0x386F0000, 0x386F2000, 0x386F4000, 0x386F6000, 0x386F8000, 0x386FA000, 0x386FC000, 0x386FE000,
  0x38700000, 0x38702000, 0x38704000, 0x38706000, 0x38708000, 0x3870A000, 0x3870C000, 0x3870E000, 0x38710000, 0x38712000, 0x38714000, 0x38716000, 0x38718000, 0x3871A000, 0x3871C000, 0x3871E000,
  0x38720000, 0x38722000, 0x38724000, 0x38726000, 0x38728000, 0x3872A000, 0x3872C000, 0x3872E000, 0x38730000, 0x38732000, 0x38734000, 0x38736000, 0x38738000, 0x3873A000, 0x3873C000, 0x3873E000,
  0x38740000, 0x38742000, 0x38744000, 0x38746000, 0x38748000, 0x3874A000, 0x3874C000, 0x3874E000, 0x38750000, 0x38752000, 0x38754000, 0x38756000, 0x38758000, 0x3875A000, 0x3875C000, 0x3875E000,
  0x38760000, 0x38762000, 0x38764000, 0x38766000, 0x38768000, 0x3876A000, 0x3876C000, 0x3876E000, 0x38770000, 0x38772000, 0x38774000, 0x38776000, 0x38778000, 0x3877A000, 0x3877C000, 0x3877E000,
  0x38780000, 0x38782000, 0x38784000, 0x38786000, 0x38788000, 0x3878A000, 0x3878C000, 0x3878E000, 0x38790000, 0x38792000, 0x38794000, 0x38796000, 0x38798000, 0x3879A000, 0x3879C000, 0x3879E000,
  0x387A0000, 0x387A2000, 0x387A4000, 0x387A6000, 0x387A8000, 0x387AA000, 0x387AC000, 0x387AE000, 0x387B0000, 0x387B2000, 0x387B4000, 0x387B6000, 0x387B8000, 0x387BA000, 0x387BC000, 0x387BE000,
  0x387C0000, 0x387C2000, 0x387C4000, 0x387C6000, 0x387C8000, 0x387CA000, 0x387CC000, 0x387CE000, 0x387D0000, 0x387D2000, 0x387D4000, 0x387D6000, 0x387D8000, 0x387DA000, 0x387DC000, 0x387DE000,
  0x387E0000, 0x387E2000, 0x387E4000, 0x387E6000, 0x387E8000, 0x387EA000, 0x387EC000, 0x387EE000, 0x387F0000, 0x387F2000, 0x387F4000, 0x387F6000, 0x387F8000, 0x387FA000, 0x387FC000, 0x387FE000 };
__constant static const uint32_t exponent_table[64] = {
  0x00000000, 0x00800000, 0x01000000, 0x01800000, 0x02000000, 0x02800000, 0x03000000, 0x03800000, 0x04000000, 0x04800000, 0x05000000, 0x05800000, 0x06000000, 0x06800000, 0x07000000, 0x07800000,
  0x08000000, 0x08800000, 0x09000000, 0x09800000, 0x0A000000, 0x0A800000, 0x0B000000, 0x0B800000, 0x0C000000, 0x0C800000, 0x0D000000, 0x0D800000, 0x0E000000, 0x0E800000, 0x0F000000, 0x47800000,
  0x80000000, 0x80800000, 0x81000000, 0x81800000, 0x82000000, 0x82800000, 0x83000000, 0x83800000, 0x84000000, 0x84800000, 0x85000000, 0x85800000, 0x86000000, 0x86800000, 0x87000000, 0x87800000,
  0x88000000, 0x88800000, 0x89000000, 0x89800000, 0x8A000000, 0x8A800000, 0x8B000000, 0x8B800000, 0x8C000000, 0x8C800000, 0x8D000000, 0x8D800000, 0x8E000000, 0x8E800000, 0x8F000000, 0xC7800000 };
__constant static const unsigned short offset_table[64] = {
  0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024,
  0, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024 };

SCALAR_FUN_ATTR uint16_t float2halfbits(float value) {
  union { float x; uint32_t y; } u;
  u.x = value;
  uint32_t bits = u.y;

  uint16_t hbits = base_table[bits>>23] + (uint16_t)((bits&0x7FFFFF)>>shift_table[bits>>23]);;

  return hbits;
}

SCALAR_FUN_ATTR float halfbits2float(uint16_t value) {
  uint32_t bits = mantissa_table[offset_table[value>>10]+(value&0x3FF)] + exponent_table[value>>10];

  union { uint32_t x; float y; } u;
  u.x = bits;
  return u.y;
}

SCALAR_FUN_ATTR uint16_t halfbitsnextafter(uint16_t from, uint16_t to) {
  int fabs = from & 0x7FFF, tabs = to & 0x7FFF;
  if(fabs > 0x7C00 || tabs > 0x7C00) {
    return ((from&0x7FFF)>0x7C00) ? (from|0x200) : (to|0x200);
  }
  if(from == to || !(fabs|tabs)) {
    return to;
  }
  if(!fabs) {
    return (to&0x8000)+1;
  }
  unsigned int out =
    from +
    (((from>>15)^(unsigned int)((from^(0x8000|(0x8000-(from>>15))))<(to^(0x8000|(0x8000-(to>>15))))))<<1)
    - 1;
  return out;
}

// End of half.h.
// Start of scalar.h.

// Implementation of the primitive scalar operations.  Very
// repetitive.  This code is inserted directly into both CUDA and
// OpenCL programs, as well as the CPU code, so it has some #ifdefs to
// work everywhere.  Some operations are defined as macros because
// this allows us to use them as constant expressions in things like
// array sizes and static initialisers.

// Some of the #ifdefs are because OpenCL uses type-generic functions
// for some operations (e.g. sqrt), while C and CUDA sensibly use
// distinct functions for different precisions (e.g. sqrtf() and
// sqrt()).  This is quite annoying.  Due to C's unfortunate casting
// rules, it is also really easy to accidentally implement
// floating-point functions in the wrong precision, so be careful.

// Double-precision definitions are only included if the preprocessor
// macro FUTHARK_F64_ENABLED is set.

#ifndef M_PI
#define M_PI 3.141592653589793
#endif

SCALAR_FUN_ATTR int32_t fptobits_f32_i32(float x);
SCALAR_FUN_ATTR float bitstofp_i32_f32(int32_t x);

SCALAR_FUN_ATTR uint8_t   add8(uint8_t x, uint8_t y)   { return x + y; }
SCALAR_FUN_ATTR uint16_t add16(uint16_t x, uint16_t y) { return x + y; }
SCALAR_FUN_ATTR uint32_t add32(uint32_t x, uint32_t y) { return x + y; }
SCALAR_FUN_ATTR uint64_t add64(uint64_t x, uint64_t y) { return x + y; }

SCALAR_FUN_ATTR uint8_t   sub8(uint8_t x, uint8_t y)   { return x - y; }
SCALAR_FUN_ATTR uint16_t sub16(uint16_t x, uint16_t y) { return x - y; }
SCALAR_FUN_ATTR uint32_t sub32(uint32_t x, uint32_t y) { return x - y; }
SCALAR_FUN_ATTR uint64_t sub64(uint64_t x, uint64_t y) { return x - y; }

SCALAR_FUN_ATTR uint8_t   mul8(uint8_t x, uint8_t y)   { return x * y; }
SCALAR_FUN_ATTR uint16_t mul16(uint16_t x, uint16_t y) { return x * y; }
SCALAR_FUN_ATTR uint32_t mul32(uint32_t x, uint32_t y) { return x * y; }
SCALAR_FUN_ATTR uint64_t mul64(uint64_t x, uint64_t y) { return x * y; }

#if defined(ISPC)

SCALAR_FUN_ATTR uint8_t udiv8(uint8_t x, uint8_t y) {
  // This strange pattern is used to prevent the ISPC compiler from
  // causing SIGFPEs and bogus results on divisions where inactive lanes
  // have 0-valued divisors. It ensures that any inactive lane instead
  // has a divisor of 1. https://github.com/ispc/ispc/issues/2292
  uint8_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR uint16_t udiv16(uint16_t x, uint16_t y) {
  uint16_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR uint32_t udiv32(uint32_t x, uint32_t y) {
  uint32_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR uint64_t udiv64(uint64_t x, uint64_t y) {
  uint64_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR uint8_t udiv_up8(uint8_t x, uint8_t y) {
  uint8_t ys = 1;
  foreach_active(i) { ys = y; }
  return (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint16_t udiv_up16(uint16_t x, uint16_t y) {
  uint16_t ys = 1;
  foreach_active(i) { ys = y; }
  return (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint32_t udiv_up32(uint32_t x, uint32_t y) {
  uint32_t ys = 1;
  foreach_active(i) { ys = y; }
  return (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint64_t udiv_up64(uint64_t x, uint64_t y) {
  uint64_t ys = 1;
  foreach_active(i) { ys = y; }
  return (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint8_t umod8(uint8_t x, uint8_t y) {
  uint8_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR uint16_t umod16(uint16_t x, uint16_t y) {
  uint16_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR uint32_t umod32(uint32_t x, uint32_t y) {
  uint32_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR uint64_t umod64(uint64_t x, uint64_t y) {
  uint64_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR uint8_t udiv_safe8(uint8_t x, uint8_t y) {
  uint8_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR uint16_t udiv_safe16(uint16_t x, uint16_t y) {
  uint16_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR uint32_t udiv_safe32(uint32_t x, uint32_t y) {
  uint32_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR uint64_t udiv_safe64(uint64_t x, uint64_t y) {
  uint64_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR uint8_t udiv_up_safe8(uint8_t x, uint8_t y) {
  uint8_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint16_t udiv_up_safe16(uint16_t x, uint16_t y) {
  uint16_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint32_t udiv_up_safe32(uint32_t x, uint32_t y) {
  uint32_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint64_t udiv_up_safe64(uint64_t x, uint64_t y) {
  uint64_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : (x + y - 1) / ys;
}

SCALAR_FUN_ATTR uint8_t umod_safe8(uint8_t x, uint8_t y) {
  uint8_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

SCALAR_FUN_ATTR uint16_t umod_safe16(uint16_t x, uint16_t y) {
  uint16_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

SCALAR_FUN_ATTR uint32_t umod_safe32(uint32_t x, uint32_t y) {
  uint32_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

SCALAR_FUN_ATTR uint64_t umod_safe64(uint64_t x, uint64_t y) {
  uint64_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

SCALAR_FUN_ATTR int8_t sdiv8(int8_t x, int8_t y) {
  int8_t ys = 1;
  foreach_active(i) { ys = y; }
  int8_t q = x / ys;
  int8_t r = x % ys;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int16_t sdiv16(int16_t x, int16_t y) {
  int16_t ys = 1;
  foreach_active(i) { ys = y; }
  int16_t q = x / ys;
  int16_t r = x % ys;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int32_t sdiv32(int32_t x, int32_t y) {
  int32_t ys = 1;
  foreach_active(i) { ys = y; }
  int32_t q = x / ys;
  int32_t r = x % ys;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int64_t sdiv64(int64_t x, int64_t y) {
  int64_t ys = 1;
  foreach_active(i) { ys = y; }
  int64_t q = x / ys;
  int64_t r = x % ys;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int8_t sdiv_up8(int8_t x, int8_t y) { return sdiv8(x + y - 1, y); }
SCALAR_FUN_ATTR int16_t sdiv_up16(int16_t x, int16_t y) { return sdiv16(x + y - 1, y); }
SCALAR_FUN_ATTR int32_t sdiv_up32(int32_t x, int32_t y) { return sdiv32(x + y - 1, y); }
SCALAR_FUN_ATTR int64_t sdiv_up64(int64_t x, int64_t y) { return sdiv64(x + y - 1, y); }

SCALAR_FUN_ATTR int8_t smod8(int8_t x, int8_t y) {
  int8_t ys = 1;
  foreach_active(i) { ys = y; }
  int8_t r = x % ys;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int16_t smod16(int16_t x, int16_t y) {
  int16_t ys = 1;
  foreach_active(i) { ys = y; }
  int16_t r = x % ys;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int32_t smod32(int32_t x, int32_t y) {
  int32_t ys = 1;
  foreach_active(i) { ys = y; }
  int32_t r = x % ys;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int64_t smod64(int64_t x, int64_t y) {
  int64_t ys = 1;
  foreach_active(i) { ys = y; }
  int64_t r = x % ys;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int8_t   sdiv_safe8(int8_t x, int8_t y)   { return y == 0 ? 0 : sdiv8(x, y); }
SCALAR_FUN_ATTR int16_t sdiv_safe16(int16_t x, int16_t y) { return y == 0 ? 0 : sdiv16(x, y); }
SCALAR_FUN_ATTR int32_t sdiv_safe32(int32_t x, int32_t y) { return y == 0 ? 0 : sdiv32(x, y); }
SCALAR_FUN_ATTR int64_t sdiv_safe64(int64_t x, int64_t y) { return y == 0 ? 0 : sdiv64(x, y); }

SCALAR_FUN_ATTR int8_t sdiv_up_safe8(int8_t x, int8_t y)     { return sdiv_safe8(x + y - 1, y); }
SCALAR_FUN_ATTR int16_t sdiv_up_safe16(int16_t x, int16_t y) { return sdiv_safe16(x + y - 1, y); }
SCALAR_FUN_ATTR int32_t sdiv_up_safe32(int32_t x, int32_t y) { return sdiv_safe32(x + y - 1, y); }
SCALAR_FUN_ATTR int64_t sdiv_up_safe64(int64_t x, int64_t y) { return sdiv_safe64(x + y - 1, y); }

SCALAR_FUN_ATTR int8_t   smod_safe8(int8_t x, int8_t y)   { return y == 0 ? 0 : smod8(x, y); }
SCALAR_FUN_ATTR int16_t smod_safe16(int16_t x, int16_t y) { return y == 0 ? 0 : smod16(x, y); }
SCALAR_FUN_ATTR int32_t smod_safe32(int32_t x, int32_t y) { return y == 0 ? 0 : smod32(x, y); }
SCALAR_FUN_ATTR int64_t smod_safe64(int64_t x, int64_t y) { return y == 0 ? 0 : smod64(x, y); }

SCALAR_FUN_ATTR int8_t squot8(int8_t x, int8_t y) {
  int8_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR int16_t squot16(int16_t x, int16_t y) {
  int16_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR int32_t squot32(int32_t x, int32_t y) {
  int32_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR int64_t squot64(int64_t x, int64_t y) {
  int64_t ys = 1;
  foreach_active(i) { ys = y; }
  return x / ys;
}

SCALAR_FUN_ATTR int8_t srem8(int8_t x, int8_t y) {
  int8_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR int16_t srem16(int16_t x, int16_t y) {
  int16_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR int32_t srem32(int32_t x, int32_t y) {
  int32_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR int64_t srem64(int64_t x, int64_t y) {
  int8_t ys = 1;
  foreach_active(i) { ys = y; }
  return x % ys;
}

SCALAR_FUN_ATTR int8_t squot_safe8(int8_t x, int8_t y) {
  int8_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR int16_t squot_safe16(int16_t x, int16_t y) {
  int16_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR int32_t squot_safe32(int32_t x, int32_t y) {
  int32_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR int64_t squot_safe64(int64_t x, int64_t y) {
  int64_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x / ys;
}

SCALAR_FUN_ATTR int8_t srem_safe8(int8_t x, int8_t y) {
  int8_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

SCALAR_FUN_ATTR int16_t srem_safe16(int16_t x, int16_t y) {
  int16_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

SCALAR_FUN_ATTR int32_t srem_safe32(int32_t x, int32_t y) {
  int32_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

SCALAR_FUN_ATTR int64_t srem_safe64(int64_t x, int64_t y) {
  int64_t ys = 1;
  foreach_active(i) { ys = y; }
  return y == 0 ? 0 : x % ys;
}

#else

SCALAR_FUN_ATTR uint8_t   udiv8(uint8_t x, uint8_t y)   { return x / y; }
SCALAR_FUN_ATTR uint16_t udiv16(uint16_t x, uint16_t y) { return x / y; }
SCALAR_FUN_ATTR uint32_t udiv32(uint32_t x, uint32_t y) { return x / y; }
SCALAR_FUN_ATTR uint64_t udiv64(uint64_t x, uint64_t y) { return x / y; }

SCALAR_FUN_ATTR uint8_t   udiv_up8(uint8_t x, uint8_t y)   { return (x + y - 1) / y; }
SCALAR_FUN_ATTR uint16_t udiv_up16(uint16_t x, uint16_t y) { return (x + y - 1) / y; }
SCALAR_FUN_ATTR uint32_t udiv_up32(uint32_t x, uint32_t y) { return (x + y - 1) / y; }
SCALAR_FUN_ATTR uint64_t udiv_up64(uint64_t x, uint64_t y) { return (x + y - 1) / y; }

SCALAR_FUN_ATTR uint8_t   umod8(uint8_t x, uint8_t y)   { return x % y; }
SCALAR_FUN_ATTR uint16_t umod16(uint16_t x, uint16_t y) { return x % y; }
SCALAR_FUN_ATTR uint32_t umod32(uint32_t x, uint32_t y) { return x % y; }
SCALAR_FUN_ATTR uint64_t umod64(uint64_t x, uint64_t y) { return x % y; }

SCALAR_FUN_ATTR uint8_t   udiv_safe8(uint8_t x, uint8_t y)   { return y == 0 ? 0 : x / y; }
SCALAR_FUN_ATTR uint16_t udiv_safe16(uint16_t x, uint16_t y) { return y == 0 ? 0 : x / y; }
SCALAR_FUN_ATTR uint32_t udiv_safe32(uint32_t x, uint32_t y) { return y == 0 ? 0 : x / y; }
SCALAR_FUN_ATTR uint64_t udiv_safe64(uint64_t x, uint64_t y) { return y == 0 ? 0 : x / y; }

SCALAR_FUN_ATTR uint8_t   udiv_up_safe8(uint8_t x, uint8_t y)   { return y == 0 ? 0 : (x + y - 1) / y; }
SCALAR_FUN_ATTR uint16_t udiv_up_safe16(uint16_t x, uint16_t y) { return y == 0 ? 0 : (x + y - 1) / y; }
SCALAR_FUN_ATTR uint32_t udiv_up_safe32(uint32_t x, uint32_t y) { return y == 0 ? 0 : (x + y - 1) / y; }
SCALAR_FUN_ATTR uint64_t udiv_up_safe64(uint64_t x, uint64_t y) { return y == 0 ? 0 : (x + y - 1) / y; }

SCALAR_FUN_ATTR uint8_t   umod_safe8(uint8_t x, uint8_t y)   { return y == 0 ? 0 : x % y; }
SCALAR_FUN_ATTR uint16_t umod_safe16(uint16_t x, uint16_t y) { return y == 0 ? 0 : x % y; }
SCALAR_FUN_ATTR uint32_t umod_safe32(uint32_t x, uint32_t y) { return y == 0 ? 0 : x % y; }
SCALAR_FUN_ATTR uint64_t umod_safe64(uint64_t x, uint64_t y) { return y == 0 ? 0 : x % y; }

SCALAR_FUN_ATTR int8_t sdiv8(int8_t x, int8_t y) {
  int8_t q = x / y;
  int8_t r = x % y;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int16_t sdiv16(int16_t x, int16_t y) {
  int16_t q = x / y;
  int16_t r = x % y;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int32_t sdiv32(int32_t x, int32_t y) {
  int32_t q = x / y;
  int32_t r = x % y;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int64_t sdiv64(int64_t x, int64_t y) {
  int64_t q = x / y;
  int64_t r = x % y;
  return q - ((r != 0 && r < 0 != y < 0) ? 1 : 0);
}

SCALAR_FUN_ATTR int8_t   sdiv_up8(int8_t x, int8_t y)   { return sdiv8(x + y - 1, y); }
SCALAR_FUN_ATTR int16_t sdiv_up16(int16_t x, int16_t y) { return sdiv16(x + y - 1, y); }
SCALAR_FUN_ATTR int32_t sdiv_up32(int32_t x, int32_t y) { return sdiv32(x + y - 1, y); }
SCALAR_FUN_ATTR int64_t sdiv_up64(int64_t x, int64_t y) { return sdiv64(x + y - 1, y); }

SCALAR_FUN_ATTR int8_t smod8(int8_t x, int8_t y) {
  int8_t r = x % y;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int16_t smod16(int16_t x, int16_t y) {
  int16_t r = x % y;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int32_t smod32(int32_t x, int32_t y) {
  int32_t r = x % y;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int64_t smod64(int64_t x, int64_t y) {
  int64_t r = x % y;
  return r + (r == 0 || (x > 0 && y > 0) || (x < 0 && y < 0) ? 0 : y);
}

SCALAR_FUN_ATTR int8_t   sdiv_safe8(int8_t x, int8_t y)   { return y == 0 ? 0 : sdiv8(x, y); }
SCALAR_FUN_ATTR int16_t sdiv_safe16(int16_t x, int16_t y) { return y == 0 ? 0 : sdiv16(x, y); }
SCALAR_FUN_ATTR int32_t sdiv_safe32(int32_t x, int32_t y) { return y == 0 ? 0 : sdiv32(x, y); }
SCALAR_FUN_ATTR int64_t sdiv_safe64(int64_t x, int64_t y) { return y == 0 ? 0 : sdiv64(x, y); }

SCALAR_FUN_ATTR int8_t   sdiv_up_safe8(int8_t x, int8_t y)   { return sdiv_safe8(x + y - 1, y);}
SCALAR_FUN_ATTR int16_t sdiv_up_safe16(int16_t x, int16_t y) { return sdiv_safe16(x + y - 1, y); }
SCALAR_FUN_ATTR int32_t sdiv_up_safe32(int32_t x, int32_t y) { return sdiv_safe32(x + y - 1, y); }
SCALAR_FUN_ATTR int64_t sdiv_up_safe64(int64_t x, int64_t y) { return sdiv_safe64(x + y - 1, y); }

SCALAR_FUN_ATTR int8_t   smod_safe8(int8_t x, int8_t y)   { return y == 0 ? 0 : smod8(x, y); }
SCALAR_FUN_ATTR int16_t smod_safe16(int16_t x, int16_t y) { return y == 0 ? 0 : smod16(x, y); }
SCALAR_FUN_ATTR int32_t smod_safe32(int32_t x, int32_t y) { return y == 0 ? 0 : smod32(x, y); }
SCALAR_FUN_ATTR int64_t smod_safe64(int64_t x, int64_t y) { return y == 0 ? 0 : smod64(x, y); }

SCALAR_FUN_ATTR int8_t   squot8(int8_t x, int8_t y)   { return x / y; }
SCALAR_FUN_ATTR int16_t squot16(int16_t x, int16_t y) { return x / y; }
SCALAR_FUN_ATTR int32_t squot32(int32_t x, int32_t y) { return x / y; }
SCALAR_FUN_ATTR int64_t squot64(int64_t x, int64_t y) { return x / y; }

SCALAR_FUN_ATTR int8_t   srem8(int8_t x, int8_t y)   { return x % y; }
SCALAR_FUN_ATTR int16_t srem16(int16_t x, int16_t y) { return x % y; }
SCALAR_FUN_ATTR int32_t srem32(int32_t x, int32_t y) { return x % y; }
SCALAR_FUN_ATTR int64_t srem64(int64_t x, int64_t y) { return x % y; }

SCALAR_FUN_ATTR int8_t   squot_safe8(int8_t x, int8_t y)   { return y == 0 ? 0 : x / y; }
SCALAR_FUN_ATTR int16_t squot_safe16(int16_t x, int16_t y) { return y == 0 ? 0 : x / y; }
SCALAR_FUN_ATTR int32_t squot_safe32(int32_t x, int32_t y) { return y == 0 ? 0 : x / y; }
SCALAR_FUN_ATTR int64_t squot_safe64(int64_t x, int64_t y) { return y == 0 ? 0 : x / y; }

SCALAR_FUN_ATTR int8_t   srem_safe8(int8_t x, int8_t y)   { return y == 0 ? 0 : x % y; }
SCALAR_FUN_ATTR int16_t srem_safe16(int16_t x, int16_t y) { return y == 0 ? 0 : x % y; }
SCALAR_FUN_ATTR int32_t srem_safe32(int32_t x, int32_t y) { return y == 0 ? 0 : x % y; }
SCALAR_FUN_ATTR int64_t srem_safe64(int64_t x, int64_t y) { return y == 0 ? 0 : x % y; }

#endif

SCALAR_FUN_ATTR int8_t   smin8(int8_t x, int8_t y)   { return x < y ? x : y; }
SCALAR_FUN_ATTR int16_t smin16(int16_t x, int16_t y) { return x < y ? x : y; }
SCALAR_FUN_ATTR int32_t smin32(int32_t x, int32_t y) { return x < y ? x : y; }
SCALAR_FUN_ATTR int64_t smin64(int64_t x, int64_t y) { return x < y ? x : y; }

SCALAR_FUN_ATTR uint8_t   umin8(uint8_t x, uint8_t y)   { return x < y ? x : y; }
SCALAR_FUN_ATTR uint16_t umin16(uint16_t x, uint16_t y) { return x < y ? x : y; }
SCALAR_FUN_ATTR uint32_t umin32(uint32_t x, uint32_t y) { return x < y ? x : y; }
SCALAR_FUN_ATTR uint64_t umin64(uint64_t x, uint64_t y) { return x < y ? x : y; }

SCALAR_FUN_ATTR int8_t  smax8(int8_t x, int8_t y)    { return x < y ? y : x; }
SCALAR_FUN_ATTR int16_t smax16(int16_t x, int16_t y) { return x < y ? y : x; }
SCALAR_FUN_ATTR int32_t smax32(int32_t x, int32_t y) { return x < y ? y : x; }
SCALAR_FUN_ATTR int64_t smax64(int64_t x, int64_t y) { return x < y ? y : x; }

SCALAR_FUN_ATTR uint8_t   umax8(uint8_t x, uint8_t y)   { return x < y ? y : x; }
SCALAR_FUN_ATTR uint16_t umax16(uint16_t x, uint16_t y) { return x < y ? y : x; }
SCALAR_FUN_ATTR uint32_t umax32(uint32_t x, uint32_t y) { return x < y ? y : x; }
SCALAR_FUN_ATTR uint64_t umax64(uint64_t x, uint64_t y) { return x < y ? y : x; }

SCALAR_FUN_ATTR uint8_t   shl8(uint8_t x, uint8_t y)   { return (uint8_t)(x << y); }
SCALAR_FUN_ATTR uint16_t shl16(uint16_t x, uint16_t y) { return (uint16_t)(x << y); }
SCALAR_FUN_ATTR uint32_t shl32(uint32_t x, uint32_t y) { return x << y; }
SCALAR_FUN_ATTR uint64_t shl64(uint64_t x, uint64_t y) { return x << y; }

SCALAR_FUN_ATTR uint8_t   lshr8(uint8_t x, uint8_t y)   { return x >> y; }
SCALAR_FUN_ATTR uint16_t lshr16(uint16_t x, uint16_t y) { return x >> y; }
SCALAR_FUN_ATTR uint32_t lshr32(uint32_t x, uint32_t y) { return x >> y; }
SCALAR_FUN_ATTR uint64_t lshr64(uint64_t x, uint64_t y) { return x >> y; }

SCALAR_FUN_ATTR int8_t   ashr8(int8_t x, int8_t y)   { return x >> y; }
SCALAR_FUN_ATTR int16_t ashr16(int16_t x, int16_t y) { return x >> y; }
SCALAR_FUN_ATTR int32_t ashr32(int32_t x, int32_t y) { return x >> y; }
SCALAR_FUN_ATTR int64_t ashr64(int64_t x, int64_t y) { return x >> y; }

SCALAR_FUN_ATTR uint8_t   and8(uint8_t x, uint8_t y)   { return x & y; }
SCALAR_FUN_ATTR uint16_t and16(uint16_t x, uint16_t y) { return x & y; }
SCALAR_FUN_ATTR uint32_t and32(uint32_t x, uint32_t y) { return x & y; }
SCALAR_FUN_ATTR uint64_t and64(uint64_t x, uint64_t y) { return x & y; }

SCALAR_FUN_ATTR uint8_t    or8(uint8_t x, uint8_t y)  { return x | y; }
SCALAR_FUN_ATTR uint16_t or16(uint16_t x, uint16_t y) { return x | y; }
SCALAR_FUN_ATTR uint32_t or32(uint32_t x, uint32_t y) { return x | y; }
SCALAR_FUN_ATTR uint64_t or64(uint64_t x, uint64_t y) { return x | y; }

SCALAR_FUN_ATTR uint8_t   xor8(uint8_t x, uint8_t y)   { return x ^ y; }
SCALAR_FUN_ATTR uint16_t xor16(uint16_t x, uint16_t y) { return x ^ y; }
SCALAR_FUN_ATTR uint32_t xor32(uint32_t x, uint32_t y) { return x ^ y; }
SCALAR_FUN_ATTR uint64_t xor64(uint64_t x, uint64_t y) { return x ^ y; }

SCALAR_FUN_ATTR bool ult8(uint8_t x, uint8_t y)    { return x < y; }
SCALAR_FUN_ATTR bool ult16(uint16_t x, uint16_t y) { return x < y; }
SCALAR_FUN_ATTR bool ult32(uint32_t x, uint32_t y) { return x < y; }
SCALAR_FUN_ATTR bool ult64(uint64_t x, uint64_t y) { return x < y; }

SCALAR_FUN_ATTR bool ule8(uint8_t x, uint8_t y)    { return x <= y; }
SCALAR_FUN_ATTR bool ule16(uint16_t x, uint16_t y) { return x <= y; }
SCALAR_FUN_ATTR bool ule32(uint32_t x, uint32_t y) { return x <= y; }
SCALAR_FUN_ATTR bool ule64(uint64_t x, uint64_t y) { return x <= y; }

SCALAR_FUN_ATTR bool  slt8(int8_t x, int8_t y)   { return x < y; }
SCALAR_FUN_ATTR bool slt16(int16_t x, int16_t y) { return x < y; }
SCALAR_FUN_ATTR bool slt32(int32_t x, int32_t y) { return x < y; }
SCALAR_FUN_ATTR bool slt64(int64_t x, int64_t y) { return x < y; }

SCALAR_FUN_ATTR bool  sle8(int8_t x, int8_t y)   { return x <= y; }
SCALAR_FUN_ATTR bool sle16(int16_t x, int16_t y) { return x <= y; }
SCALAR_FUN_ATTR bool sle32(int32_t x, int32_t y) { return x <= y; }
SCALAR_FUN_ATTR bool sle64(int64_t x, int64_t y) { return x <= y; }

SCALAR_FUN_ATTR uint8_t pow8(uint8_t x, uint8_t y) {
  uint8_t res = 1, rem = y;
  while (rem != 0) {
    if (rem & 1)
      res *= x;
    rem >>= 1;
    x *= x;
  }
  return res;
}

SCALAR_FUN_ATTR uint16_t pow16(uint16_t x, uint16_t y) {
  uint16_t res = 1, rem = y;
  while (rem != 0) {
    if (rem & 1)
      res *= x;
    rem >>= 1;
    x *= x;
  }
  return res;
}

SCALAR_FUN_ATTR uint32_t pow32(uint32_t x, uint32_t y) {
  uint32_t res = 1, rem = y;
  while (rem != 0) {
    if (rem & 1)
      res *= x;
    rem >>= 1;
    x *= x;
  }
  return res;
}

SCALAR_FUN_ATTR uint64_t pow64(uint64_t x, uint64_t y) {
  uint64_t res = 1, rem = y;
  while (rem != 0) {
    if (rem & 1)
      res *= x;
    rem >>= 1;
    x *= x;
  }
  return res;
}

SCALAR_FUN_ATTR bool  itob_i8_bool(int8_t x)  { return x != 0; }
SCALAR_FUN_ATTR bool itob_i16_bool(int16_t x) { return x != 0; }
SCALAR_FUN_ATTR bool itob_i32_bool(int32_t x) { return x != 0; }
SCALAR_FUN_ATTR bool itob_i64_bool(int64_t x) { return x != 0; }

SCALAR_FUN_ATTR int8_t btoi_bool_i8(bool x)   { return x; }
SCALAR_FUN_ATTR int16_t btoi_bool_i16(bool x) { return x; }
SCALAR_FUN_ATTR int32_t btoi_bool_i32(bool x) { return x; }
SCALAR_FUN_ATTR int64_t btoi_bool_i64(bool x) { return x; }

#define sext_i8_i8(x) ((int8_t) (int8_t) (x))
#define sext_i8_i16(x) ((int16_t) (int8_t) (x))
#define sext_i8_i32(x) ((int32_t) (int8_t) (x))
#define sext_i8_i64(x) ((int64_t) (int8_t) (x))
#define sext_i16_i8(x) ((int8_t) (int16_t) (x))
#define sext_i16_i16(x) ((int16_t) (int16_t) (x))
#define sext_i16_i32(x) ((int32_t) (int16_t) (x))
#define sext_i16_i64(x) ((int64_t) (int16_t) (x))
#define sext_i32_i8(x) ((int8_t) (int32_t) (x))
#define sext_i32_i16(x) ((int16_t) (int32_t) (x))
#define sext_i32_i32(x) ((int32_t) (int32_t) (x))
#define sext_i32_i64(x) ((int64_t) (int32_t) (x))
#define sext_i64_i8(x) ((int8_t) (int64_t) (x))
#define sext_i64_i16(x) ((int16_t) (int64_t) (x))
#define sext_i64_i32(x) ((int32_t) (int64_t) (x))
#define sext_i64_i64(x) ((int64_t) (int64_t) (x))
#define zext_i8_i8(x) ((int8_t) (uint8_t) (x))
#define zext_i8_i16(x) ((int16_t) (uint8_t) (x))
#define zext_i8_i32(x) ((int32_t) (uint8_t) (x))
#define zext_i8_i64(x) ((int64_t) (uint8_t) (x))
#define zext_i16_i8(x) ((int8_t) (uint16_t) (x))
#define zext_i16_i16(x) ((int16_t) (uint16_t) (x))
#define zext_i16_i32(x) ((int32_t) (uint16_t) (x))
#define zext_i16_i64(x) ((int64_t) (uint16_t) (x))
#define zext_i32_i8(x) ((int8_t) (uint32_t) (x))
#define zext_i32_i16(x) ((int16_t) (uint32_t) (x))
#define zext_i32_i32(x) ((int32_t) (uint32_t) (x))
#define zext_i32_i64(x) ((int64_t) (uint32_t) (x))
#define zext_i64_i8(x) ((int8_t) (uint64_t) (x))
#define zext_i64_i16(x) ((int16_t) (uint64_t) (x))
#define zext_i64_i32(x) ((int32_t) (uint64_t) (x))
#define zext_i64_i64(x) ((int64_t) (uint64_t) (x))

SCALAR_FUN_ATTR int8_t   abs8(int8_t x)  { return (int8_t)abs(x); }
SCALAR_FUN_ATTR int16_t abs16(int16_t x) { return (int16_t)abs(x); }
SCALAR_FUN_ATTR int32_t abs32(int32_t x) { return abs(x); }
SCALAR_FUN_ATTR int64_t abs64(int64_t x) {
#if defined(__OPENCL_VERSION__) || defined(ISPC)
  return abs(x);
#else
  return llabs(x);
#endif
}

#if defined(__OPENCL_VERSION__)

SCALAR_FUN_ATTR int32_t  futrts_popc8(int8_t x)  { return popcount(x); }
SCALAR_FUN_ATTR int32_t futrts_popc16(int16_t x) { return popcount(x); }
SCALAR_FUN_ATTR int32_t futrts_popc32(int32_t x) { return popcount(x); }
SCALAR_FUN_ATTR int32_t futrts_popc64(int64_t x) { return popcount(x); }

#elif defined(__CUDA_ARCH__)

SCALAR_FUN_ATTR int32_t  futrts_popc8(int8_t x)  { return __popc(zext_i8_i32(x)); }
SCALAR_FUN_ATTR int32_t futrts_popc16(int16_t x) { return __popc(zext_i16_i32(x)); }
SCALAR_FUN_ATTR int32_t futrts_popc32(int32_t x) { return __popc(x); }
SCALAR_FUN_ATTR int32_t futrts_popc64(int64_t x) { return __popcll(x); }

#else // Not OpenCL or CUDA, but plain C.

SCALAR_FUN_ATTR int32_t futrts_popc8(uint8_t x) {
  int c = 0;
  for (; x; ++c) { x &= x - 1; }
  return c;
}

SCALAR_FUN_ATTR int32_t futrts_popc16(uint16_t x) {
  int c = 0;
  for (; x; ++c) { x &= x - 1; }
  return c;
}

SCALAR_FUN_ATTR int32_t futrts_popc32(uint32_t x) {
  int c = 0;
  for (; x; ++c) { x &= x - 1; }
  return c;
}

SCALAR_FUN_ATTR int32_t futrts_popc64(uint64_t x) {
  int c = 0;
  for (; x; ++c) { x &= x - 1; }
  return c;
}
#endif

#if defined(__OPENCL_VERSION__)
SCALAR_FUN_ATTR uint8_t  futrts_umul_hi8 ( uint8_t a,  uint8_t b) { return mul_hi(a, b); }
SCALAR_FUN_ATTR uint16_t futrts_umul_hi16(uint16_t a, uint16_t b) { return mul_hi(a, b); }
SCALAR_FUN_ATTR uint32_t futrts_umul_hi32(uint32_t a, uint32_t b) { return mul_hi(a, b); }
SCALAR_FUN_ATTR uint64_t futrts_umul_hi64(uint64_t a, uint64_t b) { return mul_hi(a, b); }
SCALAR_FUN_ATTR uint8_t  futrts_smul_hi8 ( int8_t a,  int8_t b) { return mul_hi(a, b); }
SCALAR_FUN_ATTR uint16_t futrts_smul_hi16(int16_t a, int16_t b) { return mul_hi(a, b); }
SCALAR_FUN_ATTR uint32_t futrts_smul_hi32(int32_t a, int32_t b) { return mul_hi(a, b); }
SCALAR_FUN_ATTR uint64_t futrts_smul_hi64(int64_t a, int64_t b) { return mul_hi(a, b); }
#elif defined(__CUDA_ARCH__)
SCALAR_FUN_ATTR  uint8_t futrts_umul_hi8(uint8_t a, uint8_t b) { return ((uint16_t)a) * ((uint16_t)b) >> 8; }
SCALAR_FUN_ATTR uint16_t futrts_umul_hi16(uint16_t a, uint16_t b) { return ((uint32_t)a) * ((uint32_t)b) >> 16; }
SCALAR_FUN_ATTR uint32_t futrts_umul_hi32(uint32_t a, uint32_t b) { return __umulhi(a, b); }
SCALAR_FUN_ATTR uint64_t futrts_umul_hi64(uint64_t a, uint64_t b) { return __umul64hi(a, b); }
SCALAR_FUN_ATTR  uint8_t futrts_smul_hi8 ( int8_t a, int8_t b) { return ((int16_t)a) * ((int16_t)b) >> 8; }
SCALAR_FUN_ATTR uint16_t futrts_smul_hi16(int16_t a, int16_t b) { return ((int32_t)a) * ((int32_t)b) >> 16; }
SCALAR_FUN_ATTR uint32_t futrts_smul_hi32(int32_t a, int32_t b) { return __mulhi(a, b); }
SCALAR_FUN_ATTR uint64_t futrts_smul_hi64(int64_t a, int64_t b) { return __mul64hi(a, b); }
#elif defined(ISPC)
SCALAR_FUN_ATTR uint8_t futrts_umul_hi8(uint8_t a, uint8_t b) { return ((uint16_t)a) * ((uint16_t)b) >> 8; }
SCALAR_FUN_ATTR uint16_t futrts_umul_hi16(uint16_t a, uint16_t b) { return ((uint32_t)a) * ((uint32_t)b) >> 16; }
SCALAR_FUN_ATTR uint32_t futrts_umul_hi32(uint32_t a, uint32_t b) { return ((uint64_t)a) * ((uint64_t)b) >> 32; }
SCALAR_FUN_ATTR uint64_t futrts_umul_hi64(uint64_t a, uint64_t b) {
  uint64_t ah = a >> 32;
  uint64_t al = a & 0xffffffff;
  uint64_t bh = b >> 32;
  uint64_t bl = b & 0xffffffff;

  uint64_t p1 = al * bl;
  uint64_t p2 = al * bh;
  uint64_t p3 = ah * bl;
  uint64_t p4 = ah * bh;

  uint64_t p1h = p1 >> 32;
  uint64_t p2h = p2 >> 32;
  uint64_t p3h = p3 >> 32;
  uint64_t p2l = p2 & 0xffffffff;
  uint64_t p3l = p3 & 0xffffffff;

  uint64_t l = p1h + p2l + p3l;
  uint64_t m = (p2 >> 32) + (p3 >> 32);
  uint64_t h = (l >> 32) + m + p4;

  return h;
}
SCALAR_FUN_ATTR  int8_t futrts_smul_hi8 ( int8_t a,  int8_t b) { return ((uint16_t)a) * ((uint16_t)b) >> 8; }
SCALAR_FUN_ATTR int16_t futrts_smul_hi16(int16_t a, int16_t b) { return ((uint32_t)a) * ((uint32_t)b) >> 16; }
SCALAR_FUN_ATTR int32_t futrts_smul_hi32(int32_t a, int32_t b) { return ((uint64_t)a) * ((uint64_t)b) >> 32; }
SCALAR_FUN_ATTR int64_t futrts_smul_hi64(int64_t a, int64_t b) {
  uint64_t ah = a >> 32;
  uint64_t al = a & 0xffffffff;
  uint64_t bh = b >> 32;
  uint64_t bl = b & 0xffffffff;

  uint64_t p1 =  al * bl;
  int64_t  p2 = al * bh;
  int64_t  p3 = ah * bl;
  uint64_t p4 =  ah * bh;

  uint64_t p1h = p1 >> 32;
  uint64_t p2h = p2 >> 32;
  uint64_t p3h = p3 >> 32;
  uint64_t p2l = p2 & 0xffffffff;
  uint64_t p3l = p3 & 0xffffffff;

  uint64_t l = p1h + p2l + p3l;
  uint64_t m = (p2 >> 32) + (p3 >> 32);
  uint64_t h = (l >> 32) + m + p4;

  return h;
}

#else // Not OpenCL, ISPC, or CUDA, but plain C.
SCALAR_FUN_ATTR uint8_t futrts_umul_hi8(uint8_t a, uint8_t b) { return ((uint16_t)a) * ((uint16_t)b) >> 8; }
SCALAR_FUN_ATTR uint16_t futrts_umul_hi16(uint16_t a, uint16_t b) { return ((uint32_t)a) * ((uint32_t)b) >> 16; }
SCALAR_FUN_ATTR uint32_t futrts_umul_hi32(uint32_t a, uint32_t b) { return ((uint64_t)a) * ((uint64_t)b) >> 32; }
SCALAR_FUN_ATTR uint64_t futrts_umul_hi64(uint64_t a, uint64_t b) { return ((__uint128_t)a) * ((__uint128_t)b) >> 64; }
SCALAR_FUN_ATTR int8_t futrts_smul_hi8(int8_t a, int8_t b) { return ((int16_t)a) * ((int16_t)b) >> 8; }
SCALAR_FUN_ATTR int16_t futrts_smul_hi16(int16_t a, int16_t b) { return ((int32_t)a) * ((int32_t)b) >> 16; }
SCALAR_FUN_ATTR int32_t futrts_smul_hi32(int32_t a, int32_t b) { return ((int64_t)a) * ((int64_t)b) >> 32; }
SCALAR_FUN_ATTR int64_t futrts_smul_hi64(int64_t a, int64_t b) { return ((__int128_t)a) * ((__int128_t)b) >> 64; }
#endif

#if defined(__OPENCL_VERSION__)
SCALAR_FUN_ATTR  uint8_t futrts_umad_hi8 ( uint8_t a,  uint8_t b,  uint8_t c) { return mad_hi(a, b, c); }
SCALAR_FUN_ATTR uint16_t futrts_umad_hi16(uint16_t a, uint16_t b, uint16_t c) { return mad_hi(a, b, c); }
SCALAR_FUN_ATTR uint32_t futrts_umad_hi32(uint32_t a, uint32_t b, uint32_t c) { return mad_hi(a, b, c); }
SCALAR_FUN_ATTR uint64_t futrts_umad_hi64(uint64_t a, uint64_t b, uint64_t c) { return mad_hi(a, b, c); }
SCALAR_FUN_ATTR  uint8_t futrts_smad_hi8( int8_t a,  int8_t b,   int8_t c) { return mad_hi(a, b, c); }
SCALAR_FUN_ATTR uint16_t futrts_smad_hi16(int16_t a, int16_t b, int16_t c) { return mad_hi(a, b, c); }
SCALAR_FUN_ATTR uint32_t futrts_smad_hi32(int32_t a, int32_t b, int32_t c) { return mad_hi(a, b, c); }
SCALAR_FUN_ATTR uint64_t futrts_smad_hi64(int64_t a, int64_t b, int64_t c) { return mad_hi(a, b, c); }
#else // Not OpenCL

SCALAR_FUN_ATTR  uint8_t futrts_umad_hi8( uint8_t a,  uint8_t b,  uint8_t c) { return futrts_umul_hi8(a, b) + c; }
SCALAR_FUN_ATTR uint16_t futrts_umad_hi16(uint16_t a, uint16_t b, uint16_t c) { return futrts_umul_hi16(a, b) + c; }
SCALAR_FUN_ATTR uint32_t futrts_umad_hi32(uint32_t a, uint32_t b, uint32_t c) { return futrts_umul_hi32(a, b) + c; }
SCALAR_FUN_ATTR uint64_t futrts_umad_hi64(uint64_t a, uint64_t b, uint64_t c) { return futrts_umul_hi64(a, b) + c; }
SCALAR_FUN_ATTR  uint8_t futrts_smad_hi8 ( int8_t a,  int8_t b,  int8_t c) { return futrts_smul_hi8(a, b) + c; }
SCALAR_FUN_ATTR uint16_t futrts_smad_hi16(int16_t a, int16_t b, int16_t c) { return futrts_smul_hi16(a, b) + c; }
SCALAR_FUN_ATTR uint32_t futrts_smad_hi32(int32_t a, int32_t b, int32_t c) { return futrts_smul_hi32(a, b) + c; }
SCALAR_FUN_ATTR uint64_t futrts_smad_hi64(int64_t a, int64_t b, int64_t c) { return futrts_smul_hi64(a, b) + c; }
#endif

#if defined(__OPENCL_VERSION__)
SCALAR_FUN_ATTR int32_t  futrts_clzz8(int8_t x)  { return clz(x); }
SCALAR_FUN_ATTR int32_t futrts_clzz16(int16_t x) { return clz(x); }
SCALAR_FUN_ATTR int32_t futrts_clzz32(int32_t x) { return clz(x); }
SCALAR_FUN_ATTR int32_t futrts_clzz64(int64_t x) { return clz(x); }

#elif defined(__CUDA_ARCH__)

SCALAR_FUN_ATTR int32_t  futrts_clzz8(int8_t x)  { return __clz(zext_i8_i32(x)) - 24; }
SCALAR_FUN_ATTR int32_t futrts_clzz16(int16_t x) { return __clz(zext_i16_i32(x)) - 16; }
SCALAR_FUN_ATTR int32_t futrts_clzz32(int32_t x) { return __clz(x); }
SCALAR_FUN_ATTR int32_t futrts_clzz64(int64_t x) { return __clzll(x); }

#elif defined(ISPC)

SCALAR_FUN_ATTR int32_t  futrts_clzz8(int8_t x)  { return count_leading_zeros((int32_t)(uint8_t)x)-24; }
SCALAR_FUN_ATTR int32_t futrts_clzz16(int16_t x) { return count_leading_zeros((int32_t)(uint16_t)x)-16; }
SCALAR_FUN_ATTR int32_t futrts_clzz32(int32_t x) { return count_leading_zeros(x); }
SCALAR_FUN_ATTR int32_t futrts_clzz64(int64_t x) { return count_leading_zeros(x); }

#else // Not OpenCL, ISPC or CUDA, but plain C.

SCALAR_FUN_ATTR int32_t futrts_clzz8(int8_t x)
{ return x == 0 ? 8 : __builtin_clz((uint32_t)zext_i8_i32(x)) - 24; }
SCALAR_FUN_ATTR int32_t futrts_clzz16(int16_t x)
{ return x == 0 ? 16 : __builtin_clz((uint32_t)zext_i16_i32(x)) - 16; }
SCALAR_FUN_ATTR int32_t futrts_clzz32(int32_t x)
{ return x == 0 ? 32 : __builtin_clz((uint32_t)x); }
SCALAR_FUN_ATTR int32_t futrts_clzz64(int64_t x)
{ return x == 0 ? 64 : __builtin_clzll((uint64_t)x); }
#endif

#if defined(__OPENCL_VERSION__)
SCALAR_FUN_ATTR int32_t futrts_ctzz8(int8_t x) {
  int i = 0;
  for (; i < 8 && (x & 1) == 0; i++, x >>= 1) ;
  return i;
}

SCALAR_FUN_ATTR int32_t futrts_ctzz16(int16_t x) {
  int i = 0;
  for (; i < 16 && (x & 1) == 0; i++, x >>= 1) ;
  return i;
}

SCALAR_FUN_ATTR int32_t futrts_ctzz32(int32_t x) {
  int i = 0;
  for (; i < 32 && (x & 1) == 0; i++, x >>= 1) ;
  return i;
}

SCALAR_FUN_ATTR int32_t futrts_ctzz64(int64_t x) {
  int i = 0;
  for (; i < 64 && (x & 1) == 0; i++, x >>= 1) ;
  return i;
}

#elif defined(__CUDA_ARCH__)

SCALAR_FUN_ATTR int32_t futrts_ctzz8(int8_t x) {
  int y = __ffs(x);
  return y == 0 ? 8 : y - 1;
}

SCALAR_FUN_ATTR int32_t futrts_ctzz16(int16_t x) {
  int y = __ffs(x);
  return y == 0 ? 16 : y - 1;
}

SCALAR_FUN_ATTR int32_t futrts_ctzz32(int32_t x) {
  int y = __ffs(x);
  return y == 0 ? 32 : y - 1;
}

SCALAR_FUN_ATTR int32_t futrts_ctzz64(int64_t x) {
  int y = __ffsll(x);
  return y == 0 ? 64 : y - 1;
}

#elif defined(ISPC)

SCALAR_FUN_ATTR int32_t futrts_ctzz8(int8_t x) { return x == 0 ? 8 : count_trailing_zeros((int32_t)x); }
SCALAR_FUN_ATTR int32_t futrts_ctzz16(int16_t x) { return x == 0 ? 16 : count_trailing_zeros((int32_t)x); }
SCALAR_FUN_ATTR int32_t futrts_ctzz32(int32_t x) { return count_trailing_zeros(x); }
SCALAR_FUN_ATTR int32_t futrts_ctzz64(int64_t x) { return count_trailing_zeros(x); }

#else // Not OpenCL or CUDA, but plain C.

SCALAR_FUN_ATTR int32_t  futrts_ctzz8(int8_t x)  { return x == 0 ? 8 : __builtin_ctz((uint32_t)x); }
SCALAR_FUN_ATTR int32_t futrts_ctzz16(int16_t x) { return x == 0 ? 16 : __builtin_ctz((uint32_t)x); }
SCALAR_FUN_ATTR int32_t futrts_ctzz32(int32_t x) { return x == 0 ? 32 : __builtin_ctz((uint32_t)x); }
SCALAR_FUN_ATTR int32_t futrts_ctzz64(int64_t x) { return x == 0 ? 64 : __builtin_ctzll((uint64_t)x); }
#endif

SCALAR_FUN_ATTR float fdiv32(float x, float y) { return x / y; }
SCALAR_FUN_ATTR float fadd32(float x, float y) { return x + y; }
SCALAR_FUN_ATTR float fsub32(float x, float y) { return x - y; }
SCALAR_FUN_ATTR float fmul32(float x, float y) { return x * y; }
SCALAR_FUN_ATTR bool cmplt32(float x, float y) { return x < y; }
SCALAR_FUN_ATTR bool cmple32(float x, float y) { return x <= y; }
SCALAR_FUN_ATTR float sitofp_i8_f32(int8_t x)  { return (float) x; }

SCALAR_FUN_ATTR float sitofp_i16_f32(int16_t x) { return (float) x; }
SCALAR_FUN_ATTR float sitofp_i32_f32(int32_t x) { return (float) x; }
SCALAR_FUN_ATTR float sitofp_i64_f32(int64_t x) { return (float) x; }
SCALAR_FUN_ATTR float  uitofp_i8_f32(uint8_t x)  { return (float) x; }
SCALAR_FUN_ATTR float uitofp_i16_f32(uint16_t x) { return (float) x; }
SCALAR_FUN_ATTR float uitofp_i32_f32(uint32_t x) { return (float) x; }
SCALAR_FUN_ATTR float uitofp_i64_f32(uint64_t x) { return (float) x; }

#ifdef __OPENCL_VERSION__
SCALAR_FUN_ATTR float fabs32(float x)          { return fabs(x); }
SCALAR_FUN_ATTR float fmax32(float x, float y) { return fmax(x, y); }
SCALAR_FUN_ATTR float fmin32(float x, float y) { return fmin(x, y); }
SCALAR_FUN_ATTR float fpow32(float x, float y) { return pow(x, y); }

#elif defined(ISPC)

SCALAR_FUN_ATTR float fabs32(float x) { return abs(x); }
SCALAR_FUN_ATTR float fmax32(float x, float y) { return isnan(x) ? y : isnan(y) ? x : max(x, y); }
SCALAR_FUN_ATTR float fmin32(float x, float y) { return isnan(x) ? y : isnan(y) ? x : min(x, y); }
SCALAR_FUN_ATTR float fpow32(float a, float b) {
  float ret;
  foreach_active (i) {
      uniform float r = pow(extract(a, i), extract(b, i));
      ret = insert(ret, i, r);
  }
  return ret;
}

#else // Not OpenCL, but CUDA or plain C.

SCALAR_FUN_ATTR float fabs32(float x)          { return fabsf(x); }
SCALAR_FUN_ATTR float fmax32(float x, float y) { return fmaxf(x, y); }
SCALAR_FUN_ATTR float fmin32(float x, float y) { return fminf(x, y); }
SCALAR_FUN_ATTR float fpow32(float x, float y) { return powf(x, y); }
#endif

SCALAR_FUN_ATTR bool futrts_isnan32(float x) { return isnan(x); }

#if defined(ISPC)

SCALAR_FUN_ATTR bool futrts_isinf32(float x) { return !isnan(x) && isnan(x - x); }

SCALAR_FUN_ATTR bool futrts_isfinite32(float x) { return !isnan(x) && !futrts_isinf32(x); }

#else

SCALAR_FUN_ATTR bool futrts_isinf32(float x) { return isinf(x); }

#endif

SCALAR_FUN_ATTR int8_t fptosi_f32_i8(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (int8_t) x;
  }
}

SCALAR_FUN_ATTR int16_t fptosi_f32_i16(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (int16_t) x;
  }
}

SCALAR_FUN_ATTR int32_t fptosi_f32_i32(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (int32_t) x;
  }
}

SCALAR_FUN_ATTR int64_t fptosi_f32_i64(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (int64_t) x;
  };
}

SCALAR_FUN_ATTR uint8_t fptoui_f32_i8(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (uint8_t) (int8_t) x;
  }
}

SCALAR_FUN_ATTR uint16_t fptoui_f32_i16(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (uint16_t) (int16_t) x;
  }
}

SCALAR_FUN_ATTR uint32_t fptoui_f32_i32(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (uint32_t) (int32_t) x;
  }
}

SCALAR_FUN_ATTR uint64_t fptoui_f32_i64(float x) {
  if (futrts_isnan32(x) || futrts_isinf32(x)) {
    return 0;
  } else {
    return (uint64_t) (int64_t) x;
  }
}

SCALAR_FUN_ATTR bool ftob_f32_bool(float x) { return x != 0; }
SCALAR_FUN_ATTR float btof_bool_f32(bool x) { return x ? 1 : 0; }

#ifdef __OPENCL_VERSION__
SCALAR_FUN_ATTR float futrts_log32(float x) { return log(x); }
SCALAR_FUN_ATTR float futrts_log2_32(float x) { return log2(x); }
SCALAR_FUN_ATTR float futrts_log10_32(float x) { return log10(x); }
SCALAR_FUN_ATTR float futrts_log1p_32(float x) { return log1p(x); }
SCALAR_FUN_ATTR float futrts_sqrt32(float x) { return sqrt(x); }
SCALAR_FUN_ATTR float futrts_rsqrt32(float x) { return rsqrt(x); }
SCALAR_FUN_ATTR float futrts_cbrt32(float x) { return cbrt(x); }
SCALAR_FUN_ATTR float futrts_exp32(float x) { return exp(x); }
SCALAR_FUN_ATTR float futrts_cos32(float x) { return cos(x); }
SCALAR_FUN_ATTR float futrts_cospi32(float x) { return cospi(x); }
SCALAR_FUN_ATTR float futrts_sin32(float x) { return sin(x); }
SCALAR_FUN_ATTR float futrts_sinpi32(float x) { return sinpi(x); }
SCALAR_FUN_ATTR float futrts_tan32(float x) { return tan(x); }
SCALAR_FUN_ATTR float futrts_tanpi32(float x) { return tanpi(x); }
SCALAR_FUN_ATTR float futrts_acos32(float x) { return acos(x); }
SCALAR_FUN_ATTR float futrts_acospi32(float x) { return acospi(x); }
SCALAR_FUN_ATTR float futrts_asin32(float x) { return asin(x); }
SCALAR_FUN_ATTR float futrts_asinpi32(float x) { return asinpi(x); }
SCALAR_FUN_ATTR float futrts_atan32(float x) { return atan(x); }
SCALAR_FUN_ATTR float futrts_atanpi32(float x) { return atanpi(x); }
SCALAR_FUN_ATTR float futrts_cosh32(float x) { return cosh(x); }
SCALAR_FUN_ATTR float futrts_sinh32(float x) { return sinh(x); }
SCALAR_FUN_ATTR float futrts_tanh32(float x) { return tanh(x); }
SCALAR_FUN_ATTR float futrts_acosh32(float x) { return acosh(x); }
SCALAR_FUN_ATTR float futrts_asinh32(float x) { return asinh(x); }
SCALAR_FUN_ATTR float futrts_atanh32(float x) { return atanh(x); }
SCALAR_FUN_ATTR float futrts_atan2_32(float x, float y) { return atan2(x, y); }
SCALAR_FUN_ATTR float futrts_atan2pi_32(float x, float y) { return atan2pi(x, y); }
SCALAR_FUN_ATTR float futrts_hypot32(float x, float y) { return hypot(x, y); }
SCALAR_FUN_ATTR float futrts_gamma32(float x) { return tgamma(x); }
SCALAR_FUN_ATTR float futrts_lgamma32(float x) { return lgamma(x); }
SCALAR_FUN_ATTR float futrts_erf32(float x) { return erf(x); }
SCALAR_FUN_ATTR float futrts_erfc32(float x) { return erfc(x); }
SCALAR_FUN_ATTR float fmod32(float x, float y) { return fmod(x, y); }
SCALAR_FUN_ATTR float futrts_round32(float x) { return rint(x); }
SCALAR_FUN_ATTR float futrts_floor32(float x) { return floor(x); }
SCALAR_FUN_ATTR float futrts_ceil32(float x) { return ceil(x); }
SCALAR_FUN_ATTR float futrts_nextafter32(float x, float y) { return nextafter(x, y); }
SCALAR_FUN_ATTR float futrts_lerp32(float v0, float v1, float t) { return mix(v0, v1, t); }
SCALAR_FUN_ATTR float futrts_ldexp32(float x, int32_t y) { return ldexp(x, y); }
SCALAR_FUN_ATTR float futrts_copysign32(float x, float y) { return copysign(x, y); }
SCALAR_FUN_ATTR float futrts_mad32(float a, float b, float c) { return mad(a, b, c); }
SCALAR_FUN_ATTR float futrts_fma32(float a, float b, float c) { return fma(a, b, c); }

#elif defined(ISPC)

SCALAR_FUN_ATTR float futrts_log32(float x) { return futrts_isfinite32(x) || (futrts_isinf32(x) && x < 0)? log(x) : x; }
SCALAR_FUN_ATTR float futrts_log2_32(float x) { return futrts_log32(x) / log(2.0f); }
SCALAR_FUN_ATTR float futrts_log10_32(float x) { return futrts_log32(x) / log(10.0f); }

SCALAR_FUN_ATTR float futrts_log1p_32(float x) {
  if(x == -1.0f || (futrts_isinf32(x) && x > 0.0f)) return x / 0.0f;
  float y = 1.0f + x;
  float z = y - 1.0f;
  return log(y) - (z-x)/y;
}

SCALAR_FUN_ATTR float futrts_sqrt32(float x) { return sqrt(x); }
SCALAR_FUN_ATTR float futrts_rsqrt32(float x) { return 1/sqrt(x); }

extern "C" unmasked uniform float cbrtf(uniform float);
SCALAR_FUN_ATTR float futrts_cbrt32(float x) {
  float res;
  foreach_active (i) {
    uniform float r = cbrtf(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

SCALAR_FUN_ATTR float futrts_exp32(float x) { return exp(x); }
SCALAR_FUN_ATTR float futrts_cos32(float x) { return cos(x); }
SCALAR_FUN_ATTR float futrts_cospi32(float x) { return cos((float)M_PI*x); }
SCALAR_FUN_ATTR float futrts_sin32(float x) { return sin(x); }
SCALAR_FUN_ATTR float futrts_sinpi32(float x) { return sin(M_PI*x); }
SCALAR_FUN_ATTR float futrts_tan32(float x) { return tan(x); }
SCALAR_FUN_ATTR float futrts_tanpi32(float x) { return tan((float)M_PI*x); }
SCALAR_FUN_ATTR float futrts_acos32(float x) { return acos(x); }
SCALAR_FUN_ATTR float futrts_acospi32(float x) { return acos(x)/(float)M_PI; }
SCALAR_FUN_ATTR float futrts_asin32(float x) { return asin(x); }
SCALAR_FUN_ATTR float futrts_asinpi32(float x) { return asin(x)/(float)M_PI; }
SCALAR_FUN_ATTR float futrts_atan32(float x) { return atan(x); }
SCALAR_FUN_ATTR float futrts_atanpi32(float x) { return atan(x)/(float)M_PI; }
SCALAR_FUN_ATTR float futrts_cosh32(float x) { return (exp(x)+exp(-x)) / 2.0f; }
SCALAR_FUN_ATTR float futrts_sinh32(float x) { return (exp(x)-exp(-x)) / 2.0f; }
SCALAR_FUN_ATTR float futrts_tanh32(float x) { return futrts_sinh32(x)/futrts_cosh32(x); }

SCALAR_FUN_ATTR float futrts_acosh32(float x) {
  float f = x+sqrt(x*x-1);
  if (futrts_isfinite32(f)) return log(f);
  return f;
}

SCALAR_FUN_ATTR float futrts_asinh32(float x) {
  float f = x+sqrt(x*x+1);
  if (futrts_isfinite32(f)) return log(f);
  return f;
}

SCALAR_FUN_ATTR float futrts_atanh32(float x) {
  float f = (1+x)/(1-x);
  if (futrts_isfinite32(f)) return log(f)/2.0f;
  return f;
}

SCALAR_FUN_ATTR float futrts_atan2_32(float x, float y)
{ return (x == 0.0f && y == 0.0f) ? 0.0f : atan2(x, y); }
SCALAR_FUN_ATTR float futrts_atan2pi_32(float x, float y)
{ return (x == 0.0f && y == 0.0f) ? 0.0f : atan2(x, y) / (float)M_PI; }

SCALAR_FUN_ATTR float futrts_hypot32(float x, float y) {
  if (futrts_isfinite32(x) && futrts_isfinite32(y)) {
    x = abs(x);
    y = abs(y);
    float a;
    float b;
    if (x >= y){
        a = x;
        b = y;
    } else {
        a = y;
        b = x;
    }
    if(b == 0){
      return a;
    }

    int e;
    float an;
    float bn;
    an = frexp (a, &e);
    bn = ldexp (b, - e);
    float cn;
    cn = sqrt (an * an + bn * bn);
    return ldexp (cn, e);
  } else {
    if (futrts_isinf32(x) || futrts_isinf32(y)) return INFINITY;
    else return x + y;
  }

}

extern "C" unmasked uniform float tgammaf(uniform float x);
SCALAR_FUN_ATTR float futrts_gamma32(float x) {
  float res;
  foreach_active (i) {
    uniform float r = tgammaf(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform float lgammaf(uniform float x);
SCALAR_FUN_ATTR float futrts_lgamma32(float x) {
  float res;
  foreach_active (i) {
    uniform float r = lgammaf(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform float erff(uniform float x);
SCALAR_FUN_ATTR float futrts_erf32(float x) {
  float res;
  foreach_active (i) {
    uniform float r = erff(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform float erfcf(uniform float x);
SCALAR_FUN_ATTR float futrts_erfc32(float x) {
  float res;
  foreach_active (i) {
    uniform float r = erfcf(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

SCALAR_FUN_ATTR float fmod32(float x, float y) { return x - y * trunc(x/y); }
SCALAR_FUN_ATTR float futrts_round32(float x) { return round(x); }
SCALAR_FUN_ATTR float futrts_floor32(float x) { return floor(x); }
SCALAR_FUN_ATTR float futrts_ceil32(float x) { return ceil(x); }

extern "C" unmasked uniform float nextafterf(uniform float x, uniform float y);
SCALAR_FUN_ATTR float futrts_nextafter32(float x, float y) {
  float res;
  foreach_active (i) {
    uniform float r = nextafterf(extract(x, i), extract(y, i));
    res = insert(res, i, r);
  }
  return res;
}

SCALAR_FUN_ATTR float futrts_lerp32(float v0, float v1, float t) {
  return v0 + (v1 - v0) * t;
}

SCALAR_FUN_ATTR float futrts_ldexp32(float x, int32_t y) {
  return x * pow((uniform float)2.0, (float)y);
}

SCALAR_FUN_ATTR float futrts_copysign32(float x, float y) {
  int32_t xb = fptobits_f32_i32(x);
  int32_t yb = fptobits_f32_i32(y);
  return bitstofp_i32_f32((xb & ~(1<<31)) | (yb & (1<<31)));
}

SCALAR_FUN_ATTR float futrts_mad32(float a, float b, float c) {
  return a * b + c;
}

SCALAR_FUN_ATTR float futrts_fma32(float a, float b, float c) {
  return a * b + c;
}

#else // Not OpenCL or ISPC, but CUDA or plain C.

SCALAR_FUN_ATTR float futrts_log32(float x) { return logf(x); }
SCALAR_FUN_ATTR float futrts_log2_32(float x) { return log2f(x); }
SCALAR_FUN_ATTR float futrts_log10_32(float x) { return log10f(x); }
SCALAR_FUN_ATTR float futrts_log1p_32(float x) { return log1pf(x); }
SCALAR_FUN_ATTR float futrts_sqrt32(float x) { return sqrtf(x); }
SCALAR_FUN_ATTR float futrts_rsqrt32(float x) { return 1/sqrtf(x); }
SCALAR_FUN_ATTR float futrts_cbrt32(float x) { return cbrtf(x); }
SCALAR_FUN_ATTR float futrts_exp32(float x) { return expf(x); }
SCALAR_FUN_ATTR float futrts_cos32(float x) { return cosf(x); }

SCALAR_FUN_ATTR float futrts_cospi32(float x) {
#if defined(__CUDA_ARCH__)
  return cospif(x);
#else
  return cosf(((float)M_PI)*x);
#endif
}
SCALAR_FUN_ATTR float futrts_sin32(float x) { return sinf(x); }

SCALAR_FUN_ATTR float futrts_sinpi32(float x) {
#if defined(__CUDA_ARCH__)
  return sinpif(x);
#else
  return sinf((float)M_PI*x);
#endif
}

SCALAR_FUN_ATTR float futrts_tan32(float x) { return tanf(x); }
SCALAR_FUN_ATTR float futrts_tanpi32(float x) { return tanf((float)M_PI*x); }
SCALAR_FUN_ATTR float futrts_acos32(float x) { return acosf(x); }
SCALAR_FUN_ATTR float futrts_acospi32(float x) { return acosf(x)/(float)M_PI; }
SCALAR_FUN_ATTR float futrts_asin32(float x) { return asinf(x); }
SCALAR_FUN_ATTR float futrts_asinpi32(float x) { return asinf(x)/(float)M_PI; }
SCALAR_FUN_ATTR float futrts_atan32(float x) { return atanf(x); }
SCALAR_FUN_ATTR float futrts_atanpi32(float x) { return atanf(x)/(float)M_PI; }
SCALAR_FUN_ATTR float futrts_cosh32(float x) { return coshf(x); }
SCALAR_FUN_ATTR float futrts_sinh32(float x) { return sinhf(x); }
SCALAR_FUN_ATTR float futrts_tanh32(float x) { return tanhf(x); }
SCALAR_FUN_ATTR float futrts_acosh32(float x) { return acoshf(x); }
SCALAR_FUN_ATTR float futrts_asinh32(float x) { return asinhf(x); }
SCALAR_FUN_ATTR float futrts_atanh32(float x) { return atanhf(x); }
SCALAR_FUN_ATTR float futrts_atan2_32(float x, float y) { return atan2f(x, y); }
SCALAR_FUN_ATTR float futrts_atan2pi_32(float x, float y) { return atan2f(x, y) / (float)M_PI; }
SCALAR_FUN_ATTR float futrts_hypot32(float x, float y) { return hypotf(x, y); }
SCALAR_FUN_ATTR float futrts_gamma32(float x) { return tgammaf(x); }
SCALAR_FUN_ATTR float futrts_lgamma32(float x) { return lgammaf(x); }
SCALAR_FUN_ATTR float futrts_erf32(float x) { return erff(x); }
SCALAR_FUN_ATTR float futrts_erfc32(float x) { return erfcf(x); }
SCALAR_FUN_ATTR float fmod32(float x, float y) { return fmodf(x, y); }
SCALAR_FUN_ATTR float futrts_round32(float x) { return rintf(x); }
SCALAR_FUN_ATTR float futrts_floor32(float x) { return floorf(x); }
SCALAR_FUN_ATTR float futrts_ceil32(float x) { return ceilf(x); }
SCALAR_FUN_ATTR float futrts_nextafter32(float x, float y) { return nextafterf(x, y); }
SCALAR_FUN_ATTR float futrts_lerp32(float v0, float v1, float t) { return v0 + (v1 - v0) * t; }
SCALAR_FUN_ATTR float futrts_ldexp32(float x, int32_t y) { return ldexpf(x, y); }
SCALAR_FUN_ATTR float futrts_copysign32(float x, float y) { return copysignf(x, y); }
SCALAR_FUN_ATTR float futrts_mad32(float a, float b, float c) { return a * b + c; }
SCALAR_FUN_ATTR float futrts_fma32(float a, float b, float c) { return fmaf(a, b, c); }

#endif

#if defined(ISPC)

SCALAR_FUN_ATTR int32_t fptobits_f32_i32(float x) { return intbits(x); }
SCALAR_FUN_ATTR float bitstofp_i32_f32(int32_t x) { return floatbits(x); }
SCALAR_FUN_ATTR uniform int32_t fptobits_f32_i32(uniform float x) { return intbits(x); }
SCALAR_FUN_ATTR uniform float bitstofp_i32_f32(uniform int32_t x) { return floatbits(x); }

#else

SCALAR_FUN_ATTR int32_t fptobits_f32_i32(float x) {
  union {
    float f;
    int32_t t;
  } p;

  p.f = x;
  return p.t;
}

SCALAR_FUN_ATTR float bitstofp_i32_f32(int32_t x) {
  union {
    int32_t f;
    float t;
  } p;

  p.f = x;
  return p.t;
}
#endif

SCALAR_FUN_ATTR float fsignum32(float x) {
  return futrts_isnan32(x) ? x : (x > 0 ? 1 : 0) - (x < 0 ? 1 : 0);
}

#ifdef FUTHARK_F64_ENABLED

SCALAR_FUN_ATTR double bitstofp_i64_f64(int64_t x);
SCALAR_FUN_ATTR int64_t fptobits_f64_i64(double x);

#if defined(ISPC)

SCALAR_FUN_ATTR bool futrts_isinf64(float x) { return !isnan(x) && isnan(x - x); }
SCALAR_FUN_ATTR bool futrts_isfinite64(float x) { return !isnan(x) && !futrts_isinf64(x); }
SCALAR_FUN_ATTR double fdiv64(double x, double y) { return x / y; }
SCALAR_FUN_ATTR double fadd64(double x, double y) { return x + y; }
SCALAR_FUN_ATTR double fsub64(double x, double y) { return x - y; }
SCALAR_FUN_ATTR double fmul64(double x, double y) { return x * y; }
SCALAR_FUN_ATTR bool cmplt64(double x, double y) { return x < y; }
SCALAR_FUN_ATTR bool cmple64(double x, double y) { return x <= y; }
SCALAR_FUN_ATTR double sitofp_i8_f64(int8_t x) { return (double) x; }
SCALAR_FUN_ATTR double sitofp_i16_f64(int16_t x) { return (double) x; }
SCALAR_FUN_ATTR double sitofp_i32_f64(int32_t x) { return (double) x; }
SCALAR_FUN_ATTR double sitofp_i64_f64(int64_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i8_f64(uint8_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i16_f64(uint16_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i32_f64(uint32_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i64_f64(uint64_t x) { return (double) x; }
SCALAR_FUN_ATTR double fabs64(double x) { return abs(x); }
SCALAR_FUN_ATTR double fmax64(double x, double y) { return isnan(x) ? y : isnan(y) ? x : max(x, y); }
SCALAR_FUN_ATTR double fmin64(double x, double y) { return isnan(x) ? y : isnan(y) ? x : min(x, y); }

SCALAR_FUN_ATTR double fpow64(double a, double b) {
  float ret;
  foreach_active (i) {
      uniform float r = pow(extract(a, i), extract(b, i));
      ret = insert(ret, i, r);
  }
  return ret;
}
SCALAR_FUN_ATTR double futrts_log64(double x) { return futrts_isfinite64(x) || (futrts_isinf64(x) && x < 0)? log(x) : x; }
SCALAR_FUN_ATTR double futrts_log2_64(double x) { return futrts_log64(x)/log(2.0d); }
SCALAR_FUN_ATTR double futrts_log10_64(double x) { return futrts_log64(x)/log(10.0d); }

SCALAR_FUN_ATTR double futrts_log1p_64(double x) {
  if(x == -1.0d || (futrts_isinf64(x) && x > 0.0d)) return x / 0.0d;
  double y = 1.0d + x;
  double z = y - 1.0d;
  return log(y) - (z-x)/y;
}

SCALAR_FUN_ATTR double futrts_sqrt64(double x) { return sqrt(x); }
SCALAR_FUN_ATTR double futrts_rsqrt64(double x) { return 1/sqrt(x); }

SCALAR_FUN_ATTR double futrts_cbrt64(double x) {
  double res;
  foreach_active (i) {
    uniform double r = cbrtf(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}
SCALAR_FUN_ATTR double futrts_exp64(double x) { return exp(x); }
SCALAR_FUN_ATTR double futrts_cos64(double x) { return cos(x); }
SCALAR_FUN_ATTR double futrts_cospi64(double x) { return cos(M_PI*x); }
SCALAR_FUN_ATTR double futrts_sin64(double x) { return sin(x); }
SCALAR_FUN_ATTR double futrts_sinpi64(double x) { return sin(M_PI*x); }
SCALAR_FUN_ATTR double futrts_tan64(double x) { return tan(x); }
SCALAR_FUN_ATTR double futrts_tanpi64(double x) { return tan(M_PI*x); }
SCALAR_FUN_ATTR double futrts_acos64(double x) { return acos(x); }
SCALAR_FUN_ATTR double futrts_acospi64(double x) { return acos(x)/M_PI; }
SCALAR_FUN_ATTR double futrts_asin64(double x) { return asin(x); }
SCALAR_FUN_ATTR double futrts_asinpi64(double x) { return asin(x)/M_PI; }
SCALAR_FUN_ATTR double futrts_atan64(double x) { return atan(x); }
SCALAR_FUN_ATTR double futrts_atanpi64(double x) { return atan(x)/M_PI; }
SCALAR_FUN_ATTR double futrts_cosh64(double x) { return (exp(x)+exp(-x)) / 2.0d; }
SCALAR_FUN_ATTR double futrts_sinh64(double x) { return (exp(x)-exp(-x)) / 2.0d; }
SCALAR_FUN_ATTR double futrts_tanh64(double x) { return futrts_sinh64(x)/futrts_cosh64(x); }

SCALAR_FUN_ATTR double futrts_acosh64(double x) {
  double f = x+sqrt(x*x-1.0d);
  if(futrts_isfinite64(f)) return log(f);
  return f;
}

SCALAR_FUN_ATTR double futrts_asinh64(double x) {
  double f = x+sqrt(x*x+1.0d);
  if(futrts_isfinite64(f)) return log(f);
  return f;
}

SCALAR_FUN_ATTR double futrts_atanh64(double x) {
  double f = (1.0d+x)/(1.0d-x);
  if(futrts_isfinite64(f)) return log(f)/2.0d;
  return f;
}
SCALAR_FUN_ATTR double futrts_atan2_64(double x, double y) { return atan2(x, y); }

SCALAR_FUN_ATTR double futrts_atan2pi_64(double x, double y) { return atan2(x, y) / M_PI; }

extern "C" unmasked uniform double hypot(uniform double x, uniform double y);
SCALAR_FUN_ATTR double futrts_hypot64(double x, double y) {
  double res;
  foreach_active (i) {
    uniform double r = hypot(extract(x, i), extract(y, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform double tgamma(uniform double x);
SCALAR_FUN_ATTR double futrts_gamma64(double x) {
  double res;
  foreach_active (i) {
    uniform double r = tgamma(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform double lgamma(uniform double x);
SCALAR_FUN_ATTR double futrts_lgamma64(double x) {
  double res;
  foreach_active (i) {
    uniform double r = lgamma(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform double erf(uniform double x);
SCALAR_FUN_ATTR double futrts_erf64(double x) {
  double res;
  foreach_active (i) {
    uniform double r = erf(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform double erfc(uniform double x);
SCALAR_FUN_ATTR double futrts_erfc64(double x) {
  double res;
  foreach_active (i) {
    uniform double r = erfc(extract(x, i));
    res = insert(res, i, r);
  }
  return res;
}

SCALAR_FUN_ATTR double futrts_fma64(double a, double b, double c) { return a * b + c; }
SCALAR_FUN_ATTR double futrts_round64(double x) { return round(x); }
SCALAR_FUN_ATTR double futrts_ceil64(double x) { return ceil(x); }

extern "C" unmasked uniform double nextafter(uniform float x, uniform double y);
SCALAR_FUN_ATTR float futrts_nextafter64(double x, double y) {
  double res;
  foreach_active (i) {
    uniform double r = nextafter(extract(x, i), extract(y, i));
    res = insert(res, i, r);
  }
  return res;
}

SCALAR_FUN_ATTR double futrts_floor64(double x) { return floor(x); }
SCALAR_FUN_ATTR bool futrts_isnan64(double x) { return isnan(x); }

SCALAR_FUN_ATTR int8_t fptosi_f64_i8(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int8_t) x;
  }
}

SCALAR_FUN_ATTR int16_t fptosi_f64_i16(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int16_t) x;
  }
}

SCALAR_FUN_ATTR int32_t fptosi_f64_i32(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int32_t) x;
  }
}

SCALAR_FUN_ATTR int64_t fptosi_f64_i64(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int64_t) x;
  }
}

SCALAR_FUN_ATTR uint8_t fptoui_f64_i8(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint8_t) (int8_t) x;
  }
}

SCALAR_FUN_ATTR uint16_t fptoui_f64_i16(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint16_t) (int16_t) x;
  }
}

SCALAR_FUN_ATTR uint32_t fptoui_f64_i32(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint32_t) (int32_t) x;
  }
}

SCALAR_FUN_ATTR uint64_t fptoui_f64_i64(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint64_t) (int64_t) x;
  }
}

SCALAR_FUN_ATTR bool ftob_f64_bool(double x) { return x != 0.0; }
SCALAR_FUN_ATTR double btof_bool_f64(bool x) { return x ? 1.0 : 0.0; }

SCALAR_FUN_ATTR int64_t fptobits_f64_i64(double x) {
  int64_t res;
  foreach_active (i) {
    uniform double tmp = extract(x, i);
    uniform int64_t r = *((uniform int64_t* uniform)&tmp);
    res = insert(res, i, r);
  }
  return res;
}

SCALAR_FUN_ATTR double bitstofp_i64_f64(int64_t x) {
  double res;
  foreach_active (i) {
    uniform int64_t tmp = extract(x, i);
    uniform double r = *((uniform double* uniform)&tmp);
    res = insert(res, i, r);
  }
  return res;
}

SCALAR_FUN_ATTR uniform int64_t fptobits_f64_i64(uniform double x) {
  return intbits(x);
}

SCALAR_FUN_ATTR uniform double bitstofp_i64_f64(uniform int64_t x) {
  return doublebits(x);
}

SCALAR_FUN_ATTR double fmod64(double x, double y) {
  return x - y * trunc(x/y);
}

SCALAR_FUN_ATTR double fsignum64(double x) {
  return futrts_isnan64(x) ? x : (x > 0 ? 1.0d : 0.0d) - (x < 0 ? 1.0d : 0.0d);
}

SCALAR_FUN_ATTR double futrts_lerp64(double v0, double v1, double t) {
  return v0 + (v1 - v0) * t;
}

SCALAR_FUN_ATTR double futrts_ldexp64(double x, int32_t y) {
  return x * pow((uniform double)2.0, (double)y);
}

SCALAR_FUN_ATTR double futrts_copysign64(double x, double y) {
  int64_t xb = fptobits_f64_i64(x);
  int64_t yb = fptobits_f64_i64(y);
  return bitstofp_i64_f64((xb & ~(((int64_t)1)<<63)) | (yb & (((int64_t)1)<<63)));
}

SCALAR_FUN_ATTR double futrts_mad64(double a, double b, double c) { return a * b + c; }
SCALAR_FUN_ATTR float fpconv_f32_f32(float x) { return (float) x; }
SCALAR_FUN_ATTR double fpconv_f32_f64(float x) { return (double) x; }
SCALAR_FUN_ATTR float fpconv_f64_f32(double x) { return (float) x; }
SCALAR_FUN_ATTR double fpconv_f64_f64(double x) { return (double) x; }

#else

SCALAR_FUN_ATTR double fdiv64(double x, double y) { return x / y; }
SCALAR_FUN_ATTR double fadd64(double x, double y) { return x + y; }
SCALAR_FUN_ATTR double fsub64(double x, double y) { return x - y; }
SCALAR_FUN_ATTR double fmul64(double x, double y) { return x * y; }
SCALAR_FUN_ATTR bool cmplt64(double x, double y) { return x < y; }
SCALAR_FUN_ATTR bool cmple64(double x, double y) { return x <= y; }
SCALAR_FUN_ATTR double sitofp_i8_f64(int8_t x) { return (double) x; }
SCALAR_FUN_ATTR double sitofp_i16_f64(int16_t x) { return (double) x; }
SCALAR_FUN_ATTR double sitofp_i32_f64(int32_t x) { return (double) x; }
SCALAR_FUN_ATTR double sitofp_i64_f64(int64_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i8_f64(uint8_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i16_f64(uint16_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i32_f64(uint32_t x) { return (double) x; }
SCALAR_FUN_ATTR double uitofp_i64_f64(uint64_t x) { return (double) x; }
SCALAR_FUN_ATTR double fabs64(double x) { return fabs(x); }
SCALAR_FUN_ATTR double fmax64(double x, double y) { return fmax(x, y); }
SCALAR_FUN_ATTR double fmin64(double x, double y) { return fmin(x, y); }
SCALAR_FUN_ATTR double fpow64(double x, double y) { return pow(x, y); }
SCALAR_FUN_ATTR double futrts_log64(double x) { return log(x); }
SCALAR_FUN_ATTR double futrts_log2_64(double x) { return log2(x); }
SCALAR_FUN_ATTR double futrts_log10_64(double x) { return log10(x); }
SCALAR_FUN_ATTR double futrts_log1p_64(double x) { return log1p(x); }
SCALAR_FUN_ATTR double futrts_sqrt64(double x) { return sqrt(x); }
SCALAR_FUN_ATTR double futrts_rsqrt64(double x) { return 1/sqrt(x); }
SCALAR_FUN_ATTR double futrts_cbrt64(double x) { return cbrt(x); }
SCALAR_FUN_ATTR double futrts_exp64(double x) { return exp(x); }
SCALAR_FUN_ATTR double futrts_cos64(double x) { return cos(x); }

SCALAR_FUN_ATTR double futrts_cospi64(double x) {
#ifdef __OPENCL_VERSION__
  return cospi(x);
#elif defined(__CUDA_ARCH__)
  return cospi(x);
#else
  return cos(M_PI*x);
#endif
}

SCALAR_FUN_ATTR double futrts_sin64(double x) {
  return sin(x);
}

SCALAR_FUN_ATTR double futrts_sinpi64(double x) {
#ifdef __OPENCL_VERSION__
  return sinpi(x);
#elif defined(__CUDA_ARCH__)
  return sinpi(x);
#else
  return sin(M_PI*x);
#endif
}

SCALAR_FUN_ATTR double futrts_tan64(double x) {
  return tan(x);
}

SCALAR_FUN_ATTR double futrts_tanpi64(double x) {
#ifdef __OPENCL_VERSION__
  return tanpi(x);
#else
  return tan(M_PI*x);
#endif
}

SCALAR_FUN_ATTR double futrts_acos64(double x) {
  return acos(x);
}

SCALAR_FUN_ATTR double futrts_acospi64(double x) {
#ifdef __OPENCL_VERSION__
  return acospi(x);
#else
  return acos(x) / M_PI;
#endif
}

SCALAR_FUN_ATTR double futrts_asin64(double x) {
  return asin(x);
}

SCALAR_FUN_ATTR double futrts_asinpi64(double x) {
#ifdef __OPENCL_VERSION__
  return asinpi(x);
#else
  return asin(x) / M_PI;
#endif
}

SCALAR_FUN_ATTR double futrts_atan64(double x) {
  return atan(x);
}

SCALAR_FUN_ATTR double futrts_atanpi64(double x) {
#ifdef __OPENCL_VERSION__
  return atanpi(x);
#else
  return atan(x) / M_PI;
#endif
}

SCALAR_FUN_ATTR double futrts_cosh64(double x) { return cosh(x); }
SCALAR_FUN_ATTR double futrts_sinh64(double x) { return sinh(x); }
SCALAR_FUN_ATTR double futrts_tanh64(double x) { return tanh(x); }
SCALAR_FUN_ATTR double futrts_acosh64(double x) { return acosh(x); }
SCALAR_FUN_ATTR double futrts_asinh64(double x) { return asinh(x); }
SCALAR_FUN_ATTR double futrts_atanh64(double x) { return atanh(x); }
SCALAR_FUN_ATTR double futrts_atan2_64(double x, double y) { return atan2(x, y); }

SCALAR_FUN_ATTR double futrts_atan2pi_64(double x, double y) {
#ifdef __OPENCL_VERSION__
  return atan2pi(x, y);
#else
  return atan2(x, y) / M_PI;
#endif
}

SCALAR_FUN_ATTR double futrts_hypot64(double x, double y) { return hypot(x, y); }
SCALAR_FUN_ATTR double futrts_gamma64(double x) { return tgamma(x); }
SCALAR_FUN_ATTR double futrts_lgamma64(double x) { return lgamma(x); }
SCALAR_FUN_ATTR double futrts_erf64(double x) { return erf(x); }
SCALAR_FUN_ATTR double futrts_erfc64(double x) { return erfc(x); }
SCALAR_FUN_ATTR double futrts_fma64(double a, double b, double c) { return fma(a, b, c); }
SCALAR_FUN_ATTR double futrts_round64(double x) { return rint(x); }
SCALAR_FUN_ATTR double futrts_ceil64(double x) { return ceil(x); }
SCALAR_FUN_ATTR float futrts_nextafter64(float x, float y) { return nextafter(x, y); }
SCALAR_FUN_ATTR double futrts_floor64(double x) { return floor(x); }
SCALAR_FUN_ATTR bool futrts_isnan64(double x) { return isnan(x); }
SCALAR_FUN_ATTR bool futrts_isinf64(double x) { return isinf(x); }

SCALAR_FUN_ATTR int8_t fptosi_f64_i8(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int8_t) x;
  }
}

SCALAR_FUN_ATTR int16_t fptosi_f64_i16(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int16_t) x;
  }
}

SCALAR_FUN_ATTR int32_t fptosi_f64_i32(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int32_t) x;
  }
}

SCALAR_FUN_ATTR int64_t fptosi_f64_i64(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (int64_t) x;
  }
}

SCALAR_FUN_ATTR uint8_t fptoui_f64_i8(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint8_t) (int8_t) x;
  }
}

SCALAR_FUN_ATTR uint16_t fptoui_f64_i16(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint16_t) (int16_t) x;
  }
}

SCALAR_FUN_ATTR uint32_t fptoui_f64_i32(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint32_t) (int32_t) x;
  }
}

SCALAR_FUN_ATTR uint64_t fptoui_f64_i64(double x) {
  if (futrts_isnan64(x) || futrts_isinf64(x)) {
    return 0;
  } else {
    return (uint64_t) (int64_t) x;
  }
}

SCALAR_FUN_ATTR bool ftob_f64_bool(double x) { return x != 0; }
SCALAR_FUN_ATTR double btof_bool_f64(bool x) { return x ? 1 : 0; }

SCALAR_FUN_ATTR int64_t fptobits_f64_i64(double x) {
  union {
    double f;
    int64_t t;
  } p;

  p.f = x;
  return p.t;
}

SCALAR_FUN_ATTR double bitstofp_i64_f64(int64_t x) {
  union {
    int64_t f;
    double t;
  } p;

  p.f = x;
  return p.t;
}

SCALAR_FUN_ATTR double fmod64(double x, double y) {
  return fmod(x, y);
}

SCALAR_FUN_ATTR double fsignum64(double x) {
  return futrts_isnan64(x) ? x : (x > 0) - (x < 0);
}

SCALAR_FUN_ATTR double futrts_lerp64(double v0, double v1, double t) {
#ifdef __OPENCL_VERSION__
  return mix(v0, v1, t);
#else
  return v0 + (v1 - v0) * t;
#endif
}

SCALAR_FUN_ATTR double futrts_ldexp64(double x, int32_t y) {
  return ldexp(x, y);
}

SCALAR_FUN_ATTR float futrts_copysign64(double x, double y) {
  return copysign(x, y);
}

SCALAR_FUN_ATTR double futrts_mad64(double a, double b, double c) {
#ifdef __OPENCL_VERSION__
  return mad(a, b, c);
#else
  return a * b + c;
#endif
}

SCALAR_FUN_ATTR float fpconv_f32_f32(float x) { return (float) x; }
SCALAR_FUN_ATTR double fpconv_f32_f64(float x) { return (double) x; }
SCALAR_FUN_ATTR float fpconv_f64_f32(double x) { return (float) x; }
SCALAR_FUN_ATTR double fpconv_f64_f64(double x) { return (double) x; }

#endif

#endif

#define futrts_cond_f16(x,y,z) ((x) ? (y) : (z))
#define futrts_cond_f32(x,y,z) ((x) ? (y) : (z))
#define futrts_cond_f64(x,y,z) ((x) ? (y) : (z))

#define futrts_cond_i8(x,y,z) ((x) ? (y) : (z))
#define futrts_cond_i16(x,y,z) ((x) ? (y) : (z))
#define futrts_cond_i32(x,y,z) ((x) ? (y) : (z))
#define futrts_cond_i64(x,y,z) ((x) ? (y) : (z))

#define futrts_cond_bool(x,y,z) ((x) ? (y) : (z))
#define futrts_cond_unit(x,y,z) ((x) ? (y) : (z))

// End of scalar.h.
// Start of scalar_f16.h.

// Half-precision is emulated if needed (e.g. in straight C) with the
// native type used if possible.  The emulation works by typedef'ing
// 'float' to 'f16', and then implementing all operations on single
// precision.  To cut down on duplication, we use the same code for
// those Futhark functions that require just operators or casts.  The
// in-memory representation for arrays will still be 16 bits even
// under emulation, so the compiler will have to be careful when
// generating reads or writes.

#if !defined(cl_khr_fp16) && !(defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 600) && !(defined(ISPC))
#define EMULATE_F16
#endif

#if !defined(EMULATE_F16) && defined(__OPENCL_VERSION__)
#pragma OPENCL EXTENSION cl_khr_fp16 : enable
#endif

#ifdef EMULATE_F16

// Note that the half-precision storage format is still 16 bits - the
// compiler will have to be real careful!
typedef float f16;

#elif defined(ISPC)
typedef float16 f16;

#else

#ifdef __CUDA_ARCH__
#include <cuda_fp16.h>
#endif

typedef half f16;

#endif

// Some of these functions convert to single precision because half
// precision versions are not available.
SCALAR_FUN_ATTR f16 fadd16(f16 x, f16 y) { return x + y; }
SCALAR_FUN_ATTR f16 fsub16(f16 x, f16 y) { return x - y; }
SCALAR_FUN_ATTR f16 fmul16(f16 x, f16 y) { return x * y; }
SCALAR_FUN_ATTR bool cmplt16(f16 x, f16 y) { return x < y; }
SCALAR_FUN_ATTR bool cmple16(f16 x, f16 y) { return x <= y; }
SCALAR_FUN_ATTR f16 sitofp_i8_f16(int8_t x) { return (f16) x; }
SCALAR_FUN_ATTR f16 sitofp_i16_f16(int16_t x) { return (f16) x; }
SCALAR_FUN_ATTR f16 sitofp_i32_f16(int32_t x) { return (f16) x; }
SCALAR_FUN_ATTR f16 sitofp_i64_f16(int64_t x) { return (f16) x; }
SCALAR_FUN_ATTR f16 uitofp_i8_f16(uint8_t x) { return (f16) x; }
SCALAR_FUN_ATTR f16 uitofp_i16_f16(uint16_t x) { return (f16) x; }
SCALAR_FUN_ATTR f16 uitofp_i32_f16(uint32_t x) { return (f16) x; }
SCALAR_FUN_ATTR f16 uitofp_i64_f16(uint64_t x) { return (f16) x; }
SCALAR_FUN_ATTR int8_t fptosi_f16_i8(f16 x) { return (int8_t) (float) x; }
SCALAR_FUN_ATTR int16_t fptosi_f16_i16(f16 x) { return (int16_t) x; }
SCALAR_FUN_ATTR int32_t fptosi_f16_i32(f16 x) { return (int32_t) x; }
SCALAR_FUN_ATTR int64_t fptosi_f16_i64(f16 x) { return (int64_t) x; }
SCALAR_FUN_ATTR uint8_t fptoui_f16_i8(f16 x) { return (uint8_t) (float) x; }
SCALAR_FUN_ATTR uint16_t fptoui_f16_i16(f16 x) { return (uint16_t) x; }
SCALAR_FUN_ATTR uint32_t fptoui_f16_i32(f16 x) { return (uint32_t) x; }
SCALAR_FUN_ATTR uint64_t fptoui_f16_i64(f16 x) { return (uint64_t) x; }
SCALAR_FUN_ATTR bool ftob_f16_bool(f16 x) { return x != (f16)0; }
SCALAR_FUN_ATTR f16 btof_bool_f16(bool x) { return x ? 1 : 0; }

#ifndef EMULATE_F16

SCALAR_FUN_ATTR bool futrts_isnan16(f16 x) { return isnan((float)x); }

#ifdef __OPENCL_VERSION__

SCALAR_FUN_ATTR f16 fabs16(f16 x) { return fabs(x); }
SCALAR_FUN_ATTR f16 fmax16(f16 x, f16 y) { return fmax(x, y); }
SCALAR_FUN_ATTR f16 fmin16(f16 x, f16 y) { return fmin(x, y); }
SCALAR_FUN_ATTR f16 fpow16(f16 x, f16 y) { return pow(x, y); }

#elif defined(ISPC)

SCALAR_FUN_ATTR f16 fabs16(f16 x) { return abs(x); }
SCALAR_FUN_ATTR f16 fmax16(f16 x, f16 y) { return futrts_isnan16(x) ? y : futrts_isnan16(y) ? x : max(x, y); }
SCALAR_FUN_ATTR f16 fmin16(f16 x, f16 y) { return futrts_isnan16(x) ? y : futrts_isnan16(y) ? x : min(x, y); }
SCALAR_FUN_ATTR f16 fpow16(f16 x, f16 y) { return pow(x, y); }

#else // Assuming CUDA.

SCALAR_FUN_ATTR f16 fabs16(f16 x) { return fabsf(x); }
SCALAR_FUN_ATTR f16 fmax16(f16 x, f16 y) { return fmaxf(x, y); }
SCALAR_FUN_ATTR f16 fmin16(f16 x, f16 y) { return fminf(x, y); }
SCALAR_FUN_ATTR f16 fpow16(f16 x, f16 y) { return powf(x, y); }

#endif

#if defined(ISPC)
SCALAR_FUN_ATTR bool futrts_isinf16(float x) { return !futrts_isnan16(x) && futrts_isnan16(x - x); }
SCALAR_FUN_ATTR bool futrts_isfinite16(float x) { return !futrts_isnan16(x) && !futrts_isinf16(x); }
#else
SCALAR_FUN_ATTR bool futrts_isinf16(f16 x) { return isinf((float)x); }
#endif

#ifdef __OPENCL_VERSION__
SCALAR_FUN_ATTR f16 futrts_log16(f16 x) { return log(x); }
SCALAR_FUN_ATTR f16 futrts_log2_16(f16 x) { return log2(x); }
SCALAR_FUN_ATTR f16 futrts_log10_16(f16 x) { return log10(x); }
SCALAR_FUN_ATTR f16 futrts_log1p_16(f16 x) { return log1p(x); }
SCALAR_FUN_ATTR f16 futrts_sqrt16(f16 x) { return sqrt(x); }
SCALAR_FUN_ATTR f16 futrts_rsqrt16(f16 x) { return rsqrt(x); }
SCALAR_FUN_ATTR f16 futrts_cbrt16(f16 x) { return cbrt(x); }
SCALAR_FUN_ATTR f16 futrts_exp16(f16 x) { return exp(x); }
SCALAR_FUN_ATTR f16 futrts_cos16(f16 x) { return cos(x); }
SCALAR_FUN_ATTR f16 futrts_cospi16(f16 x) { return cospi(x); }
SCALAR_FUN_ATTR f16 futrts_sin16(f16 x) { return sin(x); }
SCALAR_FUN_ATTR f16 futrts_sinpi16(f16 x) { return sinpi(x); }
SCALAR_FUN_ATTR f16 futrts_tan16(f16 x) { return tan(x); }
SCALAR_FUN_ATTR f16 futrts_tanpi16(f16 x) { return tanpi(x); }
SCALAR_FUN_ATTR f16 futrts_acos16(f16 x) { return acos(x); }
SCALAR_FUN_ATTR f16 futrts_acospi16(f16 x) { return acospi(x); }
SCALAR_FUN_ATTR f16 futrts_asin16(f16 x) { return asin(x); }
SCALAR_FUN_ATTR f16 futrts_asinpi16(f16 x) { return asinpi(x); }
SCALAR_FUN_ATTR f16 futrts_atan16(f16 x) { return atan(x); }
SCALAR_FUN_ATTR f16 futrts_atanpi16(f16 x) { return atanpi(x); }
SCALAR_FUN_ATTR f16 futrts_cosh16(f16 x) { return cosh(x); }
SCALAR_FUN_ATTR f16 futrts_sinh16(f16 x) { return sinh(x); }
SCALAR_FUN_ATTR f16 futrts_tanh16(f16 x) { return tanh(x); }
SCALAR_FUN_ATTR f16 futrts_acosh16(f16 x) { return acosh(x); }
SCALAR_FUN_ATTR f16 futrts_asinh16(f16 x) { return asinh(x); }
SCALAR_FUN_ATTR f16 futrts_atanh16(f16 x) { return atanh(x); }
SCALAR_FUN_ATTR f16 futrts_atan2_16(f16 x, f16 y) { return atan2(x, y); }
SCALAR_FUN_ATTR f16 futrts_atan2pi_16(f16 x, f16 y) { return atan2pi(x, y); }
SCALAR_FUN_ATTR f16 futrts_hypot16(f16 x, f16 y) { return hypot(x, y); }
SCALAR_FUN_ATTR f16 futrts_gamma16(f16 x) { return tgamma(x); }
SCALAR_FUN_ATTR f16 futrts_lgamma16(f16 x) { return lgamma(x); }
SCALAR_FUN_ATTR f16 futrts_erf16(f16 x) { return erf(x); }
SCALAR_FUN_ATTR f16 futrts_erfc16(f16 x) { return erfc(x); }
SCALAR_FUN_ATTR f16 fmod16(f16 x, f16 y) { return fmod(x, y); }
SCALAR_FUN_ATTR f16 futrts_round16(f16 x) { return rint(x); }
SCALAR_FUN_ATTR f16 futrts_floor16(f16 x) { return floor(x); }
SCALAR_FUN_ATTR f16 futrts_ceil16(f16 x) { return ceil(x); }
SCALAR_FUN_ATTR f16 futrts_nextafter16(f16 x, f16 y) { return nextafter(x, y); }
SCALAR_FUN_ATTR f16 futrts_lerp16(f16 v0, f16 v1, f16 t) { return mix(v0, v1, t); }
SCALAR_FUN_ATTR f16 futrts_ldexp16(f16 x, int32_t y) { return ldexp(x, y); }
SCALAR_FUN_ATTR f16 futrts_copysign16(f16 x, f16 y) { return copysign(x, y); }
SCALAR_FUN_ATTR f16 futrts_mad16(f16 a, f16 b, f16 c) { return mad(a, b, c); }
SCALAR_FUN_ATTR f16 futrts_fma16(f16 a, f16 b, f16 c) { return fma(a, b, c); }

#elif defined(ISPC)

SCALAR_FUN_ATTR f16 futrts_log16(f16 x) { return futrts_isfinite16(x) || (futrts_isinf16(x) && x < 0) ? log(x) : x; }
SCALAR_FUN_ATTR f16 futrts_log2_16(f16 x) { return futrts_log16(x) / log(2.0f16); }
SCALAR_FUN_ATTR f16 futrts_log10_16(f16 x) { return futrts_log16(x) / log(10.0f16); }
SCALAR_FUN_ATTR f16 futrts_log1p_16(f16 x) {
  if(x == -1.0f16 || (futrts_isinf16(x) && x > 0.0f16)) return x / 0.0f16;
  f16 y = 1.0f16 + x;
  f16 z = y - 1.0f16;
  return log(y) - (z-x)/y;
}
SCALAR_FUN_ATTR f16 futrts_sqrt16(f16 x) { return (float16)sqrt((float)x); }
SCALAR_FUN_ATTR f16 futrts_rsqrt16(f16 x) { return (float16)1/sqrt((float)x); }
SCALAR_FUN_ATTR f16 futrts_exp16(f16 x) { return exp(x); }
SCALAR_FUN_ATTR f16 futrts_cos16(f16 x) { return (float16)cos((float)x); }
SCALAR_FUN_ATTR f16 futrts_cospi16(f16 x) { return (float16)cos((float)M_PI*(float)x); }
SCALAR_FUN_ATTR f16 futrts_sin16(f16 x) { return (float16)sin((float)x); }
SCALAR_FUN_ATTR f16 futrts_sinpi16(f16 x) { return (float16)sin((float)M_PI*(float)x); }
SCALAR_FUN_ATTR f16 futrts_tan16(f16 x) { return (float16)tan((float)x); }
SCALAR_FUN_ATTR f16 futrts_tanpi16(f16 x) { return (float16)(tan((float)M_PI*(float)x)); }
SCALAR_FUN_ATTR f16 futrts_acos16(f16 x) { return (float16)acos((float)x); }
SCALAR_FUN_ATTR f16 futrts_acospi16(f16 x) { return (float16)(acos((float)x)/(float)M_PI); }
SCALAR_FUN_ATTR f16 futrts_asin16(f16 x) { return (float16)asin((float)x); }
SCALAR_FUN_ATTR f16 futrts_asinpi16(f16 x) { return (float16)(asin((float)x)/(float)M_PI); }
SCALAR_FUN_ATTR f16 futrts_atan16(f16 x) { return (float16)atan((float)x); }
SCALAR_FUN_ATTR f16 futrts_atanpi16(f16 x) { return (float16)(atan((float)x)/(float)M_PI); }
SCALAR_FUN_ATTR f16 futrts_cosh16(f16 x) { return (exp(x)+exp(-x)) / 2.0f16; }
SCALAR_FUN_ATTR f16 futrts_sinh16(f16 x) { return (exp(x)-exp(-x)) / 2.0f16; }
SCALAR_FUN_ATTR f16 futrts_tanh16(f16 x) { return futrts_sinh16(x)/futrts_cosh16(x); }
SCALAR_FUN_ATTR f16 futrts_acosh16(f16 x) {
  float16 f = x+(float16)sqrt((float)(x*x-1));
  if(futrts_isfinite16(f)) return log(f);
  return f;
}
SCALAR_FUN_ATTR f16 futrts_asinh16(f16 x) {
  float16 f = x+(float16)sqrt((float)(x*x+1));
  if(futrts_isfinite16(f)) return log(f);
  return f;
}
SCALAR_FUN_ATTR f16 futrts_atanh16(f16 x) {
  float16 f = (1+x)/(1-x);
  if(futrts_isfinite16(f)) return log(f)/2.0f16;
  return f;
}
SCALAR_FUN_ATTR f16 futrts_atan2_16(f16 x, f16 y) { return (float16)atan2((float)x, (float)y); }
SCALAR_FUN_ATTR f16 futrts_atan2pi_16(f16 x, f16 y) { return (float16)(atan2((float)x, (float)y)/(float)M_PI); }
SCALAR_FUN_ATTR f16 futrts_hypot16(f16 x, f16 y) { return (float16)futrts_hypot32((float)x, (float)y); }

extern "C" unmasked uniform float tgammaf(uniform float x);
SCALAR_FUN_ATTR f16 futrts_gamma16(f16 x) {
  f16 res;
  foreach_active (i) {
    uniform f16 r = (f16)tgammaf(extract((float)x, i));
    res = insert(res, i, r);
  }
  return res;
}

extern "C" unmasked uniform float lgammaf(uniform float x);
SCALAR_FUN_ATTR f16 futrts_lgamma16(f16 x) {
  f16 res;
  foreach_active (i) {
    uniform f16 r = (f16)lgammaf(extract((float)x, i));
    res = insert(res, i, r);
  }
  return res;
}
SCALAR_FUN_ATTR f16 futrts_cbrt16(f16 x) { return (f16)futrts_cbrt32((float)x); }
SCALAR_FUN_ATTR f16 futrts_erf16(f16 x) { return (f16)futrts_erf32((float)x); }
SCALAR_FUN_ATTR f16 futrts_erfc16(f16 x) { return (f16)futrts_erfc32((float)x); }
SCALAR_FUN_ATTR f16 fmod16(f16 x, f16 y) { return x - y * (float16)trunc((float) (x/y)); }
SCALAR_FUN_ATTR f16 futrts_round16(f16 x) { return (float16)round((float)x); }
SCALAR_FUN_ATTR f16 futrts_floor16(f16 x) { return (float16)floor((float)x); }
SCALAR_FUN_ATTR f16 futrts_ceil16(f16 x) { return (float16)ceil((float)x); }
SCALAR_FUN_ATTR f16 futrts_nextafter16(f16 x, f16 y) { return (float16)futrts_nextafter32((float)x, (float) y); }
SCALAR_FUN_ATTR f16 futrts_lerp16(f16 v0, f16 v1, f16 t) { return v0 + (v1 - v0) * t; }
SCALAR_FUN_ATTR f16 futrts_ldexp16(f16 x, int32_t y) { return futrts_ldexp32((float)x, y); }
SCALAR_FUN_ATTR f16 futrts_copysign16(f16 x, f16 y) { return futrts_copysign32((float)x, y); }
SCALAR_FUN_ATTR f16 futrts_mad16(f16 a, f16 b, f16 c) { return a * b + c; }
SCALAR_FUN_ATTR f16 futrts_fma16(f16 a, f16 b, f16 c) { return a * b + c; }

#else // Assume CUDA.

SCALAR_FUN_ATTR f16 futrts_log16(f16 x) { return hlog(x); }
SCALAR_FUN_ATTR f16 futrts_log2_16(f16 x) { return hlog2(x); }
SCALAR_FUN_ATTR f16 futrts_log10_16(f16 x) { return hlog10(x); }
SCALAR_FUN_ATTR f16 futrts_log1p_16(f16 x) { return (f16)log1pf((float)x); }
SCALAR_FUN_ATTR f16 futrts_sqrt16(f16 x) { return hsqrt(x); }
SCALAR_FUN_ATTR f16 futrts_rsqrt16(f16 x) { return hrsqrt(x); }
SCALAR_FUN_ATTR f16 futrts_cbrt16(f16 x) { return cbrtf(x); }
SCALAR_FUN_ATTR f16 futrts_exp16(f16 x) { return hexp(x); }
SCALAR_FUN_ATTR f16 futrts_cos16(f16 x) { return hcos(x); }
SCALAR_FUN_ATTR f16 futrts_cospi16(f16 x) { return hcos((f16)M_PI*x); }
SCALAR_FUN_ATTR f16 futrts_sin16(f16 x) { return hsin(x); }
SCALAR_FUN_ATTR f16 futrts_sinpi16(f16 x) { return hsin((f16)M_PI*x); }
SCALAR_FUN_ATTR f16 futrts_tan16(f16 x) { return tanf(x); }
SCALAR_FUN_ATTR f16 futrts_tanpi16(f16 x) { return tanf((f16)M_PI*x); }
SCALAR_FUN_ATTR f16 futrts_acos16(f16 x) { return acosf(x); }
SCALAR_FUN_ATTR f16 futrts_acospi16(f16 x) { return (f16)acosf(x)/(f16)M_PI; }
SCALAR_FUN_ATTR f16 futrts_asin16(f16 x) { return asinf(x); }
SCALAR_FUN_ATTR f16 futrts_asinpi16(f16 x) { return (f16)asinf(x)/(f16)M_PI; }
SCALAR_FUN_ATTR f16 futrts_atan16(f16 x) { return (f16)atanf(x); }
SCALAR_FUN_ATTR f16 futrts_atanpi16(f16 x) { return (f16)atanf(x)/(f16)M_PI; }
SCALAR_FUN_ATTR f16 futrts_cosh16(f16 x) { return coshf(x); }
SCALAR_FUN_ATTR f16 futrts_sinh16(f16 x) { return sinhf(x); }
SCALAR_FUN_ATTR f16 futrts_tanh16(f16 x) { return tanhf(x); }
SCALAR_FUN_ATTR f16 futrts_acosh16(f16 x) { return acoshf(x); }
SCALAR_FUN_ATTR f16 futrts_asinh16(f16 x) { return asinhf(x); }
SCALAR_FUN_ATTR f16 futrts_atanh16(f16 x) { return atanhf(x); }
SCALAR_FUN_ATTR f16 futrts_atan2_16(f16 x, f16 y) { return (f16)atan2f(x, y); }
SCALAR_FUN_ATTR f16 futrts_atan2pi_16(f16 x, f16 y) { return (f16)atan2f(x, y)/(f16)M_PI; }
SCALAR_FUN_ATTR f16 futrts_hypot16(f16 x, f16 y) { return hypotf(x, y); }
SCALAR_FUN_ATTR f16 futrts_gamma16(f16 x) { return tgammaf(x); }
SCALAR_FUN_ATTR f16 futrts_lgamma16(f16 x) { return lgammaf(x); }
SCALAR_FUN_ATTR f16 futrts_erf16(f16 x) { return erff(x); }
SCALAR_FUN_ATTR f16 futrts_erfc16(f16 x) { return erfcf(x); }
SCALAR_FUN_ATTR f16 fmod16(f16 x, f16 y) { return fmodf(x, y); }
SCALAR_FUN_ATTR f16 futrts_round16(f16 x) { return rintf(x); }
SCALAR_FUN_ATTR f16 futrts_floor16(f16 x) { return hfloor(x); }
SCALAR_FUN_ATTR f16 futrts_ceil16(f16 x) { return hceil(x); }
SCALAR_FUN_ATTR f16 futrts_nextafter16(f16 x, f16 y) { return __ushort_as_half(halfbitsnextafter(__half_as_ushort(x), __half_as_ushort(y))); }
SCALAR_FUN_ATTR f16 futrts_lerp16(f16 v0, f16 v1, f16 t) { return v0 + (v1 - v0) * t; }
SCALAR_FUN_ATTR f16 futrts_ldexp16(f16 x, int32_t y) { return futrts_ldexp32((float)x, y); }
SCALAR_FUN_ATTR f16 futrts_copysign16(f16 x, f16 y) { return futrts_copysign32((float)x, y); }
SCALAR_FUN_ATTR f16 futrts_mad16(f16 a, f16 b, f16 c) { return a * b + c; }
SCALAR_FUN_ATTR f16 futrts_fma16(f16 a, f16 b, f16 c) { return fmaf(a, b, c); }

#endif

// The CUDA __half type cannot be put in unions for some reason, so we
// use bespoke conversion functions instead.
#ifdef __CUDA_ARCH__
SCALAR_FUN_ATTR int16_t fptobits_f16_i16(f16 x) { return __half_as_ushort(x); }
SCALAR_FUN_ATTR f16 bitstofp_i16_f16(int16_t x) { return __ushort_as_half(x); }
#elif defined(ISPC)
SCALAR_FUN_ATTR int16_t fptobits_f16_i16(f16 x) { varying int16_t y = *((varying int16_t * uniform)&x); return y;
}
SCALAR_FUN_ATTR f16 bitstofp_i16_f16(int16_t x) { varying f16 y = *((varying f16 * uniform)&x); return y; }
#else
SCALAR_FUN_ATTR int16_t fptobits_f16_i16(f16 x) {
  union {
    f16 f;
    int16_t t;
  } p;

  p.f = x;
  return p.t;
}

SCALAR_FUN_ATTR f16 bitstofp_i16_f16(int16_t x) {
  union {
    int16_t f;
    f16 t;
  } p;

  p.f = x;
  return p.t;
}
#endif

#else // No native f16 - emulate.

SCALAR_FUN_ATTR f16 fabs16(f16 x) { return fabs32(x); }
SCALAR_FUN_ATTR f16 fmax16(f16 x, f16 y) { return fmax32(x, y); }
SCALAR_FUN_ATTR f16 fmin16(f16 x, f16 y) { return fmin32(x, y); }
SCALAR_FUN_ATTR f16 fpow16(f16 x, f16 y) { return fpow32(x, y); }
SCALAR_FUN_ATTR bool futrts_isnan16(f16 x) { return futrts_isnan32(x); }
SCALAR_FUN_ATTR bool futrts_isinf16(f16 x) { return futrts_isinf32(x); }
SCALAR_FUN_ATTR f16 futrts_log16(f16 x) { return futrts_log32(x); }
SCALAR_FUN_ATTR f16 futrts_log2_16(f16 x) { return futrts_log2_32(x); }
SCALAR_FUN_ATTR f16 futrts_log10_16(f16 x) { return futrts_log10_32(x); }
SCALAR_FUN_ATTR f16 futrts_log1p_16(f16 x) { return futrts_log1p_32(x); }
SCALAR_FUN_ATTR f16 futrts_sqrt16(f16 x) { return futrts_sqrt32(x); }
SCALAR_FUN_ATTR f16 futrts_rsqrt16(f16 x) { return futrts_rsqrt32(x); }
SCALAR_FUN_ATTR f16 futrts_cbrt16(f16 x) { return futrts_cbrt32(x); }
SCALAR_FUN_ATTR f16 futrts_exp16(f16 x) { return futrts_exp32(x); }
SCALAR_FUN_ATTR f16 futrts_cos16(f16 x) { return futrts_cos32(x); }
SCALAR_FUN_ATTR f16 futrts_cospi16(f16 x) { return futrts_cospi32(x); }
SCALAR_FUN_ATTR f16 futrts_sin16(f16 x) { return futrts_sin32(x); }
SCALAR_FUN_ATTR f16 futrts_sinpi16(f16 x) { return futrts_sinpi32(x); }
SCALAR_FUN_ATTR f16 futrts_tan16(f16 x) { return futrts_tan32(x); }
SCALAR_FUN_ATTR f16 futrts_tanpi16(f16 x) { return futrts_tanpi32(x); }
SCALAR_FUN_ATTR f16 futrts_acos16(f16 x) { return futrts_acos32(x); }
SCALAR_FUN_ATTR f16 futrts_acospi16(f16 x) { return futrts_acospi32(x); }
SCALAR_FUN_ATTR f16 futrts_asin16(f16 x) { return futrts_asin32(x); }
SCALAR_FUN_ATTR f16 futrts_asinpi16(f16 x) { return futrts_asinpi32(x); }
SCALAR_FUN_ATTR f16 futrts_atan16(f16 x) { return futrts_atan32(x); }
SCALAR_FUN_ATTR f16 futrts_atanpi16(f16 x) { return futrts_atanpi32(x); }
SCALAR_FUN_ATTR f16 futrts_cosh16(f16 x) { return futrts_cosh32(x); }
SCALAR_FUN_ATTR f16 futrts_sinh16(f16 x) { return futrts_sinh32(x); }
SCALAR_FUN_ATTR f16 futrts_tanh16(f16 x) { return futrts_tanh32(x); }
SCALAR_FUN_ATTR f16 futrts_acosh16(f16 x) { return futrts_acosh32(x); }
SCALAR_FUN_ATTR f16 futrts_asinh16(f16 x) { return futrts_asinh32(x); }
SCALAR_FUN_ATTR f16 futrts_atanh16(f16 x) { return futrts_atanh32(x); }
SCALAR_FUN_ATTR f16 futrts_atan2_16(f16 x, f16 y) { return futrts_atan2_32(x, y); }
SCALAR_FUN_ATTR f16 futrts_atan2pi_16(f16 x, f16 y) { return futrts_atan2pi_32(x, y); }
SCALAR_FUN_ATTR f16 futrts_hypot16(f16 x, f16 y) { return futrts_hypot32(x, y); }
SCALAR_FUN_ATTR f16 futrts_gamma16(f16 x) { return futrts_gamma32(x); }
SCALAR_FUN_ATTR f16 futrts_lgamma16(f16 x) { return futrts_lgamma32(x); }
SCALAR_FUN_ATTR f16 futrts_erf16(f16 x) { return futrts_erf32(x); }
SCALAR_FUN_ATTR f16 futrts_erfc16(f16 x) { return futrts_erfc32(x); }
SCALAR_FUN_ATTR f16 fmod16(f16 x, f16 y) { return fmod32(x, y); }
SCALAR_FUN_ATTR f16 futrts_round16(f16 x) { return futrts_round32(x); }
SCALAR_FUN_ATTR f16 futrts_floor16(f16 x) { return futrts_floor32(x); }
SCALAR_FUN_ATTR f16 futrts_ceil16(f16 x) { return futrts_ceil32(x); }
SCALAR_FUN_ATTR f16 futrts_nextafter16(f16 x, f16 y) { return halfbits2float(halfbitsnextafter(float2halfbits(x), float2halfbits(y))); }
SCALAR_FUN_ATTR f16 futrts_lerp16(f16 v0, f16 v1, f16 t) { return futrts_lerp32(v0, v1, t); }
SCALAR_FUN_ATTR f16 futrts_ldexp16(f16 x, int32_t y) { return futrts_ldexp32(x, y); }
SCALAR_FUN_ATTR f16 futrts_copysign16(f16 x, f16 y) { return futrts_copysign32((float)x, y); }
SCALAR_FUN_ATTR f16 futrts_mad16(f16 a, f16 b, f16 c) { return futrts_mad32(a, b, c); }
SCALAR_FUN_ATTR f16 futrts_fma16(f16 a, f16 b, f16 c) { return futrts_fma32(a, b, c); }

// Even when we are using an OpenCL that does not support cl_khr_fp16,
// it must still support vload_half for actually creating a
// half-precision number, which can then be efficiently converted to a
// float.  Similarly for vstore_half.
#ifdef __OPENCL_VERSION__

SCALAR_FUN_ATTR int16_t fptobits_f16_i16(f16 x) {
  int16_t y;
  // Violating strict aliasing here.
  vstore_half((float)x, 0, (half*)&y);
  return y;
}

SCALAR_FUN_ATTR f16 bitstofp_i16_f16(int16_t x) {
  return (f16)vload_half(0, (half*)&x);
}

#else
SCALAR_FUN_ATTR int16_t fptobits_f16_i16(f16 x) { return (int16_t)float2halfbits(x); }
SCALAR_FUN_ATTR f16 bitstofp_i16_f16(int16_t x) { return halfbits2float((uint16_t)x); }
SCALAR_FUN_ATTR f16 fsignum16(f16 x) { return futrts_isnan16(x) ? x : (x > 0 ? 1 : 0) - (x < 0 ? 1 : 0); }

#endif

#endif

SCALAR_FUN_ATTR float fpconv_f16_f16(f16 x) { return x; }
SCALAR_FUN_ATTR float fpconv_f16_f32(f16 x) { return x; }
SCALAR_FUN_ATTR f16 fpconv_f32_f16(float x) { return (f16) x; }

#ifdef FUTHARK_F64_ENABLED
SCALAR_FUN_ATTR double fpconv_f16_f64(f16 x) { return (double) x; }
#if defined(ISPC)
SCALAR_FUN_ATTR f16 fpconv_f64_f16(double x) { return (f16) ((float)x); }
#else
SCALAR_FUN_ATTR f16 fpconv_f64_f16(double x) { return (f16) x; }
#endif
#endif

// End of scalar_f16.h.
// Start of atomics64.h

#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP) || defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)

SCALAR_FUN_ATTR int64_t atomic_xchg_i64_global(volatile __global int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_xchg_i64_shared(volatile __local int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_cmpxchg_i64_global(volatile __global int64_t *p,
                                                         int64_t cmp, int64_t val);
SCALAR_FUN_ATTR int64_t atomic_cmpxchg_i64_shared(volatile __local int64_t *p,
                                                        int64_t cmp, int64_t val);
SCALAR_FUN_ATTR int64_t atomic_add_i64_global(volatile __global int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_add_i64_shared(volatile __local int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_smax_i64_global(volatile __global int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_smax_i64_shared(volatile __local int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_smin_i64_global(volatile __global int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_smin_i64_shared(volatile __local int64_t *p, int64_t x);
SCALAR_FUN_ATTR uint64_t atomic_umax_i64_global(volatile __global uint64_t *p, uint64_t x);
SCALAR_FUN_ATTR uint64_t atomic_umax_i64_shared(volatile __local uint64_t *p, uint64_t x);
SCALAR_FUN_ATTR uint64_t atomic_umin_i64_global(volatile __global uint64_t *p, uint64_t x);
SCALAR_FUN_ATTR uint64_t atomic_umin_i64_shared(volatile __local uint64_t *p, uint64_t x);
SCALAR_FUN_ATTR int64_t atomic_and_i64_global(volatile __global int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_and_i64_shared(volatile __local int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_or_i64_global(volatile __global int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_or_i64_shared(volatile __local int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_xor_i64_global(volatile __global int64_t *p, int64_t x);
SCALAR_FUN_ATTR int64_t atomic_xor_i64_shared(volatile __local int64_t *p, int64_t x);

#ifdef FUTHARK_F64_ENABLED
SCALAR_FUN_ATTR double atomic_fadd_f64_global(volatile __global double *p, double x);
SCALAR_FUN_ATTR double atomic_fadd_f64_shared(volatile __local double *p, double x);
#endif

SCALAR_FUN_ATTR int64_t atomic_xchg_i64_global(volatile __global int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicExch((unsigned long long*)p, x);
#else
  return atom_xchg(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_xchg_i64_shared(volatile __local int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicExch((unsigned long long*)p, x);
#else
  return atom_xchg(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_cmpxchg_i64_global(volatile __global int64_t *p,
                                                         int64_t cmp, int64_t val) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicCAS((unsigned long long*)p, cmp, val);
#else
  return atom_cmpxchg(p, cmp, val);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_cmpxchg_i64_shared(volatile __local int64_t *p,
                                                        int64_t cmp, int64_t val) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicCAS((unsigned long long*)p, cmp, val);
#else
  return atom_cmpxchg(p, cmp, val);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_add_i64_global(volatile __global int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAdd((unsigned long long*)p, x);
#else
  return atom_add(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_add_i64_shared(volatile __local int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAdd((unsigned long long*)p, x);
#else
  return atom_add(p, x);
#endif
}

#ifdef FUTHARK_F64_ENABLED

SCALAR_FUN_ATTR double atomic_fadd_f64_global(volatile __global double *p, double x) {
#if defined(FUTHARK_CUDA) && __CUDA_ARCH__ >= 600 || defined(FUTHARK_HIP)
  return atomicAdd((double*)p, x);
  // On OpenCL, use technique from
  // https://pipinspace.github.io/blog/atomic-float-addition-in-opencl.html
#elif defined(cl_nv_pragma_unroll)
  // use hardware-supported atomic addition on Nvidia GPUs with inline
  // PTX assembly
  double ret;
  asm volatile("atom.global.add.f64 %0,[%1],%2;":"=d"(ret):"l"(p),"d"(x):"memory");
  return ret;
#elif __has_builtin(__builtin_amdgcn_global_atomic_fadd_f64)
  // use hardware-supported atomic addition on some AMD GPUs
  return __builtin_amdgcn_global_atomic_fadd_f64(p, x);
#else
  // fallback emulation:
  // https://forums.developer.nvidia.com/t/atomicadd-float-float-atomicmul-float-float/14639/5
  union {int64_t i; double f;} old;
  union {int64_t i; double f;} ret;
  old.f = x;
  while (1) {
    ret.i = atomic_xchg_i64_global((volatile __global int64_t*)p, (int64_t)0);
    ret.f += old.f;
    old.i = atomic_xchg_i64_global((volatile __global int64_t*)p, ret.i);
    if (old.i == 0) {
      break;
    }
  }
  return ret.f;
#endif
}

SCALAR_FUN_ATTR double atomic_fadd_f64_shared(volatile __local double *p, double x) {
#if defined(FUTHARK_CUDA) && __CUDA_ARCH__ >= 600 || defined(FUTHARK_HIP)
  return atomicAdd((double*)p, x);
#else
  union { int64_t i; double f; } old;
  union { int64_t i; double f; } assumed;
  old.f = *p;
  do {
    assumed.f = old.f;
    old.f = old.f + x;
    old.i = atomic_cmpxchg_i64_shared((volatile __local int64_t*)p, assumed.i, old.i);
  } while (assumed.i != old.i);
  return old.f;
#endif
}

#endif

SCALAR_FUN_ATTR int64_t atomic_smax_i64_global(volatile __global int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA)
  return atomicMax((long long*)p, x);
#elif defined(FUTHARK_HIP)
  // Currentely missing in HIP; probably a temporary oversight.
  int64_t old = *p, assumed;
  do {
    assumed = old;
    old = smax64(old, x);
    old = atomic_cmpxchg_i64_global((volatile __global int64_t*)p, assumed, old);
  } while (assumed != old);
  return old;
#else
  return atom_max(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_smax_i64_shared(volatile __local int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA)
  return atomicMax((long long*)p, x);
#elif defined(FUTHARK_HIP)
  // Currentely missing in HIP; probably a temporary oversight.
  int64_t old = *p, assumed;
  do {
    assumed = old;
    old = smax64(old, x);
    old = atomic_cmpxchg_i64_shared((volatile __local int64_t*)p, assumed, old);
  } while (assumed != old);
  return old;
#else
  return atom_max(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_smin_i64_global(volatile __global int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA)
  return atomicMin((long long*)p, x);
#elif defined(FUTHARK_HIP)
  // Currentely missing in HIP; probably a temporary oversight.
  int64_t old = *p, assumed;
  do {
    assumed = old;
    old = smin64(old, x);
    old = atomic_cmpxchg_i64_global((volatile __global int64_t*)p, assumed, old);
  } while (assumed != old);
  return old;
#else
  return atom_min(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_smin_i64_shared(volatile __local int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA)
  return atomicMin((long long*)p, x);
#elif defined(FUTHARK_HIP)
  // Currentely missing in HIP; probably a temporary oversight.
  int64_t old = *p, assumed;
  do {
    assumed = old;
    old = smin64(old, x);
    old = atomic_cmpxchg_i64_shared((volatile __local int64_t*)p, assumed, old);
  } while (assumed != old);
  return old;
#else
  return atom_min(p, x);
#endif
}

SCALAR_FUN_ATTR uint64_t atomic_umax_i64_global(volatile __global uint64_t *p, uint64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMax((unsigned long long*)p, x);
#else
  return atom_max(p, x);
#endif
}

SCALAR_FUN_ATTR uint64_t atomic_umax_i64_shared(volatile __local uint64_t *p, uint64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMax((unsigned long long*)p, x);
#else
  return atom_max(p, x);
#endif
}

SCALAR_FUN_ATTR uint64_t atomic_umin_i64_global(volatile __global uint64_t *p, uint64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMin((unsigned long long*)p, x);
#else
  return atom_min(p, x);
#endif
}

SCALAR_FUN_ATTR uint64_t atomic_umin_i64_shared(volatile __local uint64_t *p, uint64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMin((unsigned long long*)p, x);
#else
  return atom_min(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_and_i64_global(volatile __global int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAnd((unsigned long long*)p, x);
#else
  return atom_and(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_and_i64_shared(volatile __local int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAnd((unsigned long long*)p, x);
#else
  return atom_and(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_or_i64_global(volatile __global int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicOr((unsigned long long*)p, x);
#else
  return atom_or(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_or_i64_shared(volatile __local int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicOr((unsigned long long*)p, x);
#else
  return atom_or(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_xor_i64_global(volatile __global int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicXor((unsigned long long*)p, x);
#else
  return atom_xor(p, x);
#endif
}

SCALAR_FUN_ATTR int64_t atomic_xor_i64_shared(volatile __local int64_t *p, int64_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicXor((unsigned long long*)p, x);
#else
  return atom_xor(p, x);
#endif
}

#endif // defined(FUTHARK_CUDA) || defined(FUTHARK_HIP) || defined(cl_khr_int64_base_atomics) && defined(cl_khr_int64_extended_atomics)

// End of atomics64.h
// Start of atomics.h

SCALAR_FUN_ATTR int32_t atomic_xchg_i32_global(volatile __global int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_xchg_i32_shared(volatile __local int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_cmpxchg_i32_global(volatile __global int32_t *p,
                                                  int32_t cmp, int32_t val);
SCALAR_FUN_ATTR int32_t atomic_cmpxchg_i32_shared(volatile __local int32_t *p,
                                                  int32_t cmp, int32_t val);
SCALAR_FUN_ATTR int32_t atomic_add_i32_global(volatile __global int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_add_i32_shared(volatile __local int32_t *p, int32_t x);
SCALAR_FUN_ATTR float atomic_fadd_f32_global(volatile __global float *p, float x);
SCALAR_FUN_ATTR float atomic_fadd_f32_shared(volatile __local float *p, float x);
SCALAR_FUN_ATTR int32_t atomic_smax_i32_global(volatile __global int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_smax_i32_shared(volatile __local int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_smin_i32_global(volatile __global int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_smin_i32_shared(volatile __local int32_t *p, int32_t x);
SCALAR_FUN_ATTR uint32_t atomic_umax_i32_global(volatile __global uint32_t *p, uint32_t x);
SCALAR_FUN_ATTR uint32_t atomic_umax_i32_shared(volatile __local uint32_t *p, uint32_t x);
SCALAR_FUN_ATTR uint32_t atomic_umin_i32_global(volatile __global uint32_t *p, uint32_t x);
SCALAR_FUN_ATTR uint32_t atomic_umin_i32_shared(volatile __local uint32_t *p, uint32_t x);
SCALAR_FUN_ATTR int32_t atomic_and_i32_global(volatile __global int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_and_i32_shared(volatile __local int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_or_i32_global(volatile __global int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_or_i32_shared(volatile __local int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_xor_i32_global(volatile __global int32_t *p, int32_t x);
SCALAR_FUN_ATTR int32_t atomic_xor_i32_shared(volatile __local int32_t *p, int32_t x);

SCALAR_FUN_ATTR int32_t atomic_xchg_i32_global(volatile __global int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicExch((int32_t*)p, x);
#else
  return atomic_xor(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_xchg_i32_shared(volatile __local int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicExch((int32_t*)p, x);
#else
  return atomic_xor(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_cmpxchg_i32_global(volatile __global int32_t *p,
                                                  int32_t cmp, int32_t val) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicCAS((int32_t*)p, cmp, val);
#else
  return atomic_cmpxchg(p, cmp, val);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_cmpxchg_i32_shared(volatile __local int32_t *p,
                                                  int32_t cmp, int32_t val) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicCAS((int32_t*)p, cmp, val);
#else
  return atomic_cmpxchg(p, cmp, val);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_add_i32_global(volatile __global int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAdd((int32_t*)p, x);
#else
  return atomic_add(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_add_i32_shared(volatile __local int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAdd((int32_t*)p, x);
#else
  return atomic_add(p, x);
#endif
}

SCALAR_FUN_ATTR float atomic_fadd_f32_global(volatile __global float *p, float x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAdd((float*)p, x);
  // On OpenCL, use technique from
  // https://pipinspace.github.io/blog/atomic-float-addition-in-opencl.html
#elif defined(cl_nv_pragma_unroll)
  // use hardware-supported atomic addition on Nvidia GPUs with inline
  // PTX assembly
  float ret;
  asm volatile("atom.global.add.f32 %0,[%1],%2;":"=f"(ret):"l"(p),"f"(x):"memory");
  return ret;
#elif defined(__opencl_c_ext_fp32_global_atomic_add)
  // use hardware-supported atomic addition on some Intel GPUs
  return atomic_fetch_add_explicit((volatile __global atomic_float*)p,
                                   x,
                                   memory_order_relaxed);
#elif __has_builtin(__builtin_amdgcn_global_atomic_fadd_f32)
  // use hardware-supported atomic addition on some AMD GPUs
  return __builtin_amdgcn_global_atomic_fadd_f32(p, x);
#else
  // fallback emulation:
  // https://forums.developer.nvidia.com/t/atomicadd-float-float-atomicmul-float-float/14639/5
  float old = x;
  float ret;
  while ((old=atomic_xchg(p, ret=atomic_xchg(p, 0.0f)+old))!=0.0f);
  return ret;
#endif
}

SCALAR_FUN_ATTR float atomic_fadd_f32_shared(volatile __local float *p, float x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAdd((float*)p, x);
#else
  union { int32_t i; float f; } old;
  union { int32_t i; float f; } assumed;
  old.f = *p;
  do {
    assumed.f = old.f;
    old.f = old.f + x;
    old.i = atomic_cmpxchg_i32_shared((volatile __local int32_t*)p, assumed.i, old.i);
  } while (assumed.i != old.i);
  return old.f;
#endif
}

SCALAR_FUN_ATTR int32_t atomic_smax_i32_global(volatile __global int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMax((int32_t*)p, x);
#else
  return atomic_max(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_smax_i32_shared(volatile __local int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMax((int32_t*)p, x);
#else
  return atomic_max(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_smin_i32_global(volatile __global int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMin((int32_t*)p, x);
#else
  return atomic_min(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_smin_i32_shared(volatile __local int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMin((int32_t*)p, x);
#else
  return atomic_min(p, x);
#endif
}

SCALAR_FUN_ATTR uint32_t atomic_umax_i32_global(volatile __global uint32_t *p, uint32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMax((uint32_t*)p, x);
#else
  return atomic_max(p, x);
#endif
}

SCALAR_FUN_ATTR uint32_t atomic_umax_i32_shared(volatile __local uint32_t *p, uint32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMax((uint32_t*)p, x);
#else
  return atomic_max(p, x);
#endif
}

SCALAR_FUN_ATTR uint32_t atomic_umin_i32_global(volatile __global uint32_t *p, uint32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMin((uint32_t*)p, x);
#else
  return atomic_min(p, x);
#endif
}

SCALAR_FUN_ATTR uint32_t atomic_umin_i32_shared(volatile __local uint32_t *p, uint32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicMin((uint32_t*)p, x);
#else
  return atomic_min(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_and_i32_global(volatile __global int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAnd((int32_t*)p, x);
#else
  return atomic_and(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_and_i32_shared(volatile __local int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicAnd((int32_t*)p, x);
#else
  return atomic_and(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_or_i32_global(volatile __global int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicOr((int32_t*)p, x);
#else
  return atomic_or(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_or_i32_shared(volatile __local int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicOr((int32_t*)p, x);
#else
  return atomic_or(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_xor_i32_global(volatile __global int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicXor((int32_t*)p, x);
#else
  return atomic_xor(p, x);
#endif
}

SCALAR_FUN_ATTR int32_t atomic_xor_i32_shared(volatile __local int32_t *p, int32_t x) {
#if defined(FUTHARK_CUDA) || defined(FUTHARK_HIP)
  return atomicXor((int32_t*)p, x);
#else
  return atomic_xor(p, x);
#endif
}

// End of atomics.h
// Start of atomics16.h

SCALAR_FUN_ATTR int16_t atomic_cmpxchg_i16_global(volatile __global int16_t *p,
                                                  int16_t cmp, int16_t val);
SCALAR_FUN_ATTR int16_t atomic_cmpxchg_i16_shared(volatile __local int16_t *p,
                                                  int16_t cmp, int16_t val);
SCALAR_FUN_ATTR int16_t atomic_add_i16_global(volatile __global int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_add_i16_shared(volatile __local int16_t *p, int16_t x);
SCALAR_FUN_ATTR f16 atomic_fadd_f16_global(volatile __global uint16_t *p, f16 x);
SCALAR_FUN_ATTR f16 atomic_fadd_f16_shared(volatile __local uint16_t *p, f16 x);
SCALAR_FUN_ATTR int16_t atomic_smax_i16_global(volatile __global int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_smax_i16_shared(volatile __local int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_smin_i16_global(volatile __global int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_smin_i16_shared(volatile __local int16_t *p, int16_t x);
SCALAR_FUN_ATTR uint16_t atomic_umax_i16_global(volatile __global uint16_t *p, uint16_t x);
SCALAR_FUN_ATTR uint16_t atomic_umax_i16_shared(volatile __local uint16_t *p, uint16_t x);
SCALAR_FUN_ATTR uint16_t atomic_umin_i16_global(volatile __global uint16_t *p, uint16_t x);
SCALAR_FUN_ATTR uint16_t atomic_umin_i16_shared(volatile __local uint16_t *p, uint16_t x);
SCALAR_FUN_ATTR int16_t atomic_and_i16_global(volatile __global int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_and_i16_shared(volatile __local int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_or_i16_global(volatile __global int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_or_i16_shared(volatile __local int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_xor_i16_global(volatile __global int16_t *p, int16_t x);
SCALAR_FUN_ATTR int16_t atomic_xor_i16_shared(volatile __local int16_t *p, int16_t x);

SCALAR_FUN_ATTR int16_t atomic_cmpxchg_i16_global(volatile __global int16_t *p,
                                                  int16_t cmp, int16_t val) {
  int offset = ((uintptr_t)p >> 1 & 1);
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);

  int shift = offset * 16;
  int32_t mask = 0xffff << shift;
  int32_t shifted_val = val << shift;
  int32_t shifted_cmp = cmp << shift;

  uint32_t old = shifted_cmp;
  uint32_t upd = shifted_val;
  uint32_t got;

  while ((got=atomic_cmpxchg_i32_global(p32, old, upd)) != old) {
    old = got;
    upd = (old & ~mask) | shifted_val;
  }

  return old >> shift;
}

SCALAR_FUN_ATTR int16_t atomic_cmpxchg_i16_shared(volatile __local int16_t *p,
                                                  int16_t cmp, int16_t val) {
  int offset = ((uintptr_t)p >> 1 & 1);
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);

  int shift = offset * 16;
  int32_t mask = 0xffff << shift;
  int32_t shifted_val = val << shift;
  int32_t shifted_cmp = cmp << shift;

  uint32_t old = shifted_cmp;
  uint32_t upd = shifted_val;
  uint32_t got;

  while ((got=atomic_cmpxchg_i32_shared(p32, old, upd)) != old) {
    old = got;
    upd = (old & ~mask) | shifted_val;
  }

  return old >> shift;
}

// Convenience macro for arithmetic.
#define DEFINE_16BIT_ATOMIC(name, T, op)                                \
  SCALAR_FUN_ATTR T                                                     \
  atomic_##name##_i16_global(volatile __global T *p, T val) {           \
    int offset = ((uintptr_t)p >> 1 & 1);                               \
    volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3); \
    int shift = offset * 16;                                            \
    int32_t mask = 0xffff << shift;                                     \
    int32_t old = 0;                                                    \
    int32_t upd = mask & (op(old >> shift, val) << shift);              \
    int32_t saw;                                                        \
    while ((saw=atomic_cmpxchg_i32_global(p32, old, upd)) != old) {     \
      old = saw;                                                        \
      upd = (old & ~mask) | ((op(old >> shift, val)) << shift);         \
    }                                                                   \
    return old >> shift;                                                \
  }                                                                     \
  SCALAR_FUN_ATTR T                                                     \
  atomic_##name##_i16_shared(volatile __local T *p, T val) {            \
    int offset = ((uintptr_t)p >> 1 & 1);                               \
    volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3); \
    int shift = offset * 16;                                            \
    int32_t mask = 0xffff << shift;                                     \
    int32_t old = 0;                                                    \
    int32_t upd = mask & ((op(old >> shift, val)) << shift);            \
    int32_t saw;                                                        \
    while ((saw=atomic_cmpxchg_i32_shared(p32, old, upd)) != old) {     \
      old = saw;                                                        \
      upd = (old & ~mask) | ((op(old >> shift, val)) << shift);         \
    }                                                                   \
    return old >> shift;                                                \
  }

DEFINE_16BIT_ATOMIC(add, int16_t, add16);
DEFINE_16BIT_ATOMIC(smax, int16_t, smax16);
DEFINE_16BIT_ATOMIC(smin, int16_t, smin16);
DEFINE_16BIT_ATOMIC(umax, uint16_t, umax16);
DEFINE_16BIT_ATOMIC(umin, uint16_t, umin16);

SCALAR_FUN_ATTR int16_t atomic_and_i16_global(volatile __global int16_t *p, int16_t val) {
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p >> 1 & 1) * 16;
  int32_t mask = 0xffff << shift;
  return atomic_and_i32_global(p32, ~mask | (val<<shift)) >> shift;
}

SCALAR_FUN_ATTR int16_t atomic_and_i16_shared(volatile __local int16_t *p, int16_t val) {
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p >> 1 & 1) * 16;
  int32_t mask = 0xffff << shift;
  return atomic_and_i32_shared(p32, ~mask | (val<<shift)) >> shift;
}

SCALAR_FUN_ATTR int16_t atomic_or_i16_global(volatile __global int16_t *p, int16_t val) {
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p >> 1 & 1) * 16;
  return atomic_or_i32_global(p32, (uint16_t)val<<shift) >> shift;
}

SCALAR_FUN_ATTR int16_t atomic_or_i16_shared(volatile __local int16_t *p, int16_t val) {
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p >> 1 & 1) * 16;
  return atomic_or_i32_shared(p32, (uint16_t)val<<shift) >> shift;
}

SCALAR_FUN_ATTR int16_t atomic_xor_i16_global(volatile __global int16_t *p, int16_t val) {
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p >> 1 & 1) * 16;
  return atomic_xor_i32_global(p32, (uint16_t)val<<shift) >> shift;
}

SCALAR_FUN_ATTR int16_t atomic_xor_i16_shared(volatile __local int16_t *p, int16_t val) {
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p >> 1 & 1) * 16;
  return atomic_xor_i32_shared(p32, (uint16_t)val<<shift) >> shift;
}

SCALAR_FUN_ATTR f16 atomic_fadd_f16_global(volatile __global uint16_t *p, f16 val) {
  int offset = ((uintptr_t)p >> 1 & 1);
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);
  int shift = offset * 16;
  int32_t mask = 0xffff << shift;
  int32_t old = 0;
  int32_t upd = mask & ((int32_t)fptobits_f16_i16(val) << shift);
  int32_t saw;
  while ((saw=atomic_cmpxchg_i32_global(p32, old, upd)) != old) {
    old = saw;
    upd = (old & ~mask) | (int32_t)fptobits_f16_i16(bitstofp_i16_f16((uint32_t)old >> shift) + val) << shift;
  }
  return bitstofp_i16_f16((uint32_t)old >> shift);
}

SCALAR_FUN_ATTR f16 atomic_fadd_f16_shared(volatile __local uint16_t *p, f16 val) {
  int offset = ((uintptr_t)p >> 1 & 1);
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);
  int shift = offset * 16;
  int32_t mask = 0xffff << shift;
  int32_t old = 0;
  int32_t upd = mask & ((int32_t)fptobits_f16_i16(val) << shift);
  int32_t saw;
  while ((saw=atomic_cmpxchg_i32_shared(p32, old, upd)) != old) {
    old = saw;
    upd = (old & ~mask) | (int32_t)fptobits_f16_i16(bitstofp_i16_f16((uint32_t)old >> shift) + val) << shift;
  }
  return bitstofp_i16_f16((uint32_t)old >> shift);
}

// End of atomics16.h
// Start of atomics8.h

SCALAR_FUN_ATTR int8_t atomic_cmpxchg_i8_global(volatile __global int8_t *p,
                                                int8_t cmp, int8_t val);
SCALAR_FUN_ATTR int8_t atomic_cmpxchg_i8_shared(volatile __local int8_t *p,
                                                int8_t cmp, int8_t val);
SCALAR_FUN_ATTR int8_t atomic_add_i8_global(volatile __global int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_add_i8_shared(volatile __local int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_smax_i8_global(volatile __global int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_smax_i8_shared(volatile __local int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_smin_i8_global(volatile __global int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_smin_i8_shared(volatile __local int8_t *p, int8_t x);
SCALAR_FUN_ATTR uint8_t atomic_umax_i8_global(volatile __global uint8_t *p, uint8_t x);
SCALAR_FUN_ATTR uint8_t atomic_umax_i8_shared(volatile __local uint8_t *p, uint8_t x);
SCALAR_FUN_ATTR uint8_t atomic_umin_i8_global(volatile __global uint8_t *p, uint8_t x);
SCALAR_FUN_ATTR uint8_t atomic_umin_i8_shared(volatile __local uint8_t *p, uint8_t x);
SCALAR_FUN_ATTR int8_t atomic_and_i8_global(volatile __global int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_and_i8_shared(volatile __local int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_or_i8_global(volatile __global int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_or_i8_shared(volatile __local int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_xor_i8_global(volatile __global int8_t *p, int8_t x);
SCALAR_FUN_ATTR int8_t atomic_xor_i8_shared(volatile __local int8_t *p, int8_t x);

SCALAR_FUN_ATTR int8_t atomic_cmpxchg_i8_global(volatile __global int8_t *p,
                                                int8_t cmp, int8_t val) {
  int offset = ((uintptr_t)p & 3);
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);

  int shift = offset * 8;
  int32_t mask = 0xff << shift;
  int32_t shifted_val = val << shift;
  int32_t shifted_cmp = cmp << shift;

  uint32_t old = shifted_cmp;
  uint32_t upd = shifted_val;
  uint32_t got;

  while ((got=atomic_cmpxchg_i32_global(p32, old, upd)) != old) {
    old = got;
    upd = (old & ~mask) | shifted_val;
  }

  return old >> shift;
}

SCALAR_FUN_ATTR int8_t atomic_cmpxchg_i8_shared(volatile __local int8_t *p,
                                                int8_t cmp, int8_t val) {
  int offset = ((uintptr_t)p >> 1 & 3);
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);

  int shift = offset * 8;
  int32_t mask = 0xff << shift;
  int32_t shifted_val = val << shift;
  int32_t shifted_cmp = cmp << shift;

  uint32_t old = shifted_cmp;
  uint32_t upd = shifted_val;
  uint32_t got;

  while ((got=atomic_cmpxchg_i32_shared(p32, old, upd)) != old) {
    old = got;
    upd = (old & ~mask) | shifted_val;
  }

  return old >> shift;
}

// Convenience macro for arithmetic.
#define DEFINE_8BIT_ATOMIC(name, T, op)                                 \
  SCALAR_FUN_ATTR T                                                     \
  atomic_##name##_i8_global(volatile __global T *p, T val) {            \
    int offset = ((uintptr_t)p & 3);                                    \
    volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3); \
    int shift = offset * 8;                                             \
    int32_t mask = 0xff << shift;                                       \
    int32_t old = 0;                                                    \
    int32_t upd = mask & (op(old >> shift, val) << shift);              \
    int32_t saw;                                                        \
    while ((saw=atomic_cmpxchg_i32_global(p32, old, upd)) != old) {     \
      old = saw;                                                        \
      upd = (old & ~mask) | ((op(old >> shift, val)) << shift);         \
    }                                                                   \
    return old >> shift;                                                \
  }                                                                     \
  SCALAR_FUN_ATTR T                                                     \
  atomic_##name##_i8_shared(volatile __local T *p, T val) {             \
    int offset = ((uintptr_t)p & 3);                                    \
    volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3); \
    int shift = offset * 8;                                             \
    int32_t mask = 0xff << shift;                                       \
    int32_t old = 0;                                                    \
    int32_t upd = mask & ((op(old >> shift, val)) << shift);            \
    int32_t saw;                                                        \
    while ((saw=atomic_cmpxchg_i32_shared(p32, old, upd)) != old) {     \
      old = saw;                                                        \
      upd = (old & ~mask) | ((op(old >> shift, val)) << shift);         \
    }                                                                   \
    return old >> shift;                                                \
  }

DEFINE_8BIT_ATOMIC(add, int8_t, add8);
DEFINE_8BIT_ATOMIC(smax, int8_t, smax8);
DEFINE_8BIT_ATOMIC(smin, int8_t, smin8);
DEFINE_8BIT_ATOMIC(umax, uint8_t, umax8);
DEFINE_8BIT_ATOMIC(umin, uint8_t, umin8);

SCALAR_FUN_ATTR int8_t atomic_and_i8_global(volatile __global int8_t *p, int8_t val) {
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p & 3) * 8;
  int32_t mask = 0xff << shift;
  return atomic_and_i32_global(p32, ~mask | (val<<shift)) >> shift;
}

SCALAR_FUN_ATTR int8_t atomic_and_i8_shared(volatile __local int8_t *p, int8_t val) {
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p & 3) * 8;
  int32_t mask = 0xff << shift;
  return atomic_and_i32_shared(p32, ~mask | (val<<shift)) >> shift;
}

SCALAR_FUN_ATTR int8_t atomic_or_i8_global(volatile __global int8_t *p, int8_t val) {
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p & 3) * 8;
  return atomic_or_i32_global(p32, (uint8_t)val<<shift) >> shift;
}

SCALAR_FUN_ATTR int8_t atomic_or_i8_shared(volatile __local int8_t *p, int8_t val) {
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p & 3) * 8;
  return atomic_or_i32_shared(p32, (uint8_t)val<<shift) >> shift;
}

SCALAR_FUN_ATTR int8_t atomic_xor_i8_global(volatile __global int8_t *p, int8_t val) {
  volatile __global int32_t *p32 = (volatile __global int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p & 3) * 8;
  return atomic_xor_i32_global(p32, (uint8_t)val<<shift) >> shift;
}

SCALAR_FUN_ATTR int8_t atomic_xor_i8_shared(volatile __local int8_t *p, int8_t val) {
  volatile __local int32_t *p32 = (volatile __local int32_t*)((uintptr_t)p & ~0x3);
  int shift = ((uintptr_t)p & 3) * 8;
  return atomic_xor_i32_shared(p32, (uint8_t)val<<shift) >> shift;
}

// End of atomics8.h
// Start of transpose.cl

#define GEN_TRANSPOSE_KERNELS(NAME, ELEM_TYPE)                          \
FUTHARK_KERNEL_SIZED(TR_BLOCK_DIM*2, TR_TILE_DIM/TR_ELEMS_PER_THREAD, 1)\
void map_transpose_##NAME(SHARED_MEM_PARAM                              \
                          __global ELEM_TYPE *dst_mem,                  \
                          int64_t dst_offset,                           \
                          __global ELEM_TYPE *src_mem,                  \
                          int64_t src_offset,                           \
                          int32_t num_arrays,                           \
                          int32_t x_elems,                              \
                          int32_t y_elems,                              \
                          int32_t mulx,                                 \
                          int32_t muly,                                 \
                          int32_t repeat_1,                             \
                          int32_t repeat_2) {                           \
  (void)mulx; (void)muly;                                               \
  __local ELEM_TYPE* block = (__local ELEM_TYPE*)shared_mem;            \
  int tblock_id_0 = get_tblock_id(0);                                   \
  int global_id_0 = get_global_id(0);                                   \
  int tblock_id_1 = get_tblock_id(1);                                   \
  int global_id_1 = get_global_id(1);                                   \
  for (int i1 = 0; i1 <= repeat_1; i1++) {                              \
    int tblock_id_2 = get_tblock_id(2);                                 \
    int global_id_2 = get_global_id(2);                                 \
    for (int i2 = 0; i2 <= repeat_2; i2++) {                            \
      if (tblock_id_2 >= num_arrays) { break; }                         \
      int32_t our_array_offset = tblock_id_2 * x_elems * y_elems;       \
      int32_t odata_offset = dst_offset + our_array_offset;             \
      int32_t idata_offset = src_offset + our_array_offset;             \
      int32_t x_index = global_id_0;                                    \
      int32_t y_index = tblock_id_1 * TR_TILE_DIM + get_local_id(1);    \
      if (x_index < x_elems) {                                          \
        for (int32_t j = 0; j < TR_ELEMS_PER_THREAD; j++) {             \
          int32_t index_i = (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)) * x_elems + x_index; \
          if (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD) < y_elems) { \
            block[(get_local_id(1) + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)) * (TR_TILE_DIM+1) + \
                  get_local_id(0)] =                                    \
              src_mem[idata_offset + index_i];                          \
          }                                                             \
        }                                                               \
      }                                                                 \
      barrier_local();                                                  \
      x_index = tblock_id_1 * TR_TILE_DIM + get_local_id(0);            \
      y_index = tblock_id_0 * TR_TILE_DIM + get_local_id(1);            \
      if (x_index < y_elems) {                                          \
        for (int32_t j = 0; j < TR_ELEMS_PER_THREAD; j++) {             \
          int32_t index_out = (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)) * y_elems + x_index; \
          if (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD) < x_elems) { \
            dst_mem[(odata_offset + index_out)] =                       \
              block[get_local_id(0) * (TR_TILE_DIM+1) +                 \
                    get_local_id(1) + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)]; \
          }                                                             \
        }                                                               \
      }                                                                 \
      tblock_id_2 += get_num_tblocks(2);                                \
      global_id_2 += get_global_size(2);                                \
    }                                                                   \
    tblock_id_1 += get_num_tblocks(1);                                  \
    global_id_1 += get_global_size(1);                                  \
  }                                                                     \
}                                                                       \
                                                                        \
FUTHARK_KERNEL_SIZED(TR_BLOCK_DIM, TR_BLOCK_DIM, 1)                     \
void map_transpose_##NAME##_low_height(SHARED_MEM_PARAM                 \
                                                __global ELEM_TYPE *dst_mem, \
                                                int64_t dst_offset,     \
                                                __global ELEM_TYPE *src_mem, \
                                                int64_t src_offset,     \
                                                int32_t num_arrays,     \
                                                int32_t x_elems,        \
                                                int32_t y_elems,        \
                                                int32_t mulx,           \
                                                int32_t muly,           \
                                                int32_t repeat_1,       \
                                                int32_t repeat_2) {     \
  __local ELEM_TYPE* block = (__local ELEM_TYPE*)shared_mem;            \
  int tblock_id_0 = get_tblock_id(0);                                   \
  int global_id_0 = get_global_id(0);                                   \
  int tblock_id_1 = get_tblock_id(1);                                   \
  int global_id_1 = get_global_id(1);                                   \
  for (int i1 = 0; i1 <= repeat_1; i1++) {                              \
    int tblock_id_2 = get_tblock_id(2);                                 \
    int global_id_2 = get_global_id(2);                                 \
    for (int i2 = 0; i2 <= repeat_2; i2++) {                            \
      if (tblock_id_2 >= num_arrays) { break; }                         \
      int32_t our_array_offset = tblock_id_2 * x_elems * y_elems;       \
      int32_t odata_offset = dst_offset + our_array_offset;             \
      int32_t idata_offset = src_offset + our_array_offset;             \
      int32_t x_index =                                                 \
        tblock_id_0 * TR_BLOCK_DIM * mulx +                             \
        get_local_id(0) +                                               \
        get_local_id(1)%mulx * TR_BLOCK_DIM;                            \
      int32_t y_index = tblock_id_1 * TR_BLOCK_DIM + get_local_id(1)/mulx; \
      int32_t index_in = y_index * x_elems + x_index;                   \
      if (x_index < x_elems && y_index < y_elems) {                     \
        block[get_local_id(1) * (TR_BLOCK_DIM+1) + get_local_id(0)] =   \
          src_mem[idata_offset + index_in];                             \
      }                                                                 \
      barrier_local();                                                  \
      x_index = tblock_id_1 * TR_BLOCK_DIM + get_local_id(0)/mulx;      \
      y_index =                                                         \
        tblock_id_0 * TR_BLOCK_DIM * mulx +                             \
        get_local_id(1) +                                               \
        (get_local_id(0)%mulx) * TR_BLOCK_DIM;                          \
      int32_t index_out = y_index * y_elems + x_index;                  \
      if (x_index < y_elems && y_index < x_elems) {                     \
        dst_mem[odata_offset + index_out] =                             \
          block[get_local_id(0) * (TR_BLOCK_DIM+1) + get_local_id(1)];  \
      }                                                                 \
      tblock_id_2 += get_num_tblocks(2);                                \
      global_id_2 += get_global_size(2);                                \
    }                                                                   \
    tblock_id_1 += get_num_tblocks(1);                                  \
    global_id_1 += get_global_size(1);                                  \
  }                                                                     \
}                                                                       \
                                                                        \
FUTHARK_KERNEL_SIZED(TR_BLOCK_DIM, TR_BLOCK_DIM, 1)                     \
void map_transpose_##NAME##_low_width(SHARED_MEM_PARAM                  \
                                      __global ELEM_TYPE *dst_mem,      \
                                      int64_t dst_offset,               \
                                      __global ELEM_TYPE *src_mem,      \
                                      int64_t src_offset,               \
                                      int32_t num_arrays,               \
                                      int32_t x_elems,                  \
                                      int32_t y_elems,                  \
                                      int32_t mulx,                     \
                                      int32_t muly,                     \
                                      int32_t repeat_1,                 \
                                      int32_t repeat_2) {               \
  __local ELEM_TYPE* block = (__local ELEM_TYPE*)shared_mem;            \
  int tblock_id_0 = get_tblock_id(0);                                   \
  int global_id_0 = get_global_id(0);                                   \
  int tblock_id_1 = get_tblock_id(1);                                   \
  int global_id_1 = get_global_id(1);                                   \
  for (int i1 = 0; i1 <= repeat_1; i1++) {                              \
    int tblock_id_2 = get_tblock_id(2);                                 \
    int global_id_2 = get_global_id(2);                                 \
    for (int i2 = 0; i2 <= repeat_2; i2++) {                            \
      if (tblock_id_2 >= num_arrays) { break; }                         \
      int32_t our_array_offset = tblock_id_2 * x_elems * y_elems;       \
      int32_t odata_offset = dst_offset + our_array_offset;             \
      int32_t idata_offset = src_offset + our_array_offset;             \
      int32_t x_index = tblock_id_0 * TR_BLOCK_DIM + get_local_id(0)/muly; \
      int32_t y_index =                                                 \
        tblock_id_1 * TR_BLOCK_DIM * muly +                             \
        get_local_id(1) + (get_local_id(0)%muly) * TR_BLOCK_DIM;        \
      int32_t index_in = y_index * x_elems + x_index;                   \
      if (x_index < x_elems && y_index < y_elems) {                     \
        block[get_local_id(1) * (TR_BLOCK_DIM+1) + get_local_id(0)] =   \
          src_mem[idata_offset + index_in];                             \
      }                                                                 \
      barrier_local();                                                  \
      x_index = tblock_id_1 * TR_BLOCK_DIM * muly +                     \
        get_local_id(0) + (get_local_id(1)%muly) * TR_BLOCK_DIM;        \
      y_index = tblock_id_0 * TR_BLOCK_DIM + get_local_id(1)/muly;      \
      int32_t index_out = y_index * y_elems + x_index;                  \
      if (x_index < y_elems && y_index < x_elems) {                     \
        dst_mem[odata_offset + index_out] =                             \
          block[get_local_id(0) * (TR_BLOCK_DIM+1) + get_local_id(1)];  \
      }                                                                 \
      tblock_id_2 += get_num_tblocks(2);                                \
      global_id_2 += get_num_tblocks(2) * get_local_size(2);            \
    }                                                                   \
    tblock_id_1 += get_num_tblocks(1);                                  \
    global_id_1 += get_num_tblocks(1) * get_local_size(1);              \
  }                                                                     \
}                                                                       \
                                                                        \
FUTHARK_KERNEL_SIZED(TR_BLOCK_DIM*TR_BLOCK_DIM, 1, 1)                   \
void map_transpose_##NAME##_small(SHARED_MEM_PARAM                       \
                                  __global ELEM_TYPE *dst_mem,          \
                                  int64_t dst_offset,                   \
                                  __global ELEM_TYPE *src_mem,          \
                                  int64_t src_offset,                   \
                                  int32_t num_arrays,                   \
                                  int32_t x_elems,                      \
                                  int32_t y_elems,                      \
                                  int32_t mulx,                         \
                                  int32_t muly,                         \
                                  int32_t repeat_1,                     \
                                  int32_t repeat_2) {                   \
  (void)mulx; (void)muly;                                               \
  __local ELEM_TYPE* block = (__local ELEM_TYPE*)shared_mem;            \
  int tblock_id_0 = get_tblock_id(0);                                   \
  int global_id_0 = get_global_id(0);                                   \
  int tblock_id_1 = get_tblock_id(1);                                   \
  int global_id_1 = get_global_id(1);                                   \
  for (int i1 = 0; i1 <= repeat_1; i1++) {                              \
    int tblock_id_2 = get_tblock_id(2);                                 \
    int global_id_2 = get_global_id(2);                                 \
    for (int i2 = 0; i2 <= repeat_2; i2++) {                            \
      int32_t our_array_offset = global_id_0/(y_elems * x_elems) * y_elems * x_elems; \
      int32_t x_index = (global_id_0 % (y_elems * x_elems))/y_elems;    \
      int32_t y_index = global_id_0%y_elems;                            \
      int32_t odata_offset = dst_offset + our_array_offset;             \
      int32_t idata_offset = src_offset + our_array_offset;             \
      int32_t index_in = y_index * x_elems + x_index;                   \
      int32_t index_out = x_index * y_elems + y_index;                  \
      if (global_id_0 < x_elems * y_elems * num_arrays) {               \
        dst_mem[odata_offset + index_out] = src_mem[idata_offset + index_in]; \
      }                                                                 \
      tblock_id_2 += get_num_tblocks(2);                                \
      global_id_2 += get_global_size(2);                                \
    }                                                                   \
    tblock_id_1 += get_num_tblocks(1);                                  \
    global_id_1 += get_global_size(1);                                  \
  }                                                                     \
}                                                                       \
                                                                        \
FUTHARK_KERNEL_SIZED(TR_BLOCK_DIM*2, TR_TILE_DIM/TR_ELEMS_PER_THREAD, 1)\
void map_transpose_##NAME##_large(SHARED_MEM_PARAM                      \
                                  __global ELEM_TYPE *dst_mem,          \
                                  int64_t dst_offset,                   \
                                  __global ELEM_TYPE *src_mem,          \
                                  int64_t src_offset,                   \
                                  int64_t num_arrays,                   \
                                  int64_t x_elems,                      \
                                  int64_t y_elems,                      \
                                  int64_t mulx,                         \
                                  int64_t muly,                         \
                                  int32_t repeat_1,                     \
                                  int32_t repeat_2) {                   \
  (void)mulx; (void)muly;                                               \
  __local ELEM_TYPE* block = (__local ELEM_TYPE*)shared_mem;             \
  int tblock_id_0 = get_tblock_id(0);                                   \
  int global_id_0 = get_global_id(0);                                   \
  int tblock_id_1 = get_tblock_id(1);                                   \
  int global_id_1 = get_global_id(1);                                   \
  for (int i1 = 0; i1 <= repeat_1; i1++) {                              \
    int tblock_id_2 = get_tblock_id(2);                                 \
    int global_id_2 = get_global_id(2);                                 \
    for (int i2 = 0; i2 <= repeat_2; i2++) {                            \
      if (tblock_id_2 >= num_arrays) { break; }                         \
      int64_t our_array_offset = tblock_id_2 * x_elems * y_elems;       \
      int64_t odata_offset = dst_offset + our_array_offset;             \
      int64_t idata_offset = src_offset + our_array_offset;             \
      int64_t x_index = global_id_0;                                    \
      int64_t y_index = tblock_id_1 * TR_TILE_DIM + get_local_id(1);    \
      if (x_index < x_elems) {                                          \
        for (int64_t j = 0; j < TR_ELEMS_PER_THREAD; j++) {             \
          int64_t index_i = (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)) * x_elems + x_index; \
          if (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD) < y_elems) { \
            block[(get_local_id(1) + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)) * (TR_TILE_DIM+1) + \
                  get_local_id(0)] =                                    \
              src_mem[idata_offset + index_i];                          \
          }                                                             \
        }                                                               \
      }                                                                 \
      barrier_local();                                                  \
      x_index = tblock_id_1 * TR_TILE_DIM + get_local_id(0);            \
      y_index = tblock_id_0 * TR_TILE_DIM + get_local_id(1);            \
      if (x_index < y_elems) {                                          \
        for (int64_t j = 0; j < TR_ELEMS_PER_THREAD; j++) {             \
          int64_t index_out = (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)) * y_elems + x_index; \
          if (y_index + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD) < x_elems) { \
            dst_mem[(odata_offset + index_out)] =                       \
              block[get_local_id(0) * (TR_TILE_DIM+1) +                 \
                    get_local_id(1) + j * (TR_TILE_DIM/TR_ELEMS_PER_THREAD)]; \
          }                                                             \
        }                                                               \
      }                                                                 \
      tblock_id_2 += get_num_tblocks(2);                                \
      global_id_2 += get_global_size(2);                                \
    }                                                                   \
    tblock_id_1 += get_num_tblocks(1);                                  \
    global_id_1 += get_global_size(1);                                  \
  }                                                                     \
}                                                                       \

GEN_TRANSPOSE_KERNELS(1b, uint8_t)
GEN_TRANSPOSE_KERNELS(2b, uint16_t)
GEN_TRANSPOSE_KERNELS(4b, uint32_t)
GEN_TRANSPOSE_KERNELS(8b, uint64_t)

// End of transpose.cl
// Start of copy.cl

#define GEN_COPY_KERNEL(NAME, ELEM_TYPE) \
FUTHARK_KERNEL void lmad_copy_##NAME(SHARED_MEM_PARAM                   \
                               __global ELEM_TYPE *dst_mem,             \
                               int64_t dst_offset,                      \
                               __global ELEM_TYPE *src_mem,             \
                               int64_t src_offset,                      \
                               int64_t n,                               \
                               int r,                                   \
                               int64_t shape0, int64_t dst_stride0, int64_t src_stride0, \
                               int64_t shape1, int64_t dst_stride1, int64_t src_stride1, \
                               int64_t shape2, int64_t dst_stride2, int64_t src_stride2, \
                               int64_t shape3, int64_t dst_stride3, int64_t src_stride3, \
                               int64_t shape4, int64_t dst_stride4, int64_t src_stride4, \
                               int64_t shape5, int64_t dst_stride5, int64_t src_stride5, \
                               int64_t shape6, int64_t dst_stride6, int64_t src_stride6, \
                               int64_t shape7, int64_t dst_stride7, int64_t src_stride7) { \
  int64_t gtid = get_global_id(0);                                      \
  int64_t remainder = gtid;                                             \
                                                                        \
  if (gtid >= n) {                                                      \
    return;                                                             \
  }                                                                     \
                                                                        \
  if (r > 0) {                                                          \
    int64_t i = remainder % shape0;                                     \
    dst_offset += i * dst_stride0;                                      \
    src_offset += i * src_stride0;                                      \
    remainder /= shape0;                                                \
  }                                                                     \
  if (r > 1) {                                                          \
    int64_t i = remainder % shape1;                                     \
    dst_offset += i * dst_stride1;                                      \
    src_offset += i * src_stride1;                                      \
    remainder /= shape1;                                                \
  }                                                                     \
  if (r > 2) {                                                          \
    int64_t i = remainder % shape2;                                     \
    dst_offset += i * dst_stride2;                                      \
    src_offset += i * src_stride2;                                      \
    remainder /= shape2;                                                \
  }                                                                     \
  if (r > 3) {                                                          \
    int64_t i = remainder % shape3;                                     \
    dst_offset += i * dst_stride3;                                      \
    src_offset += i * src_stride3;                                      \
    remainder /= shape3;                                                \
  }                                                                     \
  if (r > 4) {                                                          \
    int64_t i = remainder % shape4;                                     \
    dst_offset += i * dst_stride4;                                      \
    src_offset += i * src_stride4;                                      \
    remainder /= shape4;                                                \
  }                                                                     \
  if (r > 5) {                                                          \
    int64_t i = remainder % shape5;                                     \
    dst_offset += i * dst_stride5;                                      \
    src_offset += i * src_stride5;                                      \
    remainder /= shape5;                                                \
  }                                                                     \
  if (r > 6) {                                                          \
    int64_t i = remainder % shape6;                                     \
    dst_offset += i * dst_stride6;                                      \
    src_offset += i * src_stride6;                                      \
    remainder /= shape6;                                                \
  }                                                                     \
  if (r > 7) {                                                          \
    int64_t i = remainder % shape7;                                     \
    dst_offset += i * dst_stride7;                                      \
    src_offset += i * src_stride7;                                      \
    remainder /= shape7;                                                \
  }                                                                     \
                                                                        \
  dst_mem[dst_offset] = src_mem[src_offset];                            \
}

GEN_COPY_KERNEL(1b, uint8_t)
GEN_COPY_KERNEL(2b, uint16_t)
GEN_COPY_KERNEL(4b, uint32_t)
GEN_COPY_KERNEL(8b, uint64_t)

// End of copy.cl



FUTHARK_KERNEL
void builtinzhreplicate_i32zireplicate_7700(__local uint64_t *shared_mem_aligned, int64_t num_elems_7696, int32_t val_7697, int64_t replicate_n_7699, int64_t virt_num_tblocks_7705, int64_t num_tblocks_7706, __global unsigned char *mem_7695)
{
    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    int32_t replicate_ltid_7701;
    int32_t tblock_sizze_7703;
    int32_t replicate_gid_7702;
    int32_t replicate_gtid_7700;
    int32_t phys_tblock_id_7707;
    int32_t iterations_7708;

    replicate_ltid_7701 = get_local_id(0);
    tblock_sizze_7703 = get_local_size(0);
    replicate_gid_7702 = get_tblock_id(0);
    replicate_gtid_7700 = replicate_gid_7702 * tblock_sizze_7703 + replicate_ltid_7701;
    phys_tblock_id_7707 = get_tblock_id(0);
    iterations_7708 = sdiv_up32(sext_i64_i32(virt_num_tblocks_7705) - phys_tblock_id_7707, sext_i64_i32(num_tblocks_7706));
    for (int32_t i_7709 = 0; i_7709 < iterations_7708; i_7709++) {
        int32_t virt_tblock_id_7710;
        int64_t global_tid_7711;
        int64_t slice_7713;
        int64_t rep_i_7712;
        int64_t remnant_7714;

        virt_tblock_id_7710 = phys_tblock_id_7707 + i_7709 * sext_i64_i32(num_tblocks_7706);
        global_tid_7711 = sext_i32_i64(virt_tblock_id_7710) * sext_i32_i64(tblock_sizze_7703) + sext_i32_i64(replicate_ltid_7701);
        slice_7713 = num_elems_7696;
        rep_i_7712 = global_tid_7711;
        remnant_7714 = global_tid_7711 - rep_i_7712;
        if (slt64(global_tid_7711, replicate_n_7699)) {
            ((__global int32_t *) mem_7695)[rep_i_7712] = val_7697;
        }
        barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    }

  error_1:
    return;
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzigpuseq_7748_dim1, 1, 1)
void matmul_bias_relu_sumzigpuseq_7748(__local uint64_t *shared_mem_aligned, __global int *global_failure, __global unsigned char *mem_7678, __global unsigned char *mem_7679)
{
    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7750;
    int32_t tblock_sizze_7753;
    int32_t wave_sizze_7752;
    int32_t block_id_7751;
    int32_t global_tid_7749;
    int64_t tid_7748;
    double defunc_0_reduce_res_7461;

    local_tid_7750 = get_local_id(0);
    tblock_sizze_7753 = get_local_size(0);
    wave_sizze_7752 = LOCKSTEP_WIDTH;
    block_id_7751 = get_tblock_id(0);
    global_tid_7749 = block_id_7751 * tblock_sizze_7753 + local_tid_7750;
    tid_7748 = sext_i32_i64(global_tid_7749);
    defunc_0_reduce_res_7461 = ((__global double *) mem_7678)[(int64_t) 0];
    ((__global double *) mem_7679)[(int64_t) 0] = defunc_0_reduce_res_7461;

  error_0:
    return;
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzigpuseq_8028_dim1, 1, 1)
void matmul_bias_relu_sumzigpuseq_8028(__local uint64_t *shared_mem_aligned, __global int *global_failure, __global unsigned char *mem_7653, __global unsigned char *mem_7654)
{
    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;

    if (*global_failure >= 0)
        return;

    int32_t local_tid_8030;
    int32_t tblock_sizze_8033;
    int32_t wave_sizze_8032;
    int32_t block_id_8031;
    int32_t global_tid_8029;
    int64_t tid_8028;
    double defunc_0_reduce_res_7463;

    local_tid_8030 = get_local_id(0);
    tblock_sizze_8033 = get_local_size(0);
    wave_sizze_8032 = LOCKSTEP_WIDTH;
    block_id_8031 = get_tblock_id(0);
    global_tid_8029 = block_id_8031 * tblock_sizze_8033 + local_tid_8030;
    tid_8028 = sext_i32_i64(global_tid_8029);
    defunc_0_reduce_res_7463 = ((__global double *) mem_7653)[(int64_t) 0];
    ((__global double *) mem_7654)[(int64_t) 0] = defunc_0_reduce_res_7463;

  error_0:
    return;
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegmap_6876_dim1, 1, 1)
void matmul_bias_relu_sumzisegmap_6876(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t k_6138, int64_t m_6139, __global unsigned char *bias_mem_7473, __global unsigned char *mem_7636, __global unsigned char *mem_7647, __global unsigned char *mem_7650)
{
    #define segmap_tblock_sizze_6872 (matmul_bias_relu_sumzisegmap_6876zisegmap_tblock_sizze_6872)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7756;
    int32_t tblock_sizze_7759;
    int32_t wave_sizze_7758;
    int32_t block_id_7757;
    int32_t global_tid_7755;
    int64_t phys_tid_6876;
    int64_t global_tid_7760;
    int64_t slice_7761;
    int64_t gtid_6875;
    int64_t remnant_7762;

    local_tid_7756 = get_local_id(0);
    tblock_sizze_7759 = get_local_size(0);
    wave_sizze_7758 = LOCKSTEP_WIDTH;
    block_id_7757 = get_tblock_id(0);
    global_tid_7755 = block_id_7757 * tblock_sizze_7759 + local_tid_7756;
    phys_tid_6876 = sext_i32_i64(global_tid_7755);
    global_tid_7760 = sext_i32_i64(block_id_7757) * segmap_tblock_sizze_6872 + sext_i32_i64(local_tid_7756);
    slice_7761 = n_6137;
    gtid_6875 = global_tid_7760;
    remnant_7762 = global_tid_7760 - gtid_6875;
    if (slt64(gtid_6875, n_6137)) {
        double defunc_0_f_res_6878;
        double redout_7455;

        // sample_kernel.fut:15:11-19:49
        redout_7455 = 0.0;
        for (int64_t i_7456 = 0; i_7456 < m_6139; i_7456++) {
            double eta_p_6880;
            double defunc_0_reduce_res_6881;
            double redout_7457;
            double defunc_0_f_res_6888;
            bool cond_6889;
            double lifted_lambda_res_6890;
            double defunc_0_op_res_6893;
            double redout_tmp_7763;

            eta_p_6880 = ((__global double *) bias_mem_7473)[i_7456];
            // sample_kernel.fut:8:15-47
            redout_7457 = 0.0;
            for (int64_t i_7458 = 0; i_7458 < k_6138; i_7458++) {
                double eta_p_6882;
                double eta_p_6883;
                double defunc_0_f_res_6884;
                double defunc_0_op_res_6887;
                double redout_tmp_7764;

                eta_p_6882 = ((__global double *) mem_7636)[gtid_6875 + i_7458 * n_6137];
                eta_p_6883 = ((__global double *) mem_7647)[i_7458 + i_7456 * k_6138];
                // sample_kernel.fut:8:36-39
                defunc_0_f_res_6884 = eta_p_6882 * eta_p_6883;
                // sample_kernel.fut:8:22-25
                defunc_0_op_res_6887 = defunc_0_f_res_6884 + redout_7457;
                redout_tmp_7764 = defunc_0_op_res_6887;
                redout_7457 = redout_tmp_7764;
            }
            defunc_0_reduce_res_6881 = redout_7457;
            // sample_kernel.fut:17:34-37
            defunc_0_f_res_6888 = eta_p_6880 + defunc_0_reduce_res_6881;
            // sample_kernel.fut:18:32-58
            cond_6889 = 0.0 < defunc_0_f_res_6888;
            // sample_kernel.fut:18:32-58
            if (cond_6889) {
                lifted_lambda_res_6890 = defunc_0_f_res_6888;
            } else {
                lifted_lambda_res_6890 = 0.0;
            }
            // sample_kernel.fut:19:34-37
            defunc_0_op_res_6893 = lifted_lambda_res_6890 + redout_7455;
            redout_tmp_7763 = defunc_0_op_res_6893;
            redout_7455 = redout_tmp_7763;
        }
        defunc_0_f_res_6878 = redout_7455;
        ((__global double *) mem_7650)[gtid_6875] = defunc_0_f_res_6878;
    }

  error_0:
    return;
    #undef segmap_tblock_sizze_6872
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegmap_6969_dim1, 1, 1)
void matmul_bias_relu_sumzisegmap_6969(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t m_6139, __global unsigned char *bias_mem_7473, __global unsigned char *mem_7492)
{
    #define segmap_tblock_sizze_6964 (matmul_bias_relu_sumzisegmap_6969zisegmap_tblock_sizze_6964)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7910;
    int32_t tblock_sizze_7913;
    int32_t wave_sizze_7912;
    int32_t block_id_7911;
    int32_t global_tid_7909;
    int64_t phys_tid_6969;
    int64_t global_tid_7914;
    int64_t slice_7915;
    int64_t slice_7916;
    int64_t gtid_6967;
    int64_t remnant_7917;
    int64_t gtid_6968;
    int64_t remnant_7918;

    local_tid_7910 = get_local_id(0);
    tblock_sizze_7913 = get_local_size(0);
    wave_sizze_7912 = LOCKSTEP_WIDTH;
    block_id_7911 = get_tblock_id(0);
    global_tid_7909 = block_id_7911 * tblock_sizze_7913 + local_tid_7910;
    phys_tid_6969 = sext_i32_i64(global_tid_7909);
    global_tid_7914 = sext_i32_i64(block_id_7911) * segmap_tblock_sizze_6964 + sext_i32_i64(local_tid_7910);
    slice_7915 = m_6139;
    slice_7916 = n_6137 * slice_7915;
    gtid_6967 = squot64(global_tid_7914, slice_7915);
    remnant_7917 = global_tid_7914 - gtid_6967 * slice_7915;
    gtid_6968 = remnant_7917;
    remnant_7918 = remnant_7917 - gtid_6968;
    if (slt64(gtid_6967, n_6137) && slt64(gtid_6968, m_6139)) {
        double eta_p_6970;
        double defunc_0_reduce_res_6971;
        double defunc_0_f_res_6972;
        bool cond_6973;
        double lifted_lambda_res_6974;

        eta_p_6970 = ((__global double *) bias_mem_7473)[gtid_6968];
        defunc_0_reduce_res_6971 = ((__global double *) mem_7492)[gtid_6967 * m_6139 + gtid_6968];
        // sample_kernel.fut:17:34-37
        defunc_0_f_res_6972 = eta_p_6970 + defunc_0_reduce_res_6971;
        // sample_kernel.fut:18:32-58
        cond_6973 = 0.0 < defunc_0_f_res_6972;
        // sample_kernel.fut:18:32-58
        if (cond_6973) {
            lifted_lambda_res_6974 = defunc_0_f_res_6972;
        } else {
            lifted_lambda_res_6974 = 0.0;
        }
        ((__global double *) mem_7492)[gtid_6967 * m_6139 + gtid_6968] = lifted_lambda_res_6974;
    }

  error_0:
    return;
    #undef segmap_tblock_sizze_6964
}
FUTHARK_KERNEL
void matmul_bias_relu_sumzisegmap_intrablock_6897(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t k_6138, int64_t m_6139, __global unsigned char *a_mem_7471, __global unsigned char *b_mem_7472, __global unsigned char *bias_mem_7473, __global unsigned char *mem_7624)
{
    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *red_arr_mem_7777_backing_0 = &shared_mem[0];
    const int64_t red_arr_mem_7777_backing_0_offset = 0 + ((int64_t) 8 * m_6139 + srem64((int64_t) 8 - srem64((int64_t) 8 * m_6139, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7768;
    int32_t tblock_sizze_7771;
    int32_t wave_sizze_7770;
    int32_t block_id_7769;
    int32_t global_tid_7767;
    int64_t phys_tblock_id_6897;
    int64_t slice_7773;
    int64_t ltid_pre_7772;
    int64_t remnant_7774;
    int64_t slice_7775;
    int64_t gtid_6896;
    int64_t remnant_7776;
    double defunc_0_f_res_6900;
    int64_t phys_tid_6902;
    __local unsigned char *red_arr_mem_7777;
    int64_t gtid_6901;
    double eta_p_6904;
    double defunc_0_reduce_res_6905;
    double redout_7459;
    double defunc_0_f_res_6912;
    bool cond_6913;
    double lifted_lambda_res_6914;
    int32_t offset_7780;
    int32_t skip_waves_7781;
    double eta_p_6915;
    double eta_p_6916;

    local_tid_7768 = get_local_id(0);
    tblock_sizze_7771 = get_local_size(0);
    wave_sizze_7770 = LOCKSTEP_WIDTH;
    block_id_7769 = get_tblock_id(0);
    global_tid_7767 = block_id_7769 * tblock_sizze_7771 + local_tid_7768;
    phys_tblock_id_6897 = sext_i32_i64(block_id_7769);
    slice_7773 = m_6139;
    ltid_pre_7772 = sext_i32_i64(local_tid_7768);
    remnant_7774 = sext_i32_i64(local_tid_7768) - ltid_pre_7772;
    slice_7775 = n_6137;
    gtid_6896 = sext_i32_i64(block_id_7769);
    remnant_7776 = sext_i32_i64(block_id_7769) - gtid_6896;
    phys_tid_6902 = sext_i32_i64(local_tid_7768);
    red_arr_mem_7777 = (__local unsigned char *) red_arr_mem_7777_backing_0;
    gtid_6901 = sext_i32_i64(sext_i64_i32(ltid_pre_7772));
    eta_p_6904 = ((__global double *) bias_mem_7473)[gtid_6901];
    // sample_kernel.fut:8:15-47
    redout_7459 = 0.0;
    for (int64_t i_7460 = 0; i_7460 < k_6138; i_7460++) {
        double eta_p_6906;
        double eta_p_6907;
        double defunc_0_f_res_6908;
        double defunc_0_op_res_6911;
        double redout_tmp_7779;

        eta_p_6906 = ((__global double *) a_mem_7471)[gtid_6896 * k_6138 + i_7460];
        eta_p_6907 = ((__global double *) b_mem_7472)[i_7460 * m_6139 + gtid_6901];
        // sample_kernel.fut:8:36-39
        defunc_0_f_res_6908 = eta_p_6906 * eta_p_6907;
        // sample_kernel.fut:8:22-25
        defunc_0_op_res_6911 = defunc_0_f_res_6908 + redout_7459;
        redout_tmp_7779 = defunc_0_op_res_6911;
        redout_7459 = redout_tmp_7779;
    }
    defunc_0_reduce_res_6905 = redout_7459;
    // sample_kernel.fut:17:34-37
    defunc_0_f_res_6912 = eta_p_6904 + defunc_0_reduce_res_6905;
    // sample_kernel.fut:18:32-58
    cond_6913 = 0.0 < defunc_0_f_res_6912;
    // sample_kernel.fut:18:32-58
    if (cond_6913) {
        lifted_lambda_res_6914 = defunc_0_f_res_6912;
    } else {
        lifted_lambda_res_6914 = 0.0;
    }
    ((__local double *) red_arr_mem_7777)[gtid_6901] = lifted_lambda_res_6914;
    barrier(CLK_LOCAL_MEM_FENCE);
    skip_waves_7781 = 1;
    offset_7780 = 0;
    // participating threads read initial accumulator
    if (slt32(local_tid_7768, sext_i64_i32(m_6139))) {
        eta_p_6915 = ((__local double *) red_arr_mem_7777)[sext_i32_i64(local_tid_7768 + offset_7780)];
    }
    offset_7780 = 1;
    while (slt32(offset_7780, wave_sizze_7770)) {
        if (slt32(local_tid_7768 + offset_7780, sext_i64_i32(m_6139)) && ((local_tid_7768 - squot32(local_tid_7768, wave_sizze_7770) * wave_sizze_7770) & (2 * offset_7780 - 1)) == 0) {
            double defunc_0_op_res_6917;

            // read array element
            eta_p_6916 = ((volatile __local double *) red_arr_mem_7777)[sext_i32_i64(local_tid_7768 + offset_7780)];
            // apply reduction operation
            // sample_kernel.fut:19:34-37
            defunc_0_op_res_6917 = eta_p_6915 + eta_p_6916;
            eta_p_6915 = defunc_0_op_res_6917;
            // write result of operation
            ((volatile __local double *) red_arr_mem_7777)[sext_i32_i64(local_tid_7768)] = eta_p_6915;
        }
        offset_7780 *= 2;
    }
    while (slt32(skip_waves_7781, squot32(sext_i64_i32(m_6139) + wave_sizze_7770 - 1, wave_sizze_7770))) {
        barrier(CLK_LOCAL_MEM_FENCE);
        offset_7780 = skip_waves_7781 * wave_sizze_7770;
        if (slt32(local_tid_7768 + offset_7780, sext_i64_i32(m_6139)) && ((local_tid_7768 - squot32(local_tid_7768, wave_sizze_7770) * wave_sizze_7770) == 0 && (squot32(local_tid_7768, wave_sizze_7770) & (2 * skip_waves_7781 - 1)) == 0)) {
            double defunc_0_op_res_6917;

            // read array element
            eta_p_6916 = ((__local double *) red_arr_mem_7777)[sext_i32_i64(local_tid_7768 + offset_7780)];
            // apply reduction operation
            // sample_kernel.fut:19:34-37
            defunc_0_op_res_6917 = eta_p_6915 + eta_p_6916;
            eta_p_6915 = defunc_0_op_res_6917;
            // write result of operation
            ((__local double *) red_arr_mem_7777)[sext_i32_i64(local_tid_7768)] = eta_p_6915;
        }
        skip_waves_7781 *= 2;
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    barrier(CLK_LOCAL_MEM_FENCE);
    // Save result of reduction.
    defunc_0_f_res_6900 = ((__local double *) red_arr_mem_7777)[(int64_t) 0];
    barrier(CLK_LOCAL_MEM_FENCE);
    if (local_tid_7768 == 0) {
        ((__global double *) mem_7624)[gtid_6896] = defunc_0_f_res_6900;
    }

  error_4:
    return;
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegmap_intrablock_7025_dim1, 1, 1)
void matmul_bias_relu_sumzisegmap_intrablock_7025(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t k_6138, int64_t m_6139, int64_t gridDim_x_7019, int64_t gridDim_y_7020, int64_t full_tiles_7053, int64_t kk_7221, __global unsigned char *a_mem_7471, __global unsigned char *b_mem_7472, __global unsigned char *bias_mem_7473, __global unsigned char *mem_7617)
{
    #define Ty_7008 (matmul_bias_relu_sumzisegmap_intrablock_7025ziTy_7008)
    #define Ry_7009 (matmul_bias_relu_sumzisegmap_intrablock_7025ziRy_7009)
    #define Tk_7010 (matmul_bias_relu_sumzisegmap_intrablock_7025ziTk_7010)
    #define tk_div_tx_7011 (matmul_bias_relu_sumzisegmap_intrablock_7025zitk_div_tx_7011)
    #define TxRx_7013 (matmul_bias_relu_sumzisegmap_intrablock_7025ziTxRx_7013)
    #define a_loc_szz_7016 (matmul_bias_relu_sumzisegmap_intrablock_7025zia_loc_szz_7016)
    #define loop_nonempty_7448 (matmul_bias_relu_sumzisegmap_intrablock_7025ziloop_nonempty_7448)
    #define bytes_7524 (matmul_bias_relu_sumzisegmap_intrablock_7025zibytes_7524)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *color_7686_backing_1 = &shared_mem[0];
    const int64_t color_7686_backing_1_offset = 0 + (bytes_7524 + srem64((int64_t) 8 - srem64(bytes_7524, (int64_t) 8), (int64_t) 8));
    volatile __local unsigned char *color_7685_backing_0 = &shared_mem[color_7686_backing_1_offset];
    const int64_t color_7685_backing_0_offset = color_7686_backing_1_offset + (bytes_7524 + srem64((int64_t) 8 - srem64(bytes_7524, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7785;
    int32_t tblock_sizze_7788;
    int32_t wave_sizze_7787;
    int32_t block_id_7786;
    int32_t global_tid_7784;
    int64_t gid_flat_7025;
    int64_t slice_7791;
    int64_t slice_7792;
    int64_t ltid_pre_7789;
    int64_t remnant_7793;
    int64_t ltid_pre_7790;
    int64_t remnant_7794;
    int64_t slice_7795;
    int64_t slice_7796;
    int64_t gid_y_7023;
    int64_t remnant_7797;
    int64_t gid_x_7024;
    int64_t remnant_7798;
    __local unsigned char *color_7685;
    __local unsigned char *color_7686;
    int64_t iii_7026;
    int64_t jjj_7027;
    double mem_7523[Ry_7009 * Ry_7009];
    int64_t ltid_flat_7041;
    int64_t ltid_y_7040;
    int64_t ltid_x_7039;
    double mem_7504[Ry_7009 * Ry_7009];
    double ext_mem_7562[Ry_7009 * Ry_7009];
    double mem_param_7528[Ry_7009 * Ry_7009];
    bool acc_cert_7222;
    int64_t ltid_flat_7246;
    bool acc_cert_7272;
    int64_t ltid_flat_7296;
    double mem_7584[Ry_7009 * Ry_7009];
    int64_t ltid_flat_7359;
    int64_t ltid_y_7358;
    int64_t ltid_x_7357;
    int64_t binop_x_7374;
    int64_t binop_y_7379;
    double mem_7614[Ry_7009 * Ry_7009];
    int64_t ltid_flat_7417;
    int64_t ltid_y_7416;
    int64_t ltid_x_7415;
    double mem_7595[Ry_7009 * Ry_7009];
    int64_t binop_y_7420;
    int64_t ii_7421;
    int64_t binop_y_7422;
    int64_t jj_7423;
    int64_t slice_7823;
    int64_t slice_7824;
    int64_t reg_tile_i_7821;
    int64_t remnant_7825;
    int64_t reg_tile_i_7822;
    int64_t remnant_7826;
    int64_t tile_dim_start_7827;
    int64_t tile_dim_start_7828;

    local_tid_7785 = get_local_id(0);
    tblock_sizze_7788 = get_local_size(0);
    wave_sizze_7787 = LOCKSTEP_WIDTH;
    block_id_7786 = get_tblock_id(0);
    global_tid_7784 = block_id_7786 * tblock_sizze_7788 + local_tid_7785;
    gid_flat_7025 = sext_i32_i64(block_id_7786);
    slice_7791 = Ty_7008;
    slice_7792 = Ty_7008 * slice_7791;
    ltid_pre_7789 = squot64(sext_i32_i64(local_tid_7785), slice_7791);
    remnant_7793 = sext_i32_i64(local_tid_7785) - ltid_pre_7789 * slice_7791;
    ltid_pre_7790 = remnant_7793;
    remnant_7794 = remnant_7793 - ltid_pre_7790;
    slice_7795 = gridDim_x_7019;
    slice_7796 = gridDim_y_7020 * slice_7795;
    gid_y_7023 = squot64(sext_i32_i64(block_id_7786), slice_7795);
    remnant_7797 = sext_i32_i64(block_id_7786) - gid_y_7023 * slice_7795;
    gid_x_7024 = remnant_7797;
    remnant_7798 = remnant_7797 - gid_x_7024;
    color_7685 = (__local unsigned char *) color_7685_backing_0;
    color_7686 = (__local unsigned char *) color_7686_backing_1;
    iii_7026 = TxRx_7013 * gid_y_7023;
    jjj_7027 = TxRx_7013 * gid_x_7024;
    ltid_flat_7041 = sext_i32_i64(local_tid_7785);
    ltid_y_7040 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
    ltid_x_7039 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
    for (int64_t i_7044 = 0; i_7044 < Ry_7009; i_7044++) {
        for (int64_t i_7047 = 0; i_7047 < Ry_7009; i_7047++) {
            mem_7504[i_7044 * Ry_7009 + i_7047] = 0.0;
        }
    }
    for (int64_t i_0 = 0; i_0 < Ry_7009; i_0++) {
        for (int64_t i_1 = 0; i_1 < Ry_7009; i_1++) {
            mem_7523[i_0 * Ry_7009 + i_1] = mem_7504[i_0 * Ry_7009 + i_1];
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    barrier(CLK_LOCAL_MEM_FENCE);
    for (int32_t i_2 = 0; i_2 < Ry_7009 * Ry_7009; i_2++)
        mem_param_7528[i_2] = mem_7523[i_2];
    for (int64_t i_7054 = 0; i_7054 < full_tiles_7053; i_7054++) {
        int64_t kk_7058;
        bool acc_cert_7059;
        int64_t ltid_flat_7081;
        bool acc_cert_7105;
        int64_t ltid_flat_7127;
        double mem_7555[Ry_7009 * Ry_7009];
        int64_t ltid_flat_7186;
        int64_t ltid_y_7185;
        int64_t ltid_x_7184;
        int64_t binop_x_7199;
        int64_t binop_y_7204;
        double mem_param_tmp_7801[Ry_7009 * Ry_7009];

        kk_7058 = Tk_7010 * i_7054;
        ltid_flat_7081 = sext_i32_i64(local_tid_7785);
        for (int64_t nest_i_7805 = 0; nest_i_7805 < Ry_7009; nest_i_7805++) {
            for (int64_t nest_i_7806 = 0; nest_i_7806 < tk_div_tx_7011; nest_i_7806++) {
                int64_t ltid_seq_7084;
                int64_t ltid_seq_7085;
                int64_t ltid_y_7082;
                int64_t ltid_x_7083;
                int64_t binop_y_7086;
                int64_t k_7087;
                int64_t binop_y_7088;
                int64_t i_7089;
                int64_t gtid_7090;
                int64_t as_transformed_row_seqdim_idx_7091;
                bool cond_7092;
                double as_transformed_row_elem_7093;
                bool cond_7097;
                int64_t as_transformed_row_loc_ind_7098;

                ltid_seq_7084 = nest_i_7805;
                ltid_seq_7085 = nest_i_7806;
                ltid_y_7082 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
                ltid_x_7083 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
                binop_y_7086 = Ty_7008 * ltid_seq_7085;
                k_7087 = ltid_x_7083 + binop_y_7086;
                binop_y_7088 = Ty_7008 * ltid_seq_7084;
                i_7089 = ltid_y_7082 + binop_y_7088;
                gtid_7090 = iii_7026 + i_7089;
                as_transformed_row_seqdim_idx_7091 = kk_7058 + k_7087;
                cond_7092 = slt64(gtid_7090, n_6137);
                if (cond_7092) {
                    double A_elem_7095 = ((__global double *) a_mem_7471)[gtid_7090 * k_6138 + as_transformed_row_seqdim_idx_7091];

                    as_transformed_row_elem_7093 = A_elem_7095;
                } else {
                    as_transformed_row_elem_7093 = 0.0;
                }
                cond_7097 = slt64(k_7087, Tk_7010);
                if (cond_7097) {
                    int64_t binop_y_7099;
                    int64_t x_7100;

                    binop_y_7099 = Tk_7010 * i_7089;
                    x_7100 = k_7087 + binop_y_7099;
                    as_transformed_row_loc_ind_7098 = x_7100;
                } else {
                    as_transformed_row_loc_ind_7098 = (int64_t) -1;
                }
                // UpdateAcc
                if (sle64((int64_t) 0, as_transformed_row_loc_ind_7098) && slt64(as_transformed_row_loc_ind_7098, a_loc_szz_7016)) {
                    ((__local double *) color_7686)[as_transformed_row_loc_ind_7098] = as_transformed_row_elem_7093;
                }
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        ltid_flat_7127 = sext_i32_i64(local_tid_7785);
        for (int64_t nest_i_7807 = 0; nest_i_7807 < Ry_7009; nest_i_7807++) {
            for (int64_t nest_i_7808 = 0; nest_i_7808 < tk_div_tx_7011; nest_i_7808++) {
                int64_t ltid_seq_7130;
                int64_t ltid_seq_7131;
                int64_t ltid_y_7128;
                int64_t ltid_x_7129;
                int64_t binop_y_7132;
                int64_t k_7133;
                int64_t binop_y_7134;
                int64_t i_7135;
                int64_t gtid_7136;
                int64_t as_transformed_row_seqdim_idx_7137;
                bool cond_7138;
                double as_transformed_row_elem_7139;
                bool cond_7143;
                int64_t as_transformed_row_loc_ind_7144;

                ltid_seq_7130 = nest_i_7807;
                ltid_seq_7131 = nest_i_7808;
                ltid_y_7128 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
                ltid_x_7129 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
                binop_y_7132 = Ty_7008 * ltid_seq_7131;
                k_7133 = ltid_y_7128 + binop_y_7132;
                binop_y_7134 = Ty_7008 * ltid_seq_7130;
                i_7135 = ltid_x_7129 + binop_y_7134;
                gtid_7136 = jjj_7027 + i_7135;
                as_transformed_row_seqdim_idx_7137 = kk_7058 + k_7133;
                cond_7138 = slt64(gtid_7136, m_6139);
                if (cond_7138) {
                    double A_elem_7141 = ((__global double *) b_mem_7472)[as_transformed_row_seqdim_idx_7137 * m_6139 + gtid_7136];

                    as_transformed_row_elem_7139 = A_elem_7141;
                } else {
                    as_transformed_row_elem_7139 = 0.0;
                }
                cond_7143 = slt64(k_7133, Tk_7010);
                if (cond_7143) {
                    int64_t binop_y_7145;
                    int64_t x_7146;

                    binop_y_7145 = TxRx_7013 * k_7133;
                    x_7146 = i_7135 + binop_y_7145;
                    as_transformed_row_loc_ind_7144 = x_7146;
                } else {
                    as_transformed_row_loc_ind_7144 = (int64_t) -1;
                }
                // UpdateAcc
                if (sle64((int64_t) 0, as_transformed_row_loc_ind_7144) && slt64(as_transformed_row_loc_ind_7144, a_loc_szz_7016)) {
                    ((__local double *) color_7685)[as_transformed_row_loc_ind_7144] = as_transformed_row_elem_7139;
                }
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        ltid_flat_7186 = sext_i32_i64(local_tid_7785);
        ltid_y_7185 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
        ltid_x_7184 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
        binop_x_7199 = Ry_7009 * ltid_y_7185;
        binop_y_7204 = Ry_7009 * ltid_x_7184;
        for (int64_t i_7189 = 0; i_7189 < Tk_7010; i_7189++) {
            int64_t binop_y_7206 = TxRx_7013 * i_7189;

            for (int64_t i_7193 = 0; i_7193 < Ry_7009; i_7193++) {
                int64_t binop_x_7200;
                int64_t binop_y_7201;
                int64_t as_transformed_row_loc_ind_64_7202;
                double as_transformed_row_loc_elem_7203;

                binop_x_7200 = i_7193 + binop_x_7199;
                binop_y_7201 = Tk_7010 * binop_x_7200;
                as_transformed_row_loc_ind_64_7202 = i_7189 + binop_y_7201;
                if (loop_nonempty_7448) {
                    double x_7449 = ((__local double *) color_7686)[as_transformed_row_loc_ind_64_7202];

                    as_transformed_row_loc_elem_7203 = x_7449;
                } else {
                    as_transformed_row_loc_elem_7203 = 0.0;
                }
                for (int64_t i_7196 = 0; i_7196 < Ry_7009; i_7196++) {
                    int64_t binop_x_7205;
                    int64_t as_transformed_row_loc_ind_64_7207;
                    double as_transformed_row_loc_elem_7208;
                    double c_7209;
                    double defunc_0_f_res_7212;
                    double defunc_0_op_res_7215;

                    binop_x_7205 = i_7196 + binop_y_7204;
                    as_transformed_row_loc_ind_64_7207 = binop_x_7205 + binop_y_7206;
                    as_transformed_row_loc_elem_7208 = ((__local double *) color_7685)[as_transformed_row_loc_ind_64_7207];
                    c_7209 = mem_param_7528[i_7193 * Ry_7009 + i_7196];
                    // sample_kernel.fut:8:36-39
                    defunc_0_f_res_7212 = as_transformed_row_loc_elem_7203 * as_transformed_row_loc_elem_7208;
                    // sample_kernel.fut:8:22-25
                    defunc_0_op_res_7215 = c_7209 + defunc_0_f_res_7212;
                    mem_param_7528[i_7193 * Ry_7009 + i_7196] = defunc_0_op_res_7215;
                }
            }
        }
        for (int64_t i_0 = 0; i_0 < Ry_7009; i_0++) {
            for (int64_t i_1 = 0; i_1 < Ry_7009; i_1++) {
                mem_7555[i_0 * Ry_7009 + i_1] = mem_param_7528[i_0 * Ry_7009 + i_1];
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        for (int32_t i_3 = 0; i_3 < Ry_7009 * Ry_7009; i_3++)
            mem_param_tmp_7801[i_3] = mem_7555[i_3];
        for (int32_t i_4 = 0; i_4 < Ry_7009 * Ry_7009; i_4++)
            mem_param_7528[i_4] = mem_param_tmp_7801[i_4];
    }
    for (int32_t i_5 = 0; i_5 < Ry_7009 * Ry_7009; i_5++)
        ext_mem_7562[i_5] = mem_param_7528[i_5];
    ltid_flat_7246 = sext_i32_i64(local_tid_7785);
    for (int64_t nest_i_7812 = 0; nest_i_7812 < Ry_7009; nest_i_7812++) {
        for (int64_t nest_i_7813 = 0; nest_i_7813 < tk_div_tx_7011; nest_i_7813++) {
            int64_t ltid_seq_7249;
            int64_t ltid_seq_7250;
            int64_t ltid_y_7247;
            int64_t ltid_x_7248;
            int64_t binop_y_7251;
            int64_t k_7252;
            int64_t binop_y_7253;
            int64_t i_7254;
            int64_t gtid_7255;
            int64_t as_transformed_row_seqdim_idx_7256;
            bool binop_x_7257;
            bool binop_y_7258;
            bool cond_7259;
            double as_transformed_row_elem_7260;
            bool cond_7264;
            int64_t as_transformed_row_loc_ind_7265;

            ltid_seq_7249 = nest_i_7812;
            ltid_seq_7250 = nest_i_7813;
            ltid_y_7247 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
            ltid_x_7248 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
            binop_y_7251 = Ty_7008 * ltid_seq_7250;
            k_7252 = ltid_x_7248 + binop_y_7251;
            binop_y_7253 = Ty_7008 * ltid_seq_7249;
            i_7254 = ltid_y_7247 + binop_y_7253;
            gtid_7255 = iii_7026 + i_7254;
            as_transformed_row_seqdim_idx_7256 = kk_7221 + k_7252;
            binop_x_7257 = slt64(gtid_7255, n_6137);
            binop_y_7258 = slt64(as_transformed_row_seqdim_idx_7256, k_6138);
            cond_7259 = binop_x_7257 && binop_y_7258;
            if (cond_7259) {
                double A_elem_7262 = ((__global double *) a_mem_7471)[gtid_7255 * k_6138 + as_transformed_row_seqdim_idx_7256];

                as_transformed_row_elem_7260 = A_elem_7262;
            } else {
                as_transformed_row_elem_7260 = 0.0;
            }
            cond_7264 = slt64(k_7252, Tk_7010);
            if (cond_7264) {
                int64_t binop_y_7266;
                int64_t x_7267;

                binop_y_7266 = Tk_7010 * i_7254;
                x_7267 = k_7252 + binop_y_7266;
                as_transformed_row_loc_ind_7265 = x_7267;
            } else {
                as_transformed_row_loc_ind_7265 = (int64_t) -1;
            }
            // UpdateAcc
            if (sle64((int64_t) 0, as_transformed_row_loc_ind_7265) && slt64(as_transformed_row_loc_ind_7265, a_loc_szz_7016)) {
                ((__local double *) color_7686)[as_transformed_row_loc_ind_7265] = as_transformed_row_elem_7260;
            }
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    ltid_flat_7296 = sext_i32_i64(local_tid_7785);
    for (int64_t nest_i_7814 = 0; nest_i_7814 < Ry_7009; nest_i_7814++) {
        for (int64_t nest_i_7815 = 0; nest_i_7815 < tk_div_tx_7011; nest_i_7815++) {
            int64_t ltid_seq_7299;
            int64_t ltid_seq_7300;
            int64_t ltid_y_7297;
            int64_t ltid_x_7298;
            int64_t binop_y_7301;
            int64_t k_7302;
            int64_t binop_y_7303;
            int64_t i_7304;
            int64_t gtid_7305;
            int64_t as_transformed_row_seqdim_idx_7306;
            bool binop_x_7307;
            bool binop_y_7308;
            bool cond_7309;
            double as_transformed_row_elem_7310;
            bool cond_7314;
            int64_t as_transformed_row_loc_ind_7315;

            ltid_seq_7299 = nest_i_7814;
            ltid_seq_7300 = nest_i_7815;
            ltid_y_7297 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
            ltid_x_7298 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
            binop_y_7301 = Ty_7008 * ltid_seq_7300;
            k_7302 = ltid_y_7297 + binop_y_7301;
            binop_y_7303 = Ty_7008 * ltid_seq_7299;
            i_7304 = ltid_x_7298 + binop_y_7303;
            gtid_7305 = jjj_7027 + i_7304;
            as_transformed_row_seqdim_idx_7306 = kk_7221 + k_7302;
            binop_x_7307 = slt64(gtid_7305, m_6139);
            binop_y_7308 = slt64(as_transformed_row_seqdim_idx_7306, k_6138);
            cond_7309 = binop_x_7307 && binop_y_7308;
            if (cond_7309) {
                double A_elem_7312 = ((__global double *) b_mem_7472)[as_transformed_row_seqdim_idx_7306 * m_6139 + gtid_7305];

                as_transformed_row_elem_7310 = A_elem_7312;
            } else {
                as_transformed_row_elem_7310 = 0.0;
            }
            cond_7314 = slt64(k_7302, Tk_7010);
            if (cond_7314) {
                int64_t binop_y_7316;
                int64_t x_7317;

                binop_y_7316 = TxRx_7013 * k_7302;
                x_7317 = i_7304 + binop_y_7316;
                as_transformed_row_loc_ind_7315 = x_7317;
            } else {
                as_transformed_row_loc_ind_7315 = (int64_t) -1;
            }
            // UpdateAcc
            if (sle64((int64_t) 0, as_transformed_row_loc_ind_7315) && slt64(as_transformed_row_loc_ind_7315, a_loc_szz_7016)) {
                ((__local double *) color_7685)[as_transformed_row_loc_ind_7315] = as_transformed_row_elem_7310;
            }
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    ltid_flat_7359 = sext_i32_i64(local_tid_7785);
    ltid_y_7358 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
    ltid_x_7357 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
    binop_x_7374 = Ry_7009 * ltid_y_7358;
    binop_y_7379 = Ry_7009 * ltid_x_7357;
    for (int64_t i_7362 = 0; i_7362 < Tk_7010; i_7362++) {
        int64_t cmpop_x_7364;
        bool cond_7365;
        int64_t binop_y_7381;

        cmpop_x_7364 = kk_7221 + i_7362;
        cond_7365 = slt64(cmpop_x_7364, k_6138);
        binop_y_7381 = TxRx_7013 * i_7362;
        if (cond_7365) {
            for (int64_t i_7368 = 0; i_7368 < Ry_7009; i_7368++) {
                int64_t binop_x_7375;
                int64_t binop_y_7376;
                int64_t as_transformed_row_loc_ind_64_7377;
                double as_transformed_row_loc_elem_7378;

                binop_x_7375 = i_7368 + binop_x_7374;
                binop_y_7376 = Tk_7010 * binop_x_7375;
                as_transformed_row_loc_ind_64_7377 = i_7362 + binop_y_7376;
                if (loop_nonempty_7448) {
                    double x_7446 = ((__local double *) color_7686)[as_transformed_row_loc_ind_64_7377];

                    as_transformed_row_loc_elem_7378 = x_7446;
                } else {
                    as_transformed_row_loc_elem_7378 = 0.0;
                }
                for (int64_t i_7371 = 0; i_7371 < Ry_7009; i_7371++) {
                    int64_t binop_x_7380;
                    int64_t as_transformed_row_loc_ind_64_7382;
                    double as_transformed_row_loc_elem_7383;
                    double c_7384;
                    double defunc_0_f_res_7387;
                    double defunc_0_op_res_7390;

                    binop_x_7380 = i_7371 + binop_y_7379;
                    as_transformed_row_loc_ind_64_7382 = binop_x_7380 + binop_y_7381;
                    as_transformed_row_loc_elem_7383 = ((__local double *) color_7685)[as_transformed_row_loc_ind_64_7382];
                    c_7384 = ext_mem_7562[i_7368 * Ry_7009 + i_7371];
                    // sample_kernel.fut:8:36-39
                    defunc_0_f_res_7387 = as_transformed_row_loc_elem_7378 * as_transformed_row_loc_elem_7383;
                    // sample_kernel.fut:8:22-25
                    defunc_0_op_res_7390 = c_7384 + defunc_0_f_res_7387;
                    ext_mem_7562[i_7368 * Ry_7009 + i_7371] = defunc_0_op_res_7390;
                }
            }
        }
    }
    for (int64_t i_0 = 0; i_0 < Ry_7009; i_0++) {
        for (int64_t i_1 = 0; i_1 < Ry_7009; i_1++) {
            mem_7584[i_0 * Ry_7009 + i_1] = ext_mem_7562[i_0 * Ry_7009 + i_1];
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    ltid_flat_7417 = sext_i32_i64(local_tid_7785);
    ltid_y_7416 = sext_i32_i64(sext_i64_i32(ltid_pre_7789));
    ltid_x_7415 = sext_i32_i64(sext_i64_i32(ltid_pre_7790));
    binop_y_7420 = Ry_7009 * ltid_y_7416;
    ii_7421 = iii_7026 + binop_y_7420;
    binop_y_7422 = Ry_7009 * ltid_x_7415;
    jj_7423 = jjj_7027 + binop_y_7422;
    for (int64_t i_7425 = 0; i_7425 < Ry_7009; i_7425++) {
        int64_t gtid_7432;
        bool binop_x_7434;

        gtid_7432 = ii_7421 + i_7425;
        binop_x_7434 = slt64(gtid_7432, n_6137);
        for (int64_t i_7428 = 0; i_7428 < Ry_7009; i_7428++) {
            int64_t gtid_7433;
            bool binop_y_7435;
            bool cond_7436;
            double res_elem_7437;

            gtid_7433 = jj_7423 + i_7428;
            binop_y_7435 = slt64(gtid_7433, m_6139);
            cond_7436 = binop_x_7434 && binop_y_7435;
            if (cond_7436) {
                double redomap_elm_7430;
                double eta_p_7438;
                double defunc_0_f_res_7439;
                bool cond_7440;
                double lifted_lambda_res_7441;

                redomap_elm_7430 = mem_7584[i_7425 * Ry_7009 + i_7428];
                eta_p_7438 = ((__global double *) bias_mem_7473)[gtid_7433];
                // sample_kernel.fut:17:34-37
                defunc_0_f_res_7439 = redomap_elm_7430 + eta_p_7438;
                // sample_kernel.fut:18:32-58
                cond_7440 = 0.0 < defunc_0_f_res_7439;
                // sample_kernel.fut:18:32-58
                if (cond_7440) {
                    lifted_lambda_res_7441 = defunc_0_f_res_7439;
                } else {
                    lifted_lambda_res_7441 = 0.0;
                }
                res_elem_7437 = lifted_lambda_res_7441;
            } else {
                res_elem_7437 = 0.0;
            }
            mem_7595[i_7425 * Ry_7009 + i_7428] = res_elem_7437;
        }
    }
    for (int64_t i_0 = 0; i_0 < Ry_7009; i_0++) {
        for (int64_t i_1 = 0; i_1 < Ry_7009; i_1++) {
            mem_7614[i_0 * Ry_7009 + i_1] = mem_7595[i_0 * Ry_7009 + i_1];
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    slice_7823 = Ty_7008;
    slice_7824 = Ty_7008 * slice_7823;
    reg_tile_i_7821 = squot64(sext_i32_i64(local_tid_7785), slice_7823);
    remnant_7825 = sext_i32_i64(local_tid_7785) - reg_tile_i_7821 * slice_7823;
    reg_tile_i_7822 = remnant_7825;
    remnant_7826 = remnant_7825 - reg_tile_i_7822;
    tile_dim_start_7827 = Ry_7009 * (Ty_7008 * gid_y_7023 + reg_tile_i_7821);
    tile_dim_start_7828 = Ry_7009 * (Ty_7008 * gid_x_7024 + reg_tile_i_7822);
    for (int64_t nest_i_7829 = 0; nest_i_7829 < Ry_7009; nest_i_7829++) {
        for (int64_t nest_i_7830 = 0; nest_i_7830 < Ry_7009; nest_i_7830++) {
            if (slt64(tile_dim_start_7827 + nest_i_7829, n_6137) && slt64(tile_dim_start_7828 + nest_i_7830, m_6139)) {
                double tmp_7831 = mem_7614[nest_i_7829 * Ry_7009 + nest_i_7830];

                ((__global double *) mem_7617)[(tile_dim_start_7827 + nest_i_7829) * m_6139 + (tile_dim_start_7828 + nest_i_7830)] = tmp_7831;
            }
        }
    }

  error_9:
    return;
    #undef Ty_7008
    #undef Ry_7009
    #undef Tk_7010
    #undef tk_div_tx_7011
    #undef TxRx_7013
    #undef a_loc_szz_7016
    #undef loop_nonempty_7448
    #undef bytes_7524
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegred_large_6953_dim1, 1, 1)
void matmul_bias_relu_sumzisegred_large_6953(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t k_6138, int64_t m_6139, int64_t num_tblocks_6947, int64_t blocks_per_segment_7863, int64_t q_7864, int64_t num_virtblocks_7865, int64_t threads_per_segment_7866, __global unsigned char *a_mem_7471, __global unsigned char *mem_7484, __global unsigned char *mem_7492, __global unsigned char *segred_tmp_mem_7867, __global unsigned char *counters_mem_7869)
{
    #define segred_tblock_sizze_6946 (matmul_bias_relu_sumzisegred_large_6953zisegred_tblock_sizze_6946)
    #define chunk_sizze_7832 (matmul_bias_relu_sumzisegred_large_6953zichunk_sizze_7832)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *sync_arr_mem_7878_backing_1 = &shared_mem[0];
    const int64_t sync_arr_mem_7878_backing_1_offset = 0 + 8;
    volatile __local unsigned char *red_arr_f64_mem_7876_backing_0 = &shared_mem[sync_arr_mem_7878_backing_1_offset];
    const int64_t red_arr_f64_mem_7876_backing_0_offset = sync_arr_mem_7878_backing_1_offset + ((int64_t) 8 * segred_tblock_sizze_6946 + srem64((int64_t) 8 - srem64((int64_t) 8 * segred_tblock_sizze_6946, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7872;
    int32_t tblock_sizze_7875;
    int32_t wave_sizze_7874;
    int32_t block_id_7873;
    int32_t global_tid_7871;
    int64_t phys_tid_6953;
    __local unsigned char *red_arr_f64_mem_7876;
    __local unsigned char *sync_arr_mem_7878;
    int32_t phys_tblock_id_7880;
    int32_t iterations_7881;

    local_tid_7872 = get_local_id(0);
    tblock_sizze_7875 = get_local_size(0);
    wave_sizze_7874 = LOCKSTEP_WIDTH;
    block_id_7873 = get_tblock_id(0);
    global_tid_7871 = block_id_7873 * tblock_sizze_7875 + local_tid_7872;
    phys_tid_6953 = sext_i32_i64(global_tid_7871);
    red_arr_f64_mem_7876 = (__local unsigned char *) red_arr_f64_mem_7876_backing_0;
    sync_arr_mem_7878 = (__local unsigned char *) sync_arr_mem_7878_backing_1;
    phys_tblock_id_7880 = get_tblock_id(0);
    iterations_7881 = sdiv_up32(sext_i64_i32(num_virtblocks_7865) - phys_tblock_id_7880, sext_i64_i32(num_tblocks_6947));
    for (int32_t i_7882 = 0; i_7882 < iterations_7881; i_7882++) {
        int32_t virt_tblock_id_7883;
        int64_t flat_segment_id_7884;
        int64_t global_tid_7885;
        int64_t slice_7886;
        int64_t slice_7887;
        int64_t gtid_6950;
        int64_t remnant_7888;
        int64_t gtid_6951;
        int64_t remnant_7889;
        int64_t gtid_6952;
        double eta_p_block_res_acc_7890;
        double eta_p_6959;
        double eta_p_6960;
        int64_t tblock_id_in_segment_7894;
        int64_t block_base_offset_7895;
        int32_t offset_7898;
        int32_t skip_waves_7899;
        double eta_p_7891;
        double eta_p_7892;

        virt_tblock_id_7883 = phys_tblock_id_7880 + i_7882 * sext_i64_i32(num_tblocks_6947);
        flat_segment_id_7884 = squot64(sext_i32_i64(virt_tblock_id_7883), blocks_per_segment_7863);
        global_tid_7885 = srem64(sext_i32_i64(virt_tblock_id_7883) * segred_tblock_sizze_6946 + sext_i32_i64(local_tid_7872), threads_per_segment_7866);
        slice_7886 = m_6139;
        slice_7887 = n_6137 * slice_7886;
        gtid_6950 = squot64(flat_segment_id_7884, slice_7886);
        remnant_7888 = flat_segment_id_7884 - gtid_6950 * slice_7886;
        gtid_6951 = remnant_7888;
        remnant_7889 = remnant_7888 - gtid_6951;
        // ne-initialise the outer (per-block) accumulator(s)
        eta_p_block_res_acc_7890 = 0.0;
        tblock_id_in_segment_7894 = squot64(global_tid_7885, segred_tblock_sizze_6946);
        block_base_offset_7895 = tblock_id_in_segment_7894 * q_7864 * segred_tblock_sizze_6946;
        for (int64_t i_7896 = 0; i_7896 < q_7864; i_7896++) {
            int64_t block_offset_7897 = block_base_offset_7895 + i_7896 * segred_tblock_sizze_6946;

            gtid_6952 = global_tid_7885 + threads_per_segment_7866 * i_7896;
            if (slt64(gtid_6952, k_6138)) {
                double eta_p_6956;
                double eta_p_6957;
                double defunc_0_f_res_6958;
                double defunc_0_op_res_6961;

                // apply map function(s)
                // apply map function
                eta_p_6956 = ((__global double *) a_mem_7471)[gtid_6950 * k_6138 + gtid_6952];
                eta_p_6957 = ((__global double *) mem_7484)[gtid_6952 + gtid_6951 * k_6138];
                // sample_kernel.fut:8:36-39
                defunc_0_f_res_6958 = eta_p_6956 * eta_p_6957;
                // load accumulator(s)
                eta_p_6959 = eta_p_block_res_acc_7890;
                // load next value(s)
                eta_p_6960 = defunc_0_f_res_6958;
                // apply reduction operator(s)
                // sample_kernel.fut:8:22-25
                defunc_0_op_res_6961 = eta_p_6959 + eta_p_6960;
                // store in accumulator(s)
                eta_p_block_res_acc_7890 = defunc_0_op_res_6961;
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        // store accs. prims go in lmem; non-prims in params (in global mem)
        ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872)] = eta_p_block_res_acc_7890;
        barrier(CLK_LOCAL_MEM_FENCE);
        skip_waves_7899 = 1;
        offset_7898 = 0;
        // participating threads read initial accumulator
        if (slt32(local_tid_7872, sext_i64_i32(segred_tblock_sizze_6946))) {
            eta_p_7891 = ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872 + offset_7898)];
        }
        offset_7898 = 1;
        while (slt32(offset_7898, wave_sizze_7874)) {
            if (slt32(local_tid_7872 + offset_7898, sext_i64_i32(segred_tblock_sizze_6946)) && ((local_tid_7872 - squot32(local_tid_7872, wave_sizze_7874) * wave_sizze_7874) & (2 * offset_7898 - 1)) == 0) {
                double defunc_0_op_res_7893;

                // read array element
                eta_p_7892 = ((volatile __local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872 + offset_7898)];
                // apply reduction operation
                // sample_kernel.fut:8:22-25
                defunc_0_op_res_7893 = eta_p_7891 + eta_p_7892;
                eta_p_7891 = defunc_0_op_res_7893;
                // write result of operation
                ((volatile __local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872)] = eta_p_7891;
            }
            offset_7898 *= 2;
        }
        while (slt32(skip_waves_7899, squot32(sext_i64_i32(segred_tblock_sizze_6946) + wave_sizze_7874 - 1, wave_sizze_7874))) {
            barrier(CLK_LOCAL_MEM_FENCE);
            offset_7898 = skip_waves_7899 * wave_sizze_7874;
            if (slt32(local_tid_7872 + offset_7898, sext_i64_i32(segred_tblock_sizze_6946)) && ((local_tid_7872 - squot32(local_tid_7872, wave_sizze_7874) * wave_sizze_7874) == 0 && (squot32(local_tid_7872, wave_sizze_7874) & (2 * skip_waves_7899 - 1)) == 0)) {
                double defunc_0_op_res_7893;

                // read array element
                eta_p_7892 = ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872 + offset_7898)];
                // apply reduction operation
                // sample_kernel.fut:8:22-25
                defunc_0_op_res_7893 = eta_p_7891 + eta_p_7892;
                eta_p_7891 = defunc_0_op_res_7893;
                // write result of operation
                ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872)] = eta_p_7891;
            }
            skip_waves_7899 *= 2;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        barrier(CLK_LOCAL_MEM_FENCE);
        // thread 0 updates per-block acc(s); rest reset to ne
        if (sext_i32_i64(local_tid_7872) == (int64_t) 0) {
            eta_p_block_res_acc_7890 = eta_p_7891;
        } else {
            eta_p_block_res_acc_7890 = 0.0;
        }
        if (blocks_per_segment_7863 == (int64_t) 1) {
            // first thread in block saves final result to memory
            if (local_tid_7872 == 0) {
                ((__global double *) mem_7492)[gtid_6950 * m_6139 + gtid_6951] = eta_p_block_res_acc_7890;
            }
        } else {
            int32_t old_counter_7900;
            bool is_last_block_7901;

            // first thread in block saves block result to global memory
            if (local_tid_7872 == 0) {
                ((__global double *) segred_tmp_mem_7867)[sext_i32_i64(virt_tblock_id_7883)] = eta_p_block_res_acc_7890;
                mem_fence_global();
                old_counter_7900 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7869)[srem64(flat_segment_id_7884, (int64_t) 20480)], 1);
                ((__local bool *) sync_arr_mem_7878)[(int64_t) 0] = old_counter_7900 == sext_i64_i32(blocks_per_segment_7863 - (int64_t) 1);
            }
            barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
            is_last_block_7901 = ((__local bool *) sync_arr_mem_7878)[(int64_t) 0];
            if (is_last_block_7901) {
                int64_t read_per_thread_7902;
                int32_t offset_7906;
                int32_t skip_waves_7907;
                double eta_p_7891;
                double eta_p_7892;

                if (local_tid_7872 == 0) {
                    old_counter_7900 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7869)[srem64(flat_segment_id_7884, (int64_t) 20480)], sext_i64_i32((int64_t) 0 - blocks_per_segment_7863));
                }
                // read in the per-block-results
                read_per_thread_7902 = sdiv_up64(blocks_per_segment_7863, segred_tblock_sizze_6946);
                eta_p_6959 = 0.0;
                for (int64_t i_7903 = 0; i_7903 < read_per_thread_7902; i_7903++) {
                    int64_t block_res_id_7904;
                    int64_t index_of_block_res_7905;

                    block_res_id_7904 = sext_i32_i64(local_tid_7872) * read_per_thread_7902 + i_7903;
                    index_of_block_res_7905 = flat_segment_id_7884 * blocks_per_segment_7863 + block_res_id_7904;
                    if (slt64(block_res_id_7904, blocks_per_segment_7863)) {
                        double defunc_0_op_res_6961;

                        eta_p_6960 = ((__global double *) segred_tmp_mem_7867)[index_of_block_res_7905];
                        // sample_kernel.fut:8:22-25
                        defunc_0_op_res_6961 = eta_p_6959 + eta_p_6960;
                        eta_p_6959 = defunc_0_op_res_6961;
                    }
                }
                ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872)] = eta_p_6959;
                barrier(CLK_LOCAL_MEM_FENCE);
                // reduce the per-block results
                skip_waves_7907 = 1;
                offset_7906 = 0;
                // participating threads read initial accumulator
                if (slt32(local_tid_7872, sext_i64_i32(segred_tblock_sizze_6946))) {
                    eta_p_7891 = ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872 + offset_7906)];
                }
                offset_7906 = 1;
                while (slt32(offset_7906, wave_sizze_7874)) {
                    if (slt32(local_tid_7872 + offset_7906, sext_i64_i32(segred_tblock_sizze_6946)) && ((local_tid_7872 - squot32(local_tid_7872, wave_sizze_7874) * wave_sizze_7874) & (2 * offset_7906 - 1)) == 0) {
                        double defunc_0_op_res_7893;

                        // read array element
                        eta_p_7892 = ((volatile __local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872 + offset_7906)];
                        // apply reduction operation
                        // sample_kernel.fut:8:22-25
                        defunc_0_op_res_7893 = eta_p_7891 + eta_p_7892;
                        eta_p_7891 = defunc_0_op_res_7893;
                        // write result of operation
                        ((volatile __local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872)] = eta_p_7891;
                    }
                    offset_7906 *= 2;
                }
                while (slt32(skip_waves_7907, squot32(sext_i64_i32(segred_tblock_sizze_6946) + wave_sizze_7874 - 1, wave_sizze_7874))) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                    offset_7906 = skip_waves_7907 * wave_sizze_7874;
                    if (slt32(local_tid_7872 + offset_7906, sext_i64_i32(segred_tblock_sizze_6946)) && ((local_tid_7872 - squot32(local_tid_7872, wave_sizze_7874) * wave_sizze_7874) == 0 && (squot32(local_tid_7872, wave_sizze_7874) & (2 * skip_waves_7907 - 1)) == 0)) {
                        double defunc_0_op_res_7893;

                        // read array element
                        eta_p_7892 = ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872 + offset_7906)];
                        // apply reduction operation
                        // sample_kernel.fut:8:22-25
                        defunc_0_op_res_7893 = eta_p_7891 + eta_p_7892;
                        eta_p_7891 = defunc_0_op_res_7893;
                        // write result of operation
                        ((__local double *) red_arr_f64_mem_7876)[sext_i32_i64(local_tid_7872)] = eta_p_7891;
                    }
                    skip_waves_7907 *= 2;
                }
                barrier(CLK_LOCAL_MEM_FENCE);
                // and back to memory with the final result
                if (local_tid_7872 == 0) {
                    ((__global double *) mem_7492)[gtid_6950 * m_6139 + gtid_6951] = eta_p_7891;
                }
            }
        }
        barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    }

  error_6:
    return;
    #undef segred_tblock_sizze_6946
    #undef chunk_sizze_7832
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegred_large_6984_dim1, 1, 1)
void matmul_bias_relu_sumzisegred_large_6984(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t m_6139, int64_t num_tblocks_6979, int64_t blocks_per_segment_7949, int64_t q_7950, int64_t num_virtblocks_7951, int64_t threads_per_segment_7952, __global unsigned char *ext_mem_7618, __global unsigned char *mem_7621, __global unsigned char *segred_tmp_mem_7953, __global unsigned char *counters_mem_7955)
{
    #define segred_tblock_sizze_6978 (matmul_bias_relu_sumzisegred_large_6984zisegred_tblock_sizze_6978)
    #define chunk_sizze_7920 (matmul_bias_relu_sumzisegred_large_6984zichunk_sizze_7920)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *sync_arr_mem_7964_backing_1 = &shared_mem[0];
    const int64_t sync_arr_mem_7964_backing_1_offset = 0 + 8;
    volatile __local unsigned char *red_arr_f64_mem_7962_backing_0 = &shared_mem[sync_arr_mem_7964_backing_1_offset];
    const int64_t red_arr_f64_mem_7962_backing_0_offset = sync_arr_mem_7964_backing_1_offset + ((int64_t) 8 * segred_tblock_sizze_6978 + srem64((int64_t) 8 - srem64((int64_t) 8 * segred_tblock_sizze_6978, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7958;
    int32_t tblock_sizze_7961;
    int32_t wave_sizze_7960;
    int32_t block_id_7959;
    int32_t global_tid_7957;
    int64_t phys_tid_6984;
    __local unsigned char *red_arr_f64_mem_7962;
    __local unsigned char *sync_arr_mem_7964;
    int32_t phys_tblock_id_7966;
    int32_t iterations_7967;

    local_tid_7958 = get_local_id(0);
    tblock_sizze_7961 = get_local_size(0);
    wave_sizze_7960 = LOCKSTEP_WIDTH;
    block_id_7959 = get_tblock_id(0);
    global_tid_7957 = block_id_7959 * tblock_sizze_7961 + local_tid_7958;
    phys_tid_6984 = sext_i32_i64(global_tid_7957);
    red_arr_f64_mem_7962 = (__local unsigned char *) red_arr_f64_mem_7962_backing_0;
    sync_arr_mem_7964 = (__local unsigned char *) sync_arr_mem_7964_backing_1;
    phys_tblock_id_7966 = get_tblock_id(0);
    iterations_7967 = sdiv_up32(sext_i64_i32(num_virtblocks_7951) - phys_tblock_id_7966, sext_i64_i32(num_tblocks_6979));
    for (int32_t i_7968 = 0; i_7968 < iterations_7967; i_7968++) {
        int32_t virt_tblock_id_7969;
        int64_t flat_segment_id_7970;
        int64_t global_tid_7971;
        int64_t slice_7972;
        int64_t gtid_6982;
        int64_t remnant_7973;
        int64_t gtid_6983;
        double eta_p_block_res_acc_7974;
        double eta_p_6987;
        double eta_p_6988;
        int64_t tblock_id_in_segment_7978;
        int64_t block_base_offset_7979;
        int32_t offset_7982;
        int32_t skip_waves_7983;
        double eta_p_7975;
        double eta_p_7976;

        virt_tblock_id_7969 = phys_tblock_id_7966 + i_7968 * sext_i64_i32(num_tblocks_6979);
        flat_segment_id_7970 = squot64(sext_i32_i64(virt_tblock_id_7969), blocks_per_segment_7949);
        global_tid_7971 = srem64(sext_i32_i64(virt_tblock_id_7969) * segred_tblock_sizze_6978 + sext_i32_i64(local_tid_7958), threads_per_segment_7952);
        slice_7972 = n_6137;
        gtid_6982 = flat_segment_id_7970;
        remnant_7973 = flat_segment_id_7970 - gtid_6982;
        // ne-initialise the outer (per-block) accumulator(s)
        eta_p_block_res_acc_7974 = 0.0;
        tblock_id_in_segment_7978 = squot64(global_tid_7971, segred_tblock_sizze_6978);
        block_base_offset_7979 = tblock_id_in_segment_7978 * q_7950 * segred_tblock_sizze_6978;
        for (int64_t i_7980 = 0; i_7980 < q_7950; i_7980++) {
            int64_t block_offset_7981 = block_base_offset_7979 + i_7980 * segred_tblock_sizze_6978;

            gtid_6983 = global_tid_7971 + threads_per_segment_7952 * i_7980;
            if (slt64(gtid_6983, m_6139)) {
                double x_6986;
                double defunc_0_op_res_6989;

                // apply map function(s)
                // apply map function
                x_6986 = ((__global double *) ext_mem_7618)[gtid_6982 * m_6139 + gtid_6983];
                // load accumulator(s)
                eta_p_6987 = eta_p_block_res_acc_7974;
                // load next value(s)
                eta_p_6988 = x_6986;
                // apply reduction operator(s)
                // sample_kernel.fut:19:34-37
                defunc_0_op_res_6989 = eta_p_6987 + eta_p_6988;
                // store in accumulator(s)
                eta_p_block_res_acc_7974 = defunc_0_op_res_6989;
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        // store accs. prims go in lmem; non-prims in params (in global mem)
        ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958)] = eta_p_block_res_acc_7974;
        barrier(CLK_LOCAL_MEM_FENCE);
        skip_waves_7983 = 1;
        offset_7982 = 0;
        // participating threads read initial accumulator
        if (slt32(local_tid_7958, sext_i64_i32(segred_tblock_sizze_6978))) {
            eta_p_7975 = ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958 + offset_7982)];
        }
        offset_7982 = 1;
        while (slt32(offset_7982, wave_sizze_7960)) {
            if (slt32(local_tid_7958 + offset_7982, sext_i64_i32(segred_tblock_sizze_6978)) && ((local_tid_7958 - squot32(local_tid_7958, wave_sizze_7960) * wave_sizze_7960) & (2 * offset_7982 - 1)) == 0) {
                double defunc_0_op_res_7977;

                // read array element
                eta_p_7976 = ((volatile __local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958 + offset_7982)];
                // apply reduction operation
                // sample_kernel.fut:19:34-37
                defunc_0_op_res_7977 = eta_p_7975 + eta_p_7976;
                eta_p_7975 = defunc_0_op_res_7977;
                // write result of operation
                ((volatile __local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958)] = eta_p_7975;
            }
            offset_7982 *= 2;
        }
        while (slt32(skip_waves_7983, squot32(sext_i64_i32(segred_tblock_sizze_6978) + wave_sizze_7960 - 1, wave_sizze_7960))) {
            barrier(CLK_LOCAL_MEM_FENCE);
            offset_7982 = skip_waves_7983 * wave_sizze_7960;
            if (slt32(local_tid_7958 + offset_7982, sext_i64_i32(segred_tblock_sizze_6978)) && ((local_tid_7958 - squot32(local_tid_7958, wave_sizze_7960) * wave_sizze_7960) == 0 && (squot32(local_tid_7958, wave_sizze_7960) & (2 * skip_waves_7983 - 1)) == 0)) {
                double defunc_0_op_res_7977;

                // read array element
                eta_p_7976 = ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958 + offset_7982)];
                // apply reduction operation
                // sample_kernel.fut:19:34-37
                defunc_0_op_res_7977 = eta_p_7975 + eta_p_7976;
                eta_p_7975 = defunc_0_op_res_7977;
                // write result of operation
                ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958)] = eta_p_7975;
            }
            skip_waves_7983 *= 2;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        barrier(CLK_LOCAL_MEM_FENCE);
        // thread 0 updates per-block acc(s); rest reset to ne
        if (sext_i32_i64(local_tid_7958) == (int64_t) 0) {
            eta_p_block_res_acc_7974 = eta_p_7975;
        } else {
            eta_p_block_res_acc_7974 = 0.0;
        }
        if (blocks_per_segment_7949 == (int64_t) 1) {
            // first thread in block saves final result to memory
            if (local_tid_7958 == 0) {
                ((__global double *) mem_7621)[gtid_6982] = eta_p_block_res_acc_7974;
            }
        } else {
            int32_t old_counter_7984;
            bool is_last_block_7985;

            // first thread in block saves block result to global memory
            if (local_tid_7958 == 0) {
                ((__global double *) segred_tmp_mem_7953)[sext_i32_i64(virt_tblock_id_7969)] = eta_p_block_res_acc_7974;
                mem_fence_global();
                old_counter_7984 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7955)[srem64(flat_segment_id_7970, (int64_t) 20480)], 1);
                ((__local bool *) sync_arr_mem_7964)[(int64_t) 0] = old_counter_7984 == sext_i64_i32(blocks_per_segment_7949 - (int64_t) 1);
            }
            barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
            is_last_block_7985 = ((__local bool *) sync_arr_mem_7964)[(int64_t) 0];
            if (is_last_block_7985) {
                int64_t read_per_thread_7986;
                int32_t offset_7990;
                int32_t skip_waves_7991;
                double eta_p_7975;
                double eta_p_7976;

                if (local_tid_7958 == 0) {
                    old_counter_7984 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7955)[srem64(flat_segment_id_7970, (int64_t) 20480)], sext_i64_i32((int64_t) 0 - blocks_per_segment_7949));
                }
                // read in the per-block-results
                read_per_thread_7986 = sdiv_up64(blocks_per_segment_7949, segred_tblock_sizze_6978);
                eta_p_6987 = 0.0;
                for (int64_t i_7987 = 0; i_7987 < read_per_thread_7986; i_7987++) {
                    int64_t block_res_id_7988;
                    int64_t index_of_block_res_7989;

                    block_res_id_7988 = sext_i32_i64(local_tid_7958) * read_per_thread_7986 + i_7987;
                    index_of_block_res_7989 = flat_segment_id_7970 * blocks_per_segment_7949 + block_res_id_7988;
                    if (slt64(block_res_id_7988, blocks_per_segment_7949)) {
                        double defunc_0_op_res_6989;

                        eta_p_6988 = ((__global double *) segred_tmp_mem_7953)[index_of_block_res_7989];
                        // sample_kernel.fut:19:34-37
                        defunc_0_op_res_6989 = eta_p_6987 + eta_p_6988;
                        eta_p_6987 = defunc_0_op_res_6989;
                    }
                }
                ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958)] = eta_p_6987;
                barrier(CLK_LOCAL_MEM_FENCE);
                // reduce the per-block results
                skip_waves_7991 = 1;
                offset_7990 = 0;
                // participating threads read initial accumulator
                if (slt32(local_tid_7958, sext_i64_i32(segred_tblock_sizze_6978))) {
                    eta_p_7975 = ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958 + offset_7990)];
                }
                offset_7990 = 1;
                while (slt32(offset_7990, wave_sizze_7960)) {
                    if (slt32(local_tid_7958 + offset_7990, sext_i64_i32(segred_tblock_sizze_6978)) && ((local_tid_7958 - squot32(local_tid_7958, wave_sizze_7960) * wave_sizze_7960) & (2 * offset_7990 - 1)) == 0) {
                        double defunc_0_op_res_7977;

                        // read array element
                        eta_p_7976 = ((volatile __local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958 + offset_7990)];
                        // apply reduction operation
                        // sample_kernel.fut:19:34-37
                        defunc_0_op_res_7977 = eta_p_7975 + eta_p_7976;
                        eta_p_7975 = defunc_0_op_res_7977;
                        // write result of operation
                        ((volatile __local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958)] = eta_p_7975;
                    }
                    offset_7990 *= 2;
                }
                while (slt32(skip_waves_7991, squot32(sext_i64_i32(segred_tblock_sizze_6978) + wave_sizze_7960 - 1, wave_sizze_7960))) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                    offset_7990 = skip_waves_7991 * wave_sizze_7960;
                    if (slt32(local_tid_7958 + offset_7990, sext_i64_i32(segred_tblock_sizze_6978)) && ((local_tid_7958 - squot32(local_tid_7958, wave_sizze_7960) * wave_sizze_7960) == 0 && (squot32(local_tid_7958, wave_sizze_7960) & (2 * skip_waves_7991 - 1)) == 0)) {
                        double defunc_0_op_res_7977;

                        // read array element
                        eta_p_7976 = ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958 + offset_7990)];
                        // apply reduction operation
                        // sample_kernel.fut:19:34-37
                        defunc_0_op_res_7977 = eta_p_7975 + eta_p_7976;
                        eta_p_7975 = defunc_0_op_res_7977;
                        // write result of operation
                        ((__local double *) red_arr_f64_mem_7962)[sext_i32_i64(local_tid_7958)] = eta_p_7975;
                    }
                    skip_waves_7991 *= 2;
                }
                barrier(CLK_LOCAL_MEM_FENCE);
                // and back to memory with the final result
                if (local_tid_7958 == 0) {
                    ((__global double *) mem_7621)[gtid_6982] = eta_p_7975;
                }
            }
        }
        barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    }

  error_6:
    return;
    #undef segred_tblock_sizze_6978
    #undef chunk_sizze_7920
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegred_nonseg_6544_dim1, 1, 1)
void matmul_bias_relu_sumzisegred_nonseg_6544(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t k_6138, int64_t m_6139, int64_t num_tblocks_6539, int64_t num_threads_7717, __global unsigned char *bias_mem_7473, __global unsigned char *mem_7665, __global unsigned char *mem_7676, __global unsigned char *mem_7678, __global unsigned char *counters_mem_7693, __global unsigned char *segred_tmp_mem_7715)
{
    #define segred_tblock_sizze_6538 (matmul_bias_relu_sumzisegred_nonseg_6544zisegred_tblock_sizze_6538)
    #define chunk_sizze_7692 (matmul_bias_relu_sumzisegred_nonseg_6544zichunk_sizze_7692)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *sync_arr_mem_7725_backing_1 = &shared_mem[0];
    const int64_t sync_arr_mem_7725_backing_1_offset = 0 + 8;
    volatile __local unsigned char *red_arr_f64_mem_7723_backing_0 = &shared_mem[sync_arr_mem_7725_backing_1_offset];
    const int64_t red_arr_f64_mem_7723_backing_0_offset = sync_arr_mem_7725_backing_1_offset + ((int64_t) 8 * segred_tblock_sizze_6538 + srem64((int64_t) 8 - srem64((int64_t) 8 * segred_tblock_sizze_6538, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7719;
    int32_t tblock_sizze_7722;
    int32_t wave_sizze_7721;
    int32_t block_id_7720;
    int32_t global_tid_7718;
    int64_t phys_tid_6544;
    __local unsigned char *red_arr_f64_mem_7723;
    __local unsigned char *sync_arr_mem_7725;
    int64_t dummy_6542;
    int64_t gtid_6543;
    int64_t q_7727;
    double eta_p_block_res_acc_7728;
    double eta_p_6562;
    double eta_p_6563;
    int64_t tblock_id_in_segment_7732;
    int64_t block_base_offset_7733;
    int32_t offset_7738;
    int32_t skip_waves_7739;
    double eta_p_7729;
    double eta_p_7730;
    int32_t old_counter_7740;
    bool is_last_block_7741;

    local_tid_7719 = get_local_id(0);
    tblock_sizze_7722 = get_local_size(0);
    wave_sizze_7721 = LOCKSTEP_WIDTH;
    block_id_7720 = get_tblock_id(0);
    global_tid_7718 = block_id_7720 * tblock_sizze_7722 + local_tid_7719;
    phys_tid_6544 = sext_i32_i64(global_tid_7718);
    red_arr_f64_mem_7723 = (__local unsigned char *) red_arr_f64_mem_7723_backing_0;
    sync_arr_mem_7725 = (__local unsigned char *) sync_arr_mem_7725_backing_1;
    dummy_6542 = (int64_t) 0;
    gtid_6543 = (int64_t) 0;
    q_7727 = sdiv_up64(n_6137, sext_i32_i64(sext_i64_i32(segred_tblock_sizze_6538 * num_tblocks_6539)) * chunk_sizze_7692);
    // ne-initialise the outer (per-block) accumulator(s)
    eta_p_block_res_acc_7728 = 0.0;
    tblock_id_in_segment_7732 = squot64(phys_tid_6544, segred_tblock_sizze_6538);
    block_base_offset_7733 = tblock_id_in_segment_7732 * q_7727 * segred_tblock_sizze_6538;
    for (int64_t i_7734 = 0; i_7734 < q_7727; i_7734++) {
        int64_t block_offset_7735 = block_base_offset_7733 + i_7734 * segred_tblock_sizze_6538;

        gtid_6543 = phys_tid_6544 + num_threads_7717 * i_7734;
        if (slt64(gtid_6543, n_6137)) {
            double defunc_0_f_res_6546;
            double redout_7451;
            double defunc_0_op_res_6564;

            // apply map function(s)
            // apply map function
            // sample_kernel.fut:15:11-19:49
            redout_7451 = 0.0;
            for (int64_t i_7452 = 0; i_7452 < m_6139; i_7452++) {
                double eta_p_6548;
                double defunc_0_reduce_res_6549;
                double redout_7453;
                double defunc_0_f_res_6556;
                bool cond_6557;
                double lifted_lambda_res_6558;
                double defunc_0_op_res_6561;
                double redout_tmp_7736;

                eta_p_6548 = ((__global double *) bias_mem_7473)[i_7452];
                // sample_kernel.fut:8:15-47
                redout_7453 = 0.0;
                for (int64_t i_7454 = 0; i_7454 < k_6138; i_7454++) {
                    double eta_p_6550;
                    double eta_p_6551;
                    double defunc_0_f_res_6552;
                    double defunc_0_op_res_6555;
                    double redout_tmp_7737;

                    eta_p_6550 = ((__global double *) mem_7665)[gtid_6543 + i_7454 * n_6137];
                    eta_p_6551 = ((__global double *) mem_7676)[i_7454 + i_7452 * k_6138];
                    // sample_kernel.fut:8:36-39
                    defunc_0_f_res_6552 = eta_p_6550 * eta_p_6551;
                    // sample_kernel.fut:8:22-25
                    defunc_0_op_res_6555 = defunc_0_f_res_6552 + redout_7453;
                    redout_tmp_7737 = defunc_0_op_res_6555;
                    redout_7453 = redout_tmp_7737;
                }
                defunc_0_reduce_res_6549 = redout_7453;
                // sample_kernel.fut:17:34-37
                defunc_0_f_res_6556 = eta_p_6548 + defunc_0_reduce_res_6549;
                // sample_kernel.fut:18:32-58
                cond_6557 = 0.0 < defunc_0_f_res_6556;
                // sample_kernel.fut:18:32-58
                if (cond_6557) {
                    lifted_lambda_res_6558 = defunc_0_f_res_6556;
                } else {
                    lifted_lambda_res_6558 = 0.0;
                }
                // sample_kernel.fut:19:34-37
                defunc_0_op_res_6561 = lifted_lambda_res_6558 + redout_7451;
                redout_tmp_7736 = defunc_0_op_res_6561;
                redout_7451 = redout_tmp_7736;
            }
            defunc_0_f_res_6546 = redout_7451;
            // load accumulator(s)
            eta_p_6562 = eta_p_block_res_acc_7728;
            // load next value(s)
            eta_p_6563 = defunc_0_f_res_6546;
            // apply reduction operator(s)
            // sample_kernel.fut:19:13-16
            defunc_0_op_res_6564 = eta_p_6562 + eta_p_6563;
            // store in accumulator(s)
            eta_p_block_res_acc_7728 = defunc_0_op_res_6564;
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    // store accs. prims go in lmem; non-prims in params (in global mem)
    ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719)] = eta_p_block_res_acc_7728;
    barrier(CLK_LOCAL_MEM_FENCE);
    skip_waves_7739 = 1;
    offset_7738 = 0;
    // participating threads read initial accumulator
    if (slt32(local_tid_7719, sext_i64_i32(segred_tblock_sizze_6538))) {
        eta_p_7729 = ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719 + offset_7738)];
    }
    offset_7738 = 1;
    while (slt32(offset_7738, wave_sizze_7721)) {
        if (slt32(local_tid_7719 + offset_7738, sext_i64_i32(segred_tblock_sizze_6538)) && ((local_tid_7719 - squot32(local_tid_7719, wave_sizze_7721) * wave_sizze_7721) & (2 * offset_7738 - 1)) == 0) {
            double defunc_0_op_res_7731;

            // read array element
            eta_p_7730 = ((volatile __local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719 + offset_7738)];
            // apply reduction operation
            // sample_kernel.fut:19:13-16
            defunc_0_op_res_7731 = eta_p_7729 + eta_p_7730;
            eta_p_7729 = defunc_0_op_res_7731;
            // write result of operation
            ((volatile __local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719)] = eta_p_7729;
        }
        offset_7738 *= 2;
    }
    while (slt32(skip_waves_7739, squot32(sext_i64_i32(segred_tblock_sizze_6538) + wave_sizze_7721 - 1, wave_sizze_7721))) {
        barrier(CLK_LOCAL_MEM_FENCE);
        offset_7738 = skip_waves_7739 * wave_sizze_7721;
        if (slt32(local_tid_7719 + offset_7738, sext_i64_i32(segred_tblock_sizze_6538)) && ((local_tid_7719 - squot32(local_tid_7719, wave_sizze_7721) * wave_sizze_7721) == 0 && (squot32(local_tid_7719, wave_sizze_7721) & (2 * skip_waves_7739 - 1)) == 0)) {
            double defunc_0_op_res_7731;

            // read array element
            eta_p_7730 = ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719 + offset_7738)];
            // apply reduction operation
            // sample_kernel.fut:19:13-16
            defunc_0_op_res_7731 = eta_p_7729 + eta_p_7730;
            eta_p_7729 = defunc_0_op_res_7731;
            // write result of operation
            ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719)] = eta_p_7729;
        }
        skip_waves_7739 *= 2;
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    barrier(CLK_LOCAL_MEM_FENCE);
    // thread 0 updates per-block acc(s); rest reset to ne
    if (sext_i32_i64(local_tid_7719) == (int64_t) 0) {
        eta_p_block_res_acc_7728 = eta_p_7729;
    } else {
        eta_p_block_res_acc_7728 = 0.0;
    }
    // first thread in block saves block result to global memory
    if (local_tid_7719 == 0) {
        ((__global double *) segred_tmp_mem_7715)[sext_i32_i64(block_id_7720)] = eta_p_block_res_acc_7728;
        mem_fence_global();
        old_counter_7740 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7693)[(int64_t) 0], 1);
        ((__local bool *) sync_arr_mem_7725)[(int64_t) 0] = old_counter_7740 == sext_i64_i32(num_tblocks_6539 - (int64_t) 1);
    }
    barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    is_last_block_7741 = ((__local bool *) sync_arr_mem_7725)[(int64_t) 0];
    if (is_last_block_7741) {
        int64_t read_per_thread_7742;
        int32_t offset_7746;
        int32_t skip_waves_7747;
        double eta_p_7729;
        double eta_p_7730;

        if (local_tid_7719 == 0) {
            old_counter_7740 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7693)[(int64_t) 0], sext_i64_i32((int64_t) 0 - num_tblocks_6539));
        }
        // read in the per-block-results
        read_per_thread_7742 = sdiv_up64(num_tblocks_6539, segred_tblock_sizze_6538);
        eta_p_6562 = 0.0;
        for (int64_t i_7743 = 0; i_7743 < read_per_thread_7742; i_7743++) {
            int64_t block_res_id_7744;
            int64_t index_of_block_res_7745;

            block_res_id_7744 = sext_i32_i64(local_tid_7719) * read_per_thread_7742 + i_7743;
            index_of_block_res_7745 = block_res_id_7744;
            if (slt64(block_res_id_7744, num_tblocks_6539)) {
                double defunc_0_op_res_6564;

                eta_p_6563 = ((__global double *) segred_tmp_mem_7715)[index_of_block_res_7745];
                // sample_kernel.fut:19:13-16
                defunc_0_op_res_6564 = eta_p_6562 + eta_p_6563;
                eta_p_6562 = defunc_0_op_res_6564;
            }
        }
        ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719)] = eta_p_6562;
        barrier(CLK_LOCAL_MEM_FENCE);
        // reduce the per-block results
        skip_waves_7747 = 1;
        offset_7746 = 0;
        // participating threads read initial accumulator
        if (slt32(local_tid_7719, sext_i64_i32(segred_tblock_sizze_6538))) {
            eta_p_7729 = ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719 + offset_7746)];
        }
        offset_7746 = 1;
        while (slt32(offset_7746, wave_sizze_7721)) {
            if (slt32(local_tid_7719 + offset_7746, sext_i64_i32(segred_tblock_sizze_6538)) && ((local_tid_7719 - squot32(local_tid_7719, wave_sizze_7721) * wave_sizze_7721) & (2 * offset_7746 - 1)) == 0) {
                double defunc_0_op_res_7731;

                // read array element
                eta_p_7730 = ((volatile __local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719 + offset_7746)];
                // apply reduction operation
                // sample_kernel.fut:19:13-16
                defunc_0_op_res_7731 = eta_p_7729 + eta_p_7730;
                eta_p_7729 = defunc_0_op_res_7731;
                // write result of operation
                ((volatile __local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719)] = eta_p_7729;
            }
            offset_7746 *= 2;
        }
        while (slt32(skip_waves_7747, squot32(sext_i64_i32(segred_tblock_sizze_6538) + wave_sizze_7721 - 1, wave_sizze_7721))) {
            barrier(CLK_LOCAL_MEM_FENCE);
            offset_7746 = skip_waves_7747 * wave_sizze_7721;
            if (slt32(local_tid_7719 + offset_7746, sext_i64_i32(segred_tblock_sizze_6538)) && ((local_tid_7719 - squot32(local_tid_7719, wave_sizze_7721) * wave_sizze_7721) == 0 && (squot32(local_tid_7719, wave_sizze_7721) & (2 * skip_waves_7747 - 1)) == 0)) {
                double defunc_0_op_res_7731;

                // read array element
                eta_p_7730 = ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719 + offset_7746)];
                // apply reduction operation
                // sample_kernel.fut:19:13-16
                defunc_0_op_res_7731 = eta_p_7729 + eta_p_7730;
                eta_p_7729 = defunc_0_op_res_7731;
                // write result of operation
                ((__local double *) red_arr_f64_mem_7723)[sext_i32_i64(local_tid_7719)] = eta_p_7729;
            }
            skip_waves_7747 *= 2;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        // and back to memory with the final result
        if (local_tid_7719 == 0) {
            ((__global double *) mem_7678)[(int64_t) 0] = eta_p_7729;
        }
    }

  error_5:
    return;
    #undef segred_tblock_sizze_6538
    #undef chunk_sizze_7692
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegred_nonseg_6998_dim1, 1, 1)
void matmul_bias_relu_sumzisegred_nonseg_6998(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t num_tblocks_6993, int64_t num_threads_7999, __global unsigned char *ext_mem_7651, __global unsigned char *mem_7653, __global unsigned char *counters_mem_7995, __global unsigned char *segred_tmp_mem_7997)
{
    #define segred_tblock_sizze_6992 (matmul_bias_relu_sumzisegred_nonseg_6998zisegred_tblock_sizze_6992)
    #define chunk_sizze_7994 (matmul_bias_relu_sumzisegred_nonseg_6998zichunk_sizze_7994)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *sync_arr_mem_8007_backing_1 = &shared_mem[0];
    const int64_t sync_arr_mem_8007_backing_1_offset = 0 + 8;
    volatile __local unsigned char *red_arr_f64_mem_8005_backing_0 = &shared_mem[sync_arr_mem_8007_backing_1_offset];
    const int64_t red_arr_f64_mem_8005_backing_0_offset = sync_arr_mem_8007_backing_1_offset + ((int64_t) 8 * segred_tblock_sizze_6992 + srem64((int64_t) 8 - srem64((int64_t) 8 * segred_tblock_sizze_6992, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_8001;
    int32_t tblock_sizze_8004;
    int32_t wave_sizze_8003;
    int32_t block_id_8002;
    int32_t global_tid_8000;
    int64_t phys_tid_6998;
    __local unsigned char *red_arr_f64_mem_8005;
    __local unsigned char *sync_arr_mem_8007;
    int64_t dummy_6996;
    int64_t gtid_6997;
    int64_t q_8009;
    double eta_p_block_res_acc_8010;
    double eta_p_7000;
    double eta_p_7001;
    int64_t tblock_id_in_segment_8014;
    int64_t block_base_offset_8015;
    int32_t offset_8018;
    int32_t skip_waves_8019;
    double eta_p_8011;
    double eta_p_8012;
    int32_t old_counter_8020;
    bool is_last_block_8021;

    local_tid_8001 = get_local_id(0);
    tblock_sizze_8004 = get_local_size(0);
    wave_sizze_8003 = LOCKSTEP_WIDTH;
    block_id_8002 = get_tblock_id(0);
    global_tid_8000 = block_id_8002 * tblock_sizze_8004 + local_tid_8001;
    phys_tid_6998 = sext_i32_i64(global_tid_8000);
    red_arr_f64_mem_8005 = (__local unsigned char *) red_arr_f64_mem_8005_backing_0;
    sync_arr_mem_8007 = (__local unsigned char *) sync_arr_mem_8007_backing_1;
    dummy_6996 = (int64_t) 0;
    gtid_6997 = (int64_t) 0;
    q_8009 = sdiv_up64(n_6137, sext_i32_i64(sext_i64_i32(segred_tblock_sizze_6992 * num_tblocks_6993)) * chunk_sizze_7994);
    // ne-initialise the outer (per-block) accumulator(s)
    eta_p_block_res_acc_8010 = 0.0;
    tblock_id_in_segment_8014 = squot64(phys_tid_6998, segred_tblock_sizze_6992);
    block_base_offset_8015 = tblock_id_in_segment_8014 * q_8009 * segred_tblock_sizze_6992;
    for (int64_t i_8016 = 0; i_8016 < q_8009; i_8016++) {
        int64_t block_offset_8017 = block_base_offset_8015 + i_8016 * segred_tblock_sizze_6992;

        gtid_6997 = phys_tid_6998 + num_threads_7999 * i_8016;
        if (slt64(gtid_6997, n_6137)) {
            double x_6999;
            double defunc_0_op_res_7002;

            // apply map function(s)
            // apply map function
            x_6999 = ((__global double *) ext_mem_7651)[gtid_6997];
            // load accumulator(s)
            eta_p_7000 = eta_p_block_res_acc_8010;
            // load next value(s)
            eta_p_7001 = x_6999;
            // apply reduction operator(s)
            // sample_kernel.fut:19:13-16
            defunc_0_op_res_7002 = eta_p_7000 + eta_p_7001;
            // store in accumulator(s)
            eta_p_block_res_acc_8010 = defunc_0_op_res_7002;
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    // store accs. prims go in lmem; non-prims in params (in global mem)
    ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001)] = eta_p_block_res_acc_8010;
    barrier(CLK_LOCAL_MEM_FENCE);
    skip_waves_8019 = 1;
    offset_8018 = 0;
    // participating threads read initial accumulator
    if (slt32(local_tid_8001, sext_i64_i32(segred_tblock_sizze_6992))) {
        eta_p_8011 = ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001 + offset_8018)];
    }
    offset_8018 = 1;
    while (slt32(offset_8018, wave_sizze_8003)) {
        if (slt32(local_tid_8001 + offset_8018, sext_i64_i32(segred_tblock_sizze_6992)) && ((local_tid_8001 - squot32(local_tid_8001, wave_sizze_8003) * wave_sizze_8003) & (2 * offset_8018 - 1)) == 0) {
            double defunc_0_op_res_8013;

            // read array element
            eta_p_8012 = ((volatile __local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001 + offset_8018)];
            // apply reduction operation
            // sample_kernel.fut:19:13-16
            defunc_0_op_res_8013 = eta_p_8011 + eta_p_8012;
            eta_p_8011 = defunc_0_op_res_8013;
            // write result of operation
            ((volatile __local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001)] = eta_p_8011;
        }
        offset_8018 *= 2;
    }
    while (slt32(skip_waves_8019, squot32(sext_i64_i32(segred_tblock_sizze_6992) + wave_sizze_8003 - 1, wave_sizze_8003))) {
        barrier(CLK_LOCAL_MEM_FENCE);
        offset_8018 = skip_waves_8019 * wave_sizze_8003;
        if (slt32(local_tid_8001 + offset_8018, sext_i64_i32(segred_tblock_sizze_6992)) && ((local_tid_8001 - squot32(local_tid_8001, wave_sizze_8003) * wave_sizze_8003) == 0 && (squot32(local_tid_8001, wave_sizze_8003) & (2 * skip_waves_8019 - 1)) == 0)) {
            double defunc_0_op_res_8013;

            // read array element
            eta_p_8012 = ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001 + offset_8018)];
            // apply reduction operation
            // sample_kernel.fut:19:13-16
            defunc_0_op_res_8013 = eta_p_8011 + eta_p_8012;
            eta_p_8011 = defunc_0_op_res_8013;
            // write result of operation
            ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001)] = eta_p_8011;
        }
        skip_waves_8019 *= 2;
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    barrier(CLK_LOCAL_MEM_FENCE);
    // thread 0 updates per-block acc(s); rest reset to ne
    if (sext_i32_i64(local_tid_8001) == (int64_t) 0) {
        eta_p_block_res_acc_8010 = eta_p_8011;
    } else {
        eta_p_block_res_acc_8010 = 0.0;
    }
    // first thread in block saves block result to global memory
    if (local_tid_8001 == 0) {
        ((__global double *) segred_tmp_mem_7997)[sext_i32_i64(block_id_8002)] = eta_p_block_res_acc_8010;
        mem_fence_global();
        old_counter_8020 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7995)[(int64_t) 0], 1);
        ((__local bool *) sync_arr_mem_8007)[(int64_t) 0] = old_counter_8020 == sext_i64_i32(num_tblocks_6993 - (int64_t) 1);
    }
    barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    is_last_block_8021 = ((__local bool *) sync_arr_mem_8007)[(int64_t) 0];
    if (is_last_block_8021) {
        int64_t read_per_thread_8022;
        int32_t offset_8026;
        int32_t skip_waves_8027;
        double eta_p_8011;
        double eta_p_8012;

        if (local_tid_8001 == 0) {
            old_counter_8020 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7995)[(int64_t) 0], sext_i64_i32((int64_t) 0 - num_tblocks_6993));
        }
        // read in the per-block-results
        read_per_thread_8022 = sdiv_up64(num_tblocks_6993, segred_tblock_sizze_6992);
        eta_p_7000 = 0.0;
        for (int64_t i_8023 = 0; i_8023 < read_per_thread_8022; i_8023++) {
            int64_t block_res_id_8024;
            int64_t index_of_block_res_8025;

            block_res_id_8024 = sext_i32_i64(local_tid_8001) * read_per_thread_8022 + i_8023;
            index_of_block_res_8025 = block_res_id_8024;
            if (slt64(block_res_id_8024, num_tblocks_6993)) {
                double defunc_0_op_res_7002;

                eta_p_7001 = ((__global double *) segred_tmp_mem_7997)[index_of_block_res_8025];
                // sample_kernel.fut:19:13-16
                defunc_0_op_res_7002 = eta_p_7000 + eta_p_7001;
                eta_p_7000 = defunc_0_op_res_7002;
            }
        }
        ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001)] = eta_p_7000;
        barrier(CLK_LOCAL_MEM_FENCE);
        // reduce the per-block results
        skip_waves_8027 = 1;
        offset_8026 = 0;
        // participating threads read initial accumulator
        if (slt32(local_tid_8001, sext_i64_i32(segred_tblock_sizze_6992))) {
            eta_p_8011 = ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001 + offset_8026)];
        }
        offset_8026 = 1;
        while (slt32(offset_8026, wave_sizze_8003)) {
            if (slt32(local_tid_8001 + offset_8026, sext_i64_i32(segred_tblock_sizze_6992)) && ((local_tid_8001 - squot32(local_tid_8001, wave_sizze_8003) * wave_sizze_8003) & (2 * offset_8026 - 1)) == 0) {
                double defunc_0_op_res_8013;

                // read array element
                eta_p_8012 = ((volatile __local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001 + offset_8026)];
                // apply reduction operation
                // sample_kernel.fut:19:13-16
                defunc_0_op_res_8013 = eta_p_8011 + eta_p_8012;
                eta_p_8011 = defunc_0_op_res_8013;
                // write result of operation
                ((volatile __local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001)] = eta_p_8011;
            }
            offset_8026 *= 2;
        }
        while (slt32(skip_waves_8027, squot32(sext_i64_i32(segred_tblock_sizze_6992) + wave_sizze_8003 - 1, wave_sizze_8003))) {
            barrier(CLK_LOCAL_MEM_FENCE);
            offset_8026 = skip_waves_8027 * wave_sizze_8003;
            if (slt32(local_tid_8001 + offset_8026, sext_i64_i32(segred_tblock_sizze_6992)) && ((local_tid_8001 - squot32(local_tid_8001, wave_sizze_8003) * wave_sizze_8003) == 0 && (squot32(local_tid_8001, wave_sizze_8003) & (2 * skip_waves_8027 - 1)) == 0)) {
                double defunc_0_op_res_8013;

                // read array element
                eta_p_8012 = ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001 + offset_8026)];
                // apply reduction operation
                // sample_kernel.fut:19:13-16
                defunc_0_op_res_8013 = eta_p_8011 + eta_p_8012;
                eta_p_8011 = defunc_0_op_res_8013;
                // write result of operation
                ((__local double *) red_arr_f64_mem_8005)[sext_i32_i64(local_tid_8001)] = eta_p_8011;
            }
            skip_waves_8027 *= 2;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        // and back to memory with the final result
        if (local_tid_8001 == 0) {
            ((__global double *) mem_7653)[(int64_t) 0] = eta_p_8011;
        }
    }

  error_5:
    return;
    #undef segred_tblock_sizze_6992
    #undef chunk_sizze_7994
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegred_small_6953_dim1, 1, 1)
void matmul_bias_relu_sumzisegred_small_6953(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t k_6138, int64_t m_6139, int64_t num_tblocks_6947, int64_t segment_sizze_nonzzero_7833, __global unsigned char *a_mem_7471, __global unsigned char *mem_7484, __global unsigned char *mem_7492)
{
    #define segred_tblock_sizze_6946 (matmul_bias_relu_sumzisegred_small_6953zisegred_tblock_sizze_6946)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *red_arr_f64_mem_7840_backing_0 = &shared_mem[0];
    const int64_t red_arr_f64_mem_7840_backing_0_offset = 0 + ((int64_t) 8 * segred_tblock_sizze_6946 + srem64((int64_t) 8 - srem64((int64_t) 8 * segred_tblock_sizze_6946, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7836;
    int32_t tblock_sizze_7839;
    int32_t wave_sizze_7838;
    int32_t block_id_7837;
    int32_t global_tid_7835;
    int64_t phys_tid_6953;
    __local unsigned char *red_arr_f64_mem_7840;
    int32_t phys_tblock_id_7842;
    int32_t iterations_7843;

    local_tid_7836 = get_local_id(0);
    tblock_sizze_7839 = get_local_size(0);
    wave_sizze_7838 = LOCKSTEP_WIDTH;
    block_id_7837 = get_tblock_id(0);
    global_tid_7835 = block_id_7837 * tblock_sizze_7839 + local_tid_7836;
    phys_tid_6953 = sext_i32_i64(global_tid_7835);
    red_arr_f64_mem_7840 = (__local unsigned char *) red_arr_f64_mem_7840_backing_0;
    phys_tblock_id_7842 = get_tblock_id(0);
    iterations_7843 = sdiv_up32(sext_i64_i32(sdiv_up64(n_6137 * m_6139, squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833))) - phys_tblock_id_7842, sext_i64_i32(num_tblocks_6947));
    for (int32_t i_7844 = 0; i_7844 < iterations_7843; i_7844++) {
        int32_t virt_tblock_id_7845;
        int64_t slice_7846;
        int64_t slice_7847;
        int64_t gtid_6950;
        int64_t remnant_7848;
        int64_t gtid_6951;
        int64_t remnant_7849;
        int64_t gtid_6952;

        virt_tblock_id_7845 = phys_tblock_id_7842 + i_7844 * sext_i64_i32(num_tblocks_6947);
        slice_7846 = m_6139;
        slice_7847 = n_6137 * slice_7846;
        gtid_6950 = squot64(squot64(sext_i32_i64(local_tid_7836), segment_sizze_nonzzero_7833) + sext_i32_i64(virt_tblock_id_7845) * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833), slice_7846);
        remnant_7848 = squot64(sext_i32_i64(local_tid_7836), segment_sizze_nonzzero_7833) + sext_i32_i64(virt_tblock_id_7845) * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833) - gtid_6950 * slice_7846;
        gtid_6951 = remnant_7848;
        remnant_7849 = remnant_7848 - gtid_6951;
        gtid_6952 = srem64(sext_i32_i64(local_tid_7836), k_6138);
        // apply map function if in bounds
        if (slt64((int64_t) 0, k_6138) && ((slt64(gtid_6950, n_6137) && slt64(gtid_6951, m_6139)) && slt64(sext_i32_i64(local_tid_7836), k_6138 * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833)))) {
            double eta_p_6956;
            double eta_p_6957;
            double defunc_0_f_res_6958;

            // apply map function
            eta_p_6956 = ((__global double *) a_mem_7471)[gtid_6950 * k_6138 + gtid_6952];
            eta_p_6957 = ((__global double *) mem_7484)[gtid_6952 + gtid_6951 * k_6138];
            // sample_kernel.fut:8:36-39
            defunc_0_f_res_6958 = eta_p_6956 * eta_p_6957;
            // save results to be reduced
            ((__local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)] = defunc_0_f_res_6958;
        } else {
            ((__local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)] = 0.0;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        if (slt64((int64_t) 0, k_6138)) {
            double eta_p_6959;
            double eta_p_6960;
            double eta_p_7850;
            double eta_p_7851;
            bool ltid_in_bounds_7853;
            int32_t skip_threads_7854;
            int32_t skip_threads_7857;
            bool no_carry_in_7860;
            bool inactive_7861;

            // perform segmented scan to imitate reduction
            ltid_in_bounds_7853 = slt64(sext_i32_i64(local_tid_7836), k_6138 * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833));
            // read input for in-block scan
            if (ltid_in_bounds_7853) {
                eta_p_6960 = ((volatile __local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)];
                if ((local_tid_7836 - squot32(local_tid_7836, 32) * 32) == 0) {
                    eta_p_6959 = eta_p_6960;
                }
            }
            // in-block scan (hopefully no barriers needed)
            skip_threads_7854 = 1;
            while (slt32(skip_threads_7854, 32)) {
                bool thread_active_7855;
                bool inactive_7856;

                thread_active_7855 = sle32(skip_threads_7854, local_tid_7836 - squot32(local_tid_7836, 32) * 32) && ltid_in_bounds_7853;
                if (thread_active_7855) {
                    // read operands
                    eta_p_6959 = ((volatile __local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836) - sext_i32_i64(skip_threads_7854)];
                }
                // perform operation
                inactive_7856 = slt64(srem64(sext_i32_i64(local_tid_7836), k_6138), sext_i32_i64(local_tid_7836) - sext_i32_i64(local_tid_7836 - skip_threads_7854));
                if (thread_active_7855 && inactive_7856) {
                    eta_p_6959 = eta_p_6960;
                }
                if (thread_active_7855) {
                    if (!inactive_7856) {
                        double defunc_0_op_res_6961;

                        // sample_kernel.fut:8:22-25
                        defunc_0_op_res_6961 = eta_p_6959 + eta_p_6960;
                        eta_p_6959 = defunc_0_op_res_6961;
                    }
                }
                if (sle32(wave_sizze_7838, skip_threads_7854)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                if (thread_active_7855) {
                    // write result
                    ((volatile __local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)] = eta_p_6959;
                    eta_p_6960 = eta_p_6959;
                }
                if (sle32(wave_sizze_7838, skip_threads_7854)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                skip_threads_7854 *= 2;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            // last thread of block 'i' writes its result to offset 'i'
            if ((local_tid_7836 - squot32(local_tid_7836, 32) * 32) == 31 && ltid_in_bounds_7853) {
                ((volatile __local double *) red_arr_f64_mem_7840)[sext_i32_i64(squot32(local_tid_7836, 32))] = eta_p_6959;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            // scan the first block, after which offset 'i' contains carry-in for block 'i+1'
            // read input for in-block scan
            if (squot32(local_tid_7836, 32) == 0 && ltid_in_bounds_7853) {
                eta_p_7851 = ((volatile __local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)];
                if ((local_tid_7836 - squot32(local_tid_7836, 32) * 32) == 0) {
                    eta_p_7850 = eta_p_7851;
                }
            }
            // in-block scan (hopefully no barriers needed)
            skip_threads_7857 = 1;
            while (slt32(skip_threads_7857, 32)) {
                bool thread_active_7858;
                bool inactive_7859;

                thread_active_7858 = sle32(skip_threads_7857, local_tid_7836 - squot32(local_tid_7836, 32) * 32) && (squot32(local_tid_7836, 32) == 0 && ltid_in_bounds_7853);
                if (thread_active_7858) {
                    // read operands
                    eta_p_7850 = ((volatile __local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836) - sext_i32_i64(skip_threads_7857)];
                }
                // perform operation
                inactive_7859 = slt64(srem64(sext_i32_i64(local_tid_7836 * 32 + 32 - 1), k_6138), sext_i32_i64(local_tid_7836 * 32 + 32 - 1) - sext_i32_i64((local_tid_7836 - skip_threads_7857) * 32 + 32 - 1));
                if (thread_active_7858 && inactive_7859) {
                    eta_p_7850 = eta_p_7851;
                }
                if (thread_active_7858) {
                    if (!inactive_7859) {
                        double defunc_0_op_res_7852;

                        // sample_kernel.fut:8:22-25
                        defunc_0_op_res_7852 = eta_p_7850 + eta_p_7851;
                        eta_p_7850 = defunc_0_op_res_7852;
                    }
                }
                if (sle32(wave_sizze_7838, skip_threads_7857)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                if (thread_active_7858) {
                    // write result
                    ((volatile __local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)] = eta_p_7850;
                    eta_p_7851 = eta_p_7850;
                }
                if (sle32(wave_sizze_7838, skip_threads_7857)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                skip_threads_7857 *= 2;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            no_carry_in_7860 = squot32(local_tid_7836, 32) == 0 || !ltid_in_bounds_7853;
            // carry-in for every block except the first
            // read operands
            if (!no_carry_in_7860) {
                eta_p_6960 = eta_p_6959;
                eta_p_6959 = ((__local double *) red_arr_f64_mem_7840)[sext_i32_i64(squot32(local_tid_7836, 32)) - (int64_t) 1];
            }
            // perform operation
            inactive_7861 = slt64(srem64(sext_i32_i64(local_tid_7836), k_6138), sext_i32_i64(local_tid_7836) - sext_i32_i64(squot32(local_tid_7836, 32) * 32 - 1));
            if (!no_carry_in_7860) {
                if (inactive_7861) {
                    eta_p_6959 = eta_p_6960;
                }
            }
            if (!no_carry_in_7860) {
                if (!inactive_7861) {
                    double defunc_0_op_res_6961;

                    // sample_kernel.fut:8:22-25
                    defunc_0_op_res_6961 = eta_p_6959 + eta_p_6960;
                    eta_p_6959 = defunc_0_op_res_6961;
                }
            }
            // write final result
            if (!no_carry_in_7860) {
                ((__local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)] = eta_p_6959;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            // restore correct values for first block
            if (squot32(local_tid_7836, 32) == 0 && ltid_in_bounds_7853) {
                ((__local double *) red_arr_f64_mem_7840)[sext_i32_i64(local_tid_7836)] = eta_p_6960;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        // save final values of segments
        if (slt64(sext_i32_i64(virt_tblock_id_7845) * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833) + sext_i32_i64(local_tid_7836), n_6137 * m_6139) && slt64(sext_i32_i64(local_tid_7836), squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833))) {
            double tmp_7862 = ((__local double *) red_arr_f64_mem_7840)[(sext_i32_i64(local_tid_7836) + (int64_t) 1) * segment_sizze_nonzzero_7833 - (int64_t) 1];

            ((__global double *) mem_7492)[squot64(sext_i32_i64(virt_tblock_id_7845) * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833) + sext_i32_i64(local_tid_7836), m_6139) * m_6139 + (sext_i32_i64(virt_tblock_id_7845) * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833) + sext_i32_i64(local_tid_7836) - squot64(sext_i32_i64(virt_tblock_id_7845) * squot64(segred_tblock_sizze_6946, segment_sizze_nonzzero_7833) + sext_i32_i64(local_tid_7836), m_6139) * m_6139)] = tmp_7862;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    }

  error_3:
    return;
    #undef segred_tblock_sizze_6946
}
FUTHARK_KERNEL_SIZED(matmul_bias_relu_sumzisegred_small_6984_dim1, 1, 1)
void matmul_bias_relu_sumzisegred_small_6984(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_6137, int64_t m_6139, int64_t num_tblocks_6979, int64_t segment_sizze_nonzzero_7921, __global unsigned char *ext_mem_7618, __global unsigned char *mem_7621)
{
    #define segred_tblock_sizze_6978 (matmul_bias_relu_sumzisegred_small_6984zisegred_tblock_sizze_6978)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *red_arr_f64_mem_7928_backing_0 = &shared_mem[0];
    const int64_t red_arr_f64_mem_7928_backing_0_offset = 0 + ((int64_t) 8 * segred_tblock_sizze_6978 + srem64((int64_t) 8 - srem64((int64_t) 8 * segred_tblock_sizze_6978, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7924;
    int32_t tblock_sizze_7927;
    int32_t wave_sizze_7926;
    int32_t block_id_7925;
    int32_t global_tid_7923;
    int64_t phys_tid_6984;
    __local unsigned char *red_arr_f64_mem_7928;
    int32_t phys_tblock_id_7930;
    int32_t iterations_7931;

    local_tid_7924 = get_local_id(0);
    tblock_sizze_7927 = get_local_size(0);
    wave_sizze_7926 = LOCKSTEP_WIDTH;
    block_id_7925 = get_tblock_id(0);
    global_tid_7923 = block_id_7925 * tblock_sizze_7927 + local_tid_7924;
    phys_tid_6984 = sext_i32_i64(global_tid_7923);
    red_arr_f64_mem_7928 = (__local unsigned char *) red_arr_f64_mem_7928_backing_0;
    phys_tblock_id_7930 = get_tblock_id(0);
    iterations_7931 = sdiv_up32(sext_i64_i32(sdiv_up64(n_6137, squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921))) - phys_tblock_id_7930, sext_i64_i32(num_tblocks_6979));
    for (int32_t i_7932 = 0; i_7932 < iterations_7931; i_7932++) {
        int32_t virt_tblock_id_7933;
        int64_t slice_7934;
        int64_t gtid_6982;
        int64_t remnant_7935;
        int64_t gtid_6983;

        virt_tblock_id_7933 = phys_tblock_id_7930 + i_7932 * sext_i64_i32(num_tblocks_6979);
        slice_7934 = n_6137;
        gtid_6982 = squot64(sext_i32_i64(local_tid_7924), segment_sizze_nonzzero_7921) + sext_i32_i64(virt_tblock_id_7933) * squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921);
        remnant_7935 = squot64(sext_i32_i64(local_tid_7924), segment_sizze_nonzzero_7921) + sext_i32_i64(virt_tblock_id_7933) * squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921) - gtid_6982;
        gtid_6983 = srem64(sext_i32_i64(local_tid_7924), m_6139);
        // apply map function if in bounds
        if (slt64((int64_t) 0, m_6139) && (slt64(gtid_6982, n_6137) && slt64(sext_i32_i64(local_tid_7924), m_6139 * squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921)))) {
            double x_6986;

            // apply map function
            x_6986 = ((__global double *) ext_mem_7618)[gtid_6982 * m_6139 + gtid_6983];
            // save results to be reduced
            ((__local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)] = x_6986;
        } else {
            ((__local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)] = 0.0;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        if (slt64((int64_t) 0, m_6139)) {
            double eta_p_6987;
            double eta_p_6988;
            double eta_p_7936;
            double eta_p_7937;
            bool ltid_in_bounds_7939;
            int32_t skip_threads_7940;
            int32_t skip_threads_7943;
            bool no_carry_in_7946;
            bool inactive_7947;

            // perform segmented scan to imitate reduction
            ltid_in_bounds_7939 = slt64(sext_i32_i64(local_tid_7924), m_6139 * squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921));
            // read input for in-block scan
            if (ltid_in_bounds_7939) {
                eta_p_6988 = ((volatile __local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)];
                if ((local_tid_7924 - squot32(local_tid_7924, 32) * 32) == 0) {
                    eta_p_6987 = eta_p_6988;
                }
            }
            // in-block scan (hopefully no barriers needed)
            skip_threads_7940 = 1;
            while (slt32(skip_threads_7940, 32)) {
                bool thread_active_7941;
                bool inactive_7942;

                thread_active_7941 = sle32(skip_threads_7940, local_tid_7924 - squot32(local_tid_7924, 32) * 32) && ltid_in_bounds_7939;
                if (thread_active_7941) {
                    // read operands
                    eta_p_6987 = ((volatile __local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924) - sext_i32_i64(skip_threads_7940)];
                }
                // perform operation
                inactive_7942 = slt64(srem64(sext_i32_i64(local_tid_7924), m_6139), sext_i32_i64(local_tid_7924) - sext_i32_i64(local_tid_7924 - skip_threads_7940));
                if (thread_active_7941 && inactive_7942) {
                    eta_p_6987 = eta_p_6988;
                }
                if (thread_active_7941) {
                    if (!inactive_7942) {
                        double defunc_0_op_res_6989;

                        // sample_kernel.fut:19:34-37
                        defunc_0_op_res_6989 = eta_p_6987 + eta_p_6988;
                        eta_p_6987 = defunc_0_op_res_6989;
                    }
                }
                if (sle32(wave_sizze_7926, skip_threads_7940)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                if (thread_active_7941) {
                    // write result
                    ((volatile __local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)] = eta_p_6987;
                    eta_p_6988 = eta_p_6987;
                }
                if (sle32(wave_sizze_7926, skip_threads_7940)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                skip_threads_7940 *= 2;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            // last thread of block 'i' writes its result to offset 'i'
            if ((local_tid_7924 - squot32(local_tid_7924, 32) * 32) == 31 && ltid_in_bounds_7939) {
                ((volatile __local double *) red_arr_f64_mem_7928)[sext_i32_i64(squot32(local_tid_7924, 32))] = eta_p_6987;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            // scan the first block, after which offset 'i' contains carry-in for block 'i+1'
            // read input for in-block scan
            if (squot32(local_tid_7924, 32) == 0 && ltid_in_bounds_7939) {
                eta_p_7937 = ((volatile __local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)];
                if ((local_tid_7924 - squot32(local_tid_7924, 32) * 32) == 0) {
                    eta_p_7936 = eta_p_7937;
                }
            }
            // in-block scan (hopefully no barriers needed)
            skip_threads_7943 = 1;
            while (slt32(skip_threads_7943, 32)) {
                bool thread_active_7944;
                bool inactive_7945;

                thread_active_7944 = sle32(skip_threads_7943, local_tid_7924 - squot32(local_tid_7924, 32) * 32) && (squot32(local_tid_7924, 32) == 0 && ltid_in_bounds_7939);
                if (thread_active_7944) {
                    // read operands
                    eta_p_7936 = ((volatile __local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924) - sext_i32_i64(skip_threads_7943)];
                }
                // perform operation
                inactive_7945 = slt64(srem64(sext_i32_i64(local_tid_7924 * 32 + 32 - 1), m_6139), sext_i32_i64(local_tid_7924 * 32 + 32 - 1) - sext_i32_i64((local_tid_7924 - skip_threads_7943) * 32 + 32 - 1));
                if (thread_active_7944 && inactive_7945) {
                    eta_p_7936 = eta_p_7937;
                }
                if (thread_active_7944) {
                    if (!inactive_7945) {
                        double defunc_0_op_res_7938;

                        // sample_kernel.fut:19:34-37
                        defunc_0_op_res_7938 = eta_p_7936 + eta_p_7937;
                        eta_p_7936 = defunc_0_op_res_7938;
                    }
                }
                if (sle32(wave_sizze_7926, skip_threads_7943)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                if (thread_active_7944) {
                    // write result
                    ((volatile __local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)] = eta_p_7936;
                    eta_p_7937 = eta_p_7936;
                }
                if (sle32(wave_sizze_7926, skip_threads_7943)) {
                    barrier(CLK_LOCAL_MEM_FENCE);
                }
                skip_threads_7943 *= 2;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            no_carry_in_7946 = squot32(local_tid_7924, 32) == 0 || !ltid_in_bounds_7939;
            // carry-in for every block except the first
            // read operands
            if (!no_carry_in_7946) {
                eta_p_6988 = eta_p_6987;
                eta_p_6987 = ((__local double *) red_arr_f64_mem_7928)[sext_i32_i64(squot32(local_tid_7924, 32)) - (int64_t) 1];
            }
            // perform operation
            inactive_7947 = slt64(srem64(sext_i32_i64(local_tid_7924), m_6139), sext_i32_i64(local_tid_7924) - sext_i32_i64(squot32(local_tid_7924, 32) * 32 - 1));
            if (!no_carry_in_7946) {
                if (inactive_7947) {
                    eta_p_6987 = eta_p_6988;
                }
            }
            if (!no_carry_in_7946) {
                if (!inactive_7947) {
                    double defunc_0_op_res_6989;

                    // sample_kernel.fut:19:34-37
                    defunc_0_op_res_6989 = eta_p_6987 + eta_p_6988;
                    eta_p_6987 = defunc_0_op_res_6989;
                }
            }
            // write final result
            if (!no_carry_in_7946) {
                ((__local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)] = eta_p_6987;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
            // restore correct values for first block
            if (squot32(local_tid_7924, 32) == 0 && ltid_in_bounds_7939) {
                ((__local double *) red_arr_f64_mem_7928)[sext_i32_i64(local_tid_7924)] = eta_p_6988;
            }
            barrier(CLK_LOCAL_MEM_FENCE);
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        // save final values of segments
        if (slt64(sext_i32_i64(virt_tblock_id_7933) * squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921) + sext_i32_i64(local_tid_7924), n_6137) && slt64(sext_i32_i64(local_tid_7924), squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921))) {
            double tmp_7948 = ((__local double *) red_arr_f64_mem_7928)[(sext_i32_i64(local_tid_7924) + (int64_t) 1) * segment_sizze_nonzzero_7921 - (int64_t) 1];

            ((__global double *) mem_7621)[sext_i32_i64(virt_tblock_id_7933) * squot64(segred_tblock_sizze_6978, segment_sizze_nonzzero_7921) + sext_i32_i64(local_tid_7924)] = tmp_7948;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    }

  error_3:
    return;
    #undef segred_tblock_sizze_6978
}
FUTHARK_KERNEL_SIZED(sum_squareszisegred_nonseg_6524_dim1, 1, 1)
void sum_squareszisegred_nonseg_6524(__local uint64_t *shared_mem_aligned, __global int *global_failure, int64_t n_5766, int64_t num_tblocks_6519, int64_t num_threads_7714, __global unsigned char *xs_mem_7471, __global unsigned char *mem_7473, __global unsigned char *counters_mem_7690, __global unsigned char *segred_tmp_mem_7712)
{
    #define segred_tblock_sizze_6517 (sum_squareszisegred_nonseg_6524zisegred_tblock_sizze_6517)
    #define chunk_sizze_7689 (sum_squareszisegred_nonseg_6524zichunk_sizze_7689)

    __local unsigned char *shared_mem = (__local unsigned char *) shared_mem_aligned;
    volatile __local unsigned char *sync_arr_mem_7722_backing_1 = &shared_mem[0];
    const int64_t sync_arr_mem_7722_backing_1_offset = 0 + 8;
    volatile __local unsigned char *red_arr_f64_mem_7720_backing_0 = &shared_mem[sync_arr_mem_7722_backing_1_offset];
    const int64_t red_arr_f64_mem_7720_backing_0_offset = sync_arr_mem_7722_backing_1_offset + ((int64_t) 8 * segred_tblock_sizze_6517 + srem64((int64_t) 8 - srem64((int64_t) 8 * segred_tblock_sizze_6517, (int64_t) 8), (int64_t) 8));

    if (*global_failure >= 0)
        return;

    int32_t local_tid_7716;
    int32_t tblock_sizze_7719;
    int32_t wave_sizze_7718;
    int32_t block_id_7717;
    int32_t global_tid_7715;
    int64_t phys_tid_6524;
    __local unsigned char *red_arr_f64_mem_7720;
    __local unsigned char *sync_arr_mem_7722;
    int64_t dummy_6522;
    int64_t gtid_6523;
    int64_t q_7724;
    double eta_p_block_res_acc_7725;
    double eta_p_6205;
    double eta_p_6206;
    int64_t tblock_id_in_segment_7729;
    int64_t block_base_offset_7730;
    int32_t offset_7733;
    int32_t skip_waves_7734;
    double eta_p_7726;
    double eta_p_7727;
    int32_t old_counter_7735;
    bool is_last_block_7736;

    local_tid_7716 = get_local_id(0);
    tblock_sizze_7719 = get_local_size(0);
    wave_sizze_7718 = LOCKSTEP_WIDTH;
    block_id_7717 = get_tblock_id(0);
    global_tid_7715 = block_id_7717 * tblock_sizze_7719 + local_tid_7716;
    phys_tid_6524 = sext_i32_i64(global_tid_7715);
    red_arr_f64_mem_7720 = (__local unsigned char *) red_arr_f64_mem_7720_backing_0;
    sync_arr_mem_7722 = (__local unsigned char *) sync_arr_mem_7722_backing_1;
    dummy_6522 = (int64_t) 0;
    gtid_6523 = (int64_t) 0;
    q_7724 = sdiv_up64(n_5766, sext_i32_i64(sext_i64_i32(segred_tblock_sizze_6517 * num_tblocks_6519)) * chunk_sizze_7689);
    // ne-initialise the outer (per-block) accumulator(s)
    eta_p_block_res_acc_7725 = 0.0;
    tblock_id_in_segment_7729 = squot64(phys_tid_6524, segred_tblock_sizze_6517);
    block_base_offset_7730 = tblock_id_in_segment_7729 * q_7724 * segred_tblock_sizze_6517;
    for (int64_t i_7731 = 0; i_7731 < q_7724; i_7731++) {
        int64_t block_offset_7732 = block_base_offset_7730 + i_7731 * segred_tblock_sizze_6517;

        gtid_6523 = phys_tid_6524 + num_threads_7714 * i_7731;
        if (slt64(gtid_6523, n_5766)) {
            double eta_p_6405;
            double lifted_lambda_res_6406;
            double defunc_0_op_res_6207;

            // apply map function(s)
            // apply map function
            eta_p_6405 = ((__global double *) xs_mem_7471)[gtid_6523];
            // sample_kernel.fut:3:31-33
            lifted_lambda_res_6406 = eta_p_6405 * eta_p_6405;
            // load accumulator(s)
            eta_p_6205 = eta_p_block_res_acc_7725;
            // load next value(s)
            eta_p_6206 = lifted_lambda_res_6406;
            // apply reduction operator(s)
            // sample_kernel.fut:3:10-13
            defunc_0_op_res_6207 = eta_p_6205 + eta_p_6206;
            // store in accumulator(s)
            eta_p_block_res_acc_7725 = defunc_0_op_res_6207;
        }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    // store accs. prims go in lmem; non-prims in params (in global mem)
    ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716)] = eta_p_block_res_acc_7725;
    barrier(CLK_LOCAL_MEM_FENCE);
    skip_waves_7734 = 1;
    offset_7733 = 0;
    // participating threads read initial accumulator
    if (slt32(local_tid_7716, sext_i64_i32(segred_tblock_sizze_6517))) {
        eta_p_7726 = ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716 + offset_7733)];
    }
    offset_7733 = 1;
    while (slt32(offset_7733, wave_sizze_7718)) {
        if (slt32(local_tid_7716 + offset_7733, sext_i64_i32(segred_tblock_sizze_6517)) && ((local_tid_7716 - squot32(local_tid_7716, wave_sizze_7718) * wave_sizze_7718) & (2 * offset_7733 - 1)) == 0) {
            double defunc_0_op_res_7728;

            // read array element
            eta_p_7727 = ((volatile __local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716 + offset_7733)];
            // apply reduction operation
            // sample_kernel.fut:3:10-13
            defunc_0_op_res_7728 = eta_p_7726 + eta_p_7727;
            eta_p_7726 = defunc_0_op_res_7728;
            // write result of operation
            ((volatile __local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716)] = eta_p_7726;
        }
        offset_7733 *= 2;
    }
    while (slt32(skip_waves_7734, squot32(sext_i64_i32(segred_tblock_sizze_6517) + wave_sizze_7718 - 1, wave_sizze_7718))) {
        barrier(CLK_LOCAL_MEM_FENCE);
        offset_7733 = skip_waves_7734 * wave_sizze_7718;
        if (slt32(local_tid_7716 + offset_7733, sext_i64_i32(segred_tblock_sizze_6517)) && ((local_tid_7716 - squot32(local_tid_7716, wave_sizze_7718) * wave_sizze_7718) == 0 && (squot32(local_tid_7716, wave_sizze_7718) & (2 * skip_waves_7734 - 1)) == 0)) {
            double defunc_0_op_res_7728;

            // read array element
            eta_p_7727 = ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716 + offset_7733)];
            // apply reduction operation
            // sample_kernel.fut:3:10-13
            defunc_0_op_res_7728 = eta_p_7726 + eta_p_7727;
            eta_p_7726 = defunc_0_op_res_7728;
            // write result of operation
            ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716)] = eta_p_7726;
        }
        skip_waves_7734 *= 2;
    }
    barrier(CLK_LOCAL_MEM_FENCE);
    barrier(CLK_LOCAL_MEM_FENCE);
    // thread 0 updates per-block acc(s); rest reset to ne
    if (sext_i32_i64(local_tid_7716) == (int64_t) 0) {
        eta_p_block_res_acc_7725 = eta_p_7726;
    } else {
        eta_p_block_res_acc_7725 = 0.0;
    }
    // first thread in block saves block result to global memory
    if (local_tid_7716 == 0) {
        ((__global double *) segred_tmp_mem_7712)[sext_i32_i64(block_id_7717)] = eta_p_block_res_acc_7725;
        mem_fence_global();
        old_counter_7735 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7690)[(int64_t) 0], 1);
        ((__local bool *) sync_arr_mem_7722)[(int64_t) 0] = old_counter_7735 == sext_i64_i32(num_tblocks_6519 - (int64_t) 1);
    }
    barrier(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM_FENCE);
    is_last_block_7736 = ((__local bool *) sync_arr_mem_7722)[(int64_t) 0];
    if (is_last_block_7736) {
        int64_t read_per_thread_7737;
        int32_t offset_7741;
        int32_t skip_waves_7742;
        double eta_p_7726;
        double eta_p_7727;

        if (local_tid_7716 == 0) {
            old_counter_7735 = atomic_add_i32_global(&((volatile __global int *) counters_mem_7690)[(int64_t) 0], sext_i64_i32((int64_t) 0 - num_tblocks_6519));
        }
        // read in the per-block-results
        read_per_thread_7737 = sdiv_up64(num_tblocks_6519, segred_tblock_sizze_6517);
        eta_p_6205 = 0.0;
        for (int64_t i_7738 = 0; i_7738 < read_per_thread_7737; i_7738++) {
            int64_t block_res_id_7739;
            int64_t index_of_block_res_7740;

            block_res_id_7739 = sext_i32_i64(local_tid_7716) * read_per_thread_7737 + i_7738;
            index_of_block_res_7740 = block_res_id_7739;
            if (slt64(block_res_id_7739, num_tblocks_6519)) {
                double defunc_0_op_res_6207;

                eta_p_6206 = ((__global double *) segred_tmp_mem_7712)[index_of_block_res_7740];
                // sample_kernel.fut:3:10-13
                defunc_0_op_res_6207 = eta_p_6205 + eta_p_6206;
                eta_p_6205 = defunc_0_op_res_6207;
            }
        }
        ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716)] = eta_p_6205;
        barrier(CLK_LOCAL_MEM_FENCE);
        // reduce the per-block results
        skip_waves_7742 = 1;
        offset_7741 = 0;
        // participating threads read initial accumulator
        if (slt32(local_tid_7716, sext_i64_i32(segred_tblock_sizze_6517))) {
            eta_p_7726 = ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716 + offset_7741)];
        }
        offset_7741 = 1;
        while (slt32(offset_7741, wave_sizze_7718)) {
            if (slt32(local_tid_7716 + offset_7741, sext_i64_i32(segred_tblock_sizze_6517)) && ((local_tid_7716 - squot32(local_tid_7716, wave_sizze_7718) * wave_sizze_7718) & (2 * offset_7741 - 1)) == 0) {
                double defunc_0_op_res_7728;

                // read array element
                eta_p_7727 = ((volatile __local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716 + offset_7741)];
                // apply reduction operation
                // sample_kernel.fut:3:10-13
                defunc_0_op_res_7728 = eta_p_7726 + eta_p_7727;
                eta_p_7726 = defunc_0_op_res_7728;
                // write result of operation
                ((volatile __local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716)] = eta_p_7726;
            }
            offset_7741 *= 2;
        }
        while (slt32(skip_waves_7742, squot32(sext_i64_i32(segred_tblock_sizze_6517) + wave_sizze_7718 - 1, wave_sizze_7718))) {
            barrier(CLK_LOCAL_MEM_FENCE);
            offset_7741 = skip_waves_7742 * wave_sizze_7718;
            if (slt32(local_tid_7716 + offset_7741, sext_i64_i32(segred_tblock_sizze_6517)) && ((local_tid_7716 - squot32(local_tid_7716, wave_sizze_7718) * wave_sizze_7718) == 0 && (squot32(local_tid_7716, wave_sizze_7718) & (2 * skip_waves_7742 - 1)) == 0)) {
                double defunc_0_op_res_7728;

                // read array element
                eta_p_7727 = ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716 + offset_7741)];
                // apply reduction operation
                // sample_kernel.fut:3:10-13
                defunc_0_op_res_7728 = eta_p_7726 + eta_p_7727;
                eta_p_7726 = defunc_0_op_res_7728;
                // write result of operation
                ((__local double *) red_arr_f64_mem_7720)[sext_i32_i64(local_tid_7716)] = eta_p_7726;
            }
            skip_waves_7742 *= 2;
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        // and back to memory with the final result
        if (local_tid_7716 == 0) {
            ((__global double *) mem_7473)[(int64_t) 0] = eta_p_7726;
        }
    }

  error_5:
    return;
    #undef segred_tblock_sizze_6517
    #undef chunk_sizze_7689
}
"""
# Start of values.py.

# Hacky parser/reader/writer for values written in Futhark syntax.
# Used for reading stdin when compiling standalone programs with the
# Python code generator.

import numpy as np
import string
import struct
import sys


class ReaderInput:
    def __init__(self, f):
        self.f = f
        self.lookahead_buffer = []

    def get_char(self):
        if len(self.lookahead_buffer) == 0:
            return self.f.read(1)
        else:
            c = self.lookahead_buffer[0]
            self.lookahead_buffer = self.lookahead_buffer[1:]
            return c

    def unget_char(self, c):
        self.lookahead_buffer = [c] + self.lookahead_buffer

    def get_chars(self, n):
        n1 = min(n, len(self.lookahead_buffer))
        s = b"".join(self.lookahead_buffer[:n1])
        self.lookahead_buffer = self.lookahead_buffer[n1:]
        n2 = n - n1
        if n2 > 0:
            s += self.f.read(n2)
        return s

    def peek_char(self):
        c = self.get_char()
        if c:
            self.unget_char(c)
        return c


def skip_spaces(f):
    c = f.get_char()
    while c != None:
        if c.isspace():
            c = f.get_char()
        elif c == b"-":
            # May be line comment.
            if f.peek_char() == b"-":
                # Yes, line comment. Skip to end of line.
                while c != b"\n" and c != None:
                    c = f.get_char()
            else:
                break
        else:
            break
    if c:
        f.unget_char(c)


def parse_specific_char(f, expected):
    got = f.get_char()
    if got != expected:
        f.unget_char(got)
        raise ValueError
    return True


def parse_specific_string(f, s):
    # This funky mess is intended, and is caused by the fact that if `type(b) ==
    # bytes` then `type(b[0]) == int`, but we need to match each element with a
    # `bytes`, so therefore we make each character an array element
    b = s.encode("utf8")
    bs = [b[i : i + 1] for i in range(len(b))]
    read = []
    try:
        for c in bs:
            parse_specific_char(f, c)
            read.append(c)
        return True
    except ValueError:
        for c in read[::-1]:
            f.unget_char(c)
        raise


def optional(p, *args):
    try:
        return p(*args)
    except ValueError:
        return None


def optional_specific_string(f, s):
    c = f.peek_char()
    # This funky mess is intended, and is caused by the fact that if `type(b) ==
    # bytes` then `type(b[0]) == int`, but we need to match each element with a
    # `bytes`, so therefore we make each character an array element
    b = s.encode("utf8")
    bs = [b[i : i + 1] for i in range(len(b))]
    if c == bs[0]:
        return parse_specific_string(f, s)
    else:
        return False


def sepEndBy(p, sep, *args):
    elems = []
    x = optional(p, *args)
    if x != None:
        elems += [x]
        while optional(sep, *args) != None:
            x = optional(p, *args)
            if x == None:
                break
            else:
                elems += [x]
    return elems


# Assumes '0x' has already been read
def parse_hex_int(f):
    s = b""
    c = f.get_char()
    while c != None:
        if c in b"01234556789ABCDEFabcdef":
            s += c
            c = f.get_char()
        elif c == b"_":
            c = f.get_char()  # skip _
        else:
            f.unget_char(c)
            break
    return str(int(s, 16)).encode("utf8")  # ugh


def parse_int(f):
    s = b""
    c = f.get_char()
    if c == b"0" and f.peek_char() in b"xX":
        c = f.get_char()  # skip X
        return parse_hex_int(f)
    else:
        while c != None:
            if c.isdigit():
                s += c
                c = f.get_char()
            elif c == b"_":
                c = f.get_char()  # skip _
            else:
                f.unget_char(c)
                break
        if len(s) == 0:
            raise ValueError
        return s


def parse_int_signed(f):
    s = b""
    c = f.get_char()

    if c == b"-" and f.peek_char().isdigit():
        return c + parse_int(f)
    else:
        if c != b"+":
            f.unget_char(c)
        return parse_int(f)


def read_str_comma(f):
    skip_spaces(f)
    parse_specific_char(f, b",")
    return b","


def read_str_int(f, s):
    skip_spaces(f)
    x = int(parse_int_signed(f))
    optional_specific_string(f, s)
    return x


def read_str_uint(f, s):
    skip_spaces(f)
    x = int(parse_int(f))
    optional_specific_string(f, s)
    return x


def read_str_i8(f):
    return np.int8(read_str_int(f, "i8"))


def read_str_i16(f):
    return np.int16(read_str_int(f, "i16"))


def read_str_i32(f):
    return np.int32(read_str_int(f, "i32"))


def read_str_i64(f):
    return np.int64(read_str_int(f, "i64"))


def read_str_u8(f):
    return np.uint8(read_str_int(f, "u8"))


def read_str_u16(f):
    return np.uint16(read_str_int(f, "u16"))


def read_str_u32(f):
    return np.uint32(read_str_int(f, "u32"))


def read_str_u64(f):
    return np.uint64(read_str_int(f, "u64"))


def read_char(f):
    skip_spaces(f)
    parse_specific_char(f, b"'")
    c = f.get_char()
    parse_specific_char(f, b"'")
    return c


def read_str_hex_float(f, sign):
    int_part = parse_hex_int(f)
    parse_specific_char(f, b".")
    frac_part = parse_hex_int(f)
    parse_specific_char(f, b"p")
    exponent = parse_int(f)

    int_val = int(int_part, 16)
    frac_val = float(int(frac_part, 16)) / (16 ** len(frac_part))
    exp_val = int(exponent)

    total_val = (int_val + frac_val) * (2.0**exp_val)
    if sign == b"-":
        total_val = -1 * total_val

    return float(total_val)


def read_str_decimal(f):
    skip_spaces(f)
    c = f.get_char()
    if c == b"-":
        sign = b"-"
    else:
        f.unget_char(c)
        sign = b""

    # Check for hexadecimal float
    c = f.get_char()
    if c == "0" and (f.peek_char() in ["x", "X"]):
        f.get_char()
        return read_str_hex_float(f, sign)
    else:
        f.unget_char(c)

    bef = optional(parse_int, f)
    if bef == None:
        bef = b"0"
        parse_specific_char(f, b".")
        aft = parse_int(f)
    elif optional(parse_specific_char, f, b"."):
        aft = parse_int(f)
    else:
        aft = b"0"
    if optional(parse_specific_char, f, b"E") or optional(
        parse_specific_char, f, b"e"
    ):
        expt = parse_int_signed(f)
    else:
        expt = b"0"
    return float(sign + bef + b"." + aft + b"E" + expt)


def read_str_f16(f):
    skip_spaces(f)
    try:
        parse_specific_string(f, "f16.nan")
        return np.float32(np.nan)
    except ValueError:
        try:
            parse_specific_string(f, "f16.inf")
            return np.float32(np.inf)
        except ValueError:
            try:
                parse_specific_string(f, "-f16.inf")
                return np.float32(-np.inf)
            except ValueError:
                x = read_str_decimal(f)
                optional_specific_string(f, "f16")
                return x


def read_str_f32(f):
    skip_spaces(f)
    try:
        parse_specific_string(f, "f32.nan")
        return np.float32(np.nan)
    except ValueError:
        try:
            parse_specific_string(f, "f32.inf")
            return np.float32(np.inf)
        except ValueError:
            try:
                parse_specific_string(f, "-f32.inf")
                return np.float32(-np.inf)
            except ValueError:
                x = read_str_decimal(f)
                optional_specific_string(f, "f32")
                return x


def read_str_f64(f):
    skip_spaces(f)
    try:
        parse_specific_string(f, "f64.nan")
        return np.float64(np.nan)
    except ValueError:
        try:
            parse_specific_string(f, "f64.inf")
            return np.float64(np.inf)
        except ValueError:
            try:
                parse_specific_string(f, "-f64.inf")
                return np.float64(-np.inf)
            except ValueError:
                x = read_str_decimal(f)
                optional_specific_string(f, "f64")
                return x


def read_str_bool(f):
    skip_spaces(f)
    if f.peek_char() == b"t":
        parse_specific_string(f, "true")
        return True
    elif f.peek_char() == b"f":
        parse_specific_string(f, "false")
        return False
    else:
        raise ValueError


def read_str_empty_array(f, type_name, rank):
    parse_specific_string(f, "empty")
    parse_specific_char(f, b"(")
    dims = []
    for i in range(rank):
        parse_specific_string(f, "[")
        dims += [int(parse_int(f))]
        parse_specific_string(f, "]")
    if np.prod(dims) != 0:
        raise ValueError
    parse_specific_string(f, type_name)
    parse_specific_char(f, b")")

    return tuple(dims)


def read_str_array_elems(f, elem_reader, type_name, rank):
    skip_spaces(f)
    try:
        parse_specific_char(f, b"[")
    except ValueError:
        return read_str_empty_array(f, type_name, rank)
    else:
        xs = sepEndBy(elem_reader, read_str_comma, f)
        skip_spaces(f)
        parse_specific_char(f, b"]")
        return xs


def read_str_array_helper(f, elem_reader, type_name, rank):
    def nested_row_reader(_):
        return read_str_array_helper(f, elem_reader, type_name, rank - 1)

    if rank == 1:
        row_reader = elem_reader
    else:
        row_reader = nested_row_reader
    return read_str_array_elems(f, row_reader, type_name, rank)


def expected_array_dims(l, rank):
    if rank > 1:
        n = len(l)
        if n == 0:
            elem = []
        else:
            elem = l[0]
        return [n] + expected_array_dims(elem, rank - 1)
    else:
        return [len(l)]


def verify_array_dims(l, dims):
    if dims[0] != len(l):
        raise ValueError
    if len(dims) > 1:
        for x in l:
            verify_array_dims(x, dims[1:])


def read_str_array(f, elem_reader, type_name, rank, bt):
    elems = read_str_array_helper(f, elem_reader, type_name, rank)
    if type(elems) == tuple:
        # Empty array
        return np.empty(elems, dtype=bt)
    else:
        dims = expected_array_dims(elems, rank)
        verify_array_dims(elems, dims)
        return np.array(elems, dtype=bt)


################################################################################

READ_BINARY_VERSION = 2

# struct format specified at
# https://docs.python.org/2/library/struct.html#format-characters


def mk_bin_scalar_reader(t):
    def bin_reader(f):
        fmt = FUTHARK_PRIMTYPES[t]["bin_format"]
        size = FUTHARK_PRIMTYPES[t]["size"]
        tf = FUTHARK_PRIMTYPES[t]["numpy_type"]
        return tf(struct.unpack("<" + fmt, f.get_chars(size))[0])

    return bin_reader


read_bin_i8 = mk_bin_scalar_reader("i8")
read_bin_i16 = mk_bin_scalar_reader("i16")
read_bin_i32 = mk_bin_scalar_reader("i32")
read_bin_i64 = mk_bin_scalar_reader("i64")

read_bin_u8 = mk_bin_scalar_reader("u8")
read_bin_u16 = mk_bin_scalar_reader("u16")
read_bin_u32 = mk_bin_scalar_reader("u32")
read_bin_u64 = mk_bin_scalar_reader("u64")

read_bin_f16 = mk_bin_scalar_reader("f16")
read_bin_f32 = mk_bin_scalar_reader("f32")
read_bin_f64 = mk_bin_scalar_reader("f64")

read_bin_bool = mk_bin_scalar_reader("bool")


def read_is_binary(f):
    skip_spaces(f)
    c = f.get_char()
    if c == b"b":
        bin_version = read_bin_u8(f)
        if bin_version != READ_BINARY_VERSION:
            panic(
                1,
                "binary-input: File uses version %i, but I only understand version %i.\n",
                bin_version,
                READ_BINARY_VERSION,
            )
        return True
    else:
        f.unget_char(c)
        return False


FUTHARK_PRIMTYPES = {
    "i8": {
        "binname": b"  i8",
        "size": 1,
        "bin_reader": read_bin_i8,
        "str_reader": read_str_i8,
        "bin_format": "b",
        "numpy_type": np.int8,
    },
    "i16": {
        "binname": b" i16",
        "size": 2,
        "bin_reader": read_bin_i16,
        "str_reader": read_str_i16,
        "bin_format": "h",
        "numpy_type": np.int16,
    },
    "i32": {
        "binname": b" i32",
        "size": 4,
        "bin_reader": read_bin_i32,
        "str_reader": read_str_i32,
        "bin_format": "i",
        "numpy_type": np.int32,
    },
    "i64": {
        "binname": b" i64",
        "size": 8,
        "bin_reader": read_bin_i64,
        "str_reader": read_str_i64,
        "bin_format": "q",
        "numpy_type": np.int64,
    },
    "u8": {
        "binname": b"  u8",
        "size": 1,
        "bin_reader": read_bin_u8,
        "str_reader": read_str_u8,
        "bin_format": "B",
        "numpy_type": np.uint8,
    },
    "u16": {
        "binname": b" u16",
        "size": 2,
        "bin_reader": read_bin_u16,
        "str_reader": read_str_u16,
        "bin_format": "H",
        "numpy_type": np.uint16,
    },
    "u32": {
        "binname": b" u32",
        "size": 4,
        "bin_reader": read_bin_u32,
        "str_reader": read_str_u32,
        "bin_format": "I",
        "numpy_type": np.uint32,
    },
    "u64": {
        "binname": b" u64",
        "size": 8,
        "bin_reader": read_bin_u64,
        "str_reader": read_str_u64,
        "bin_format": "Q",
        "numpy_type": np.uint64,
    },
    "f16": {
        "binname": b" f16",
        "size": 2,
        "bin_reader": read_bin_f16,
        "str_reader": read_str_f16,
        "bin_format": "e",
        "numpy_type": np.float16,
    },
    "f32": {
        "binname": b" f32",
        "size": 4,
        "bin_reader": read_bin_f32,
        "str_reader": read_str_f32,
        "bin_format": "f",
        "numpy_type": np.float32,
    },
    "f64": {
        "binname": b" f64",
        "size": 8,
        "bin_reader": read_bin_f64,
        "str_reader": read_str_f64,
        "bin_format": "d",
        "numpy_type": np.float64,
    },
    "bool": {
        "binname": b"bool",
        "size": 1,
        "bin_reader": read_bin_bool,
        "str_reader": read_str_bool,
        "bin_format": "b",
        "numpy_type": bool,
    },
}


def read_bin_read_type(f):
    read_binname = f.get_chars(4)

    for k, v in FUTHARK_PRIMTYPES.items():
        if v["binname"] == read_binname:
            return k
    panic(1, "binary-input: Did not recognize the type '%s'.\n", read_binname)


def numpy_type_to_type_name(t):
    for k, v in FUTHARK_PRIMTYPES.items():
        if v["numpy_type"] == t:
            return k
    raise Exception("Unknown Numpy type: {}".format(t))


def read_bin_ensure_scalar(f, expected_type):
    dims = read_bin_i8(f)

    if dims != 0:
        panic(
            1,
            "binary-input: Expected scalar (0 dimensions), but got array with %i dimensions.\n",
            dims,
        )

    bin_type = read_bin_read_type(f)
    if bin_type != expected_type:
        panic(
            1,
            "binary-input: Expected scalar of type %s but got scalar of type %s.\n",
            expected_type,
            bin_type,
        )


# ------------------------------------------------------------------------------
# General interface for reading Primitive Futhark Values
# ------------------------------------------------------------------------------


def read_scalar(f, ty):
    if read_is_binary(f):
        read_bin_ensure_scalar(f, ty)
        return FUTHARK_PRIMTYPES[ty]["bin_reader"](f)
    return FUTHARK_PRIMTYPES[ty]["str_reader"](f)


def read_array(f, expected_type, rank):
    if not read_is_binary(f):
        str_reader = FUTHARK_PRIMTYPES[expected_type]["str_reader"]
        return read_str_array(
            f,
            str_reader,
            expected_type,
            rank,
            FUTHARK_PRIMTYPES[expected_type]["numpy_type"],
        )

    bin_rank = read_bin_u8(f)

    if bin_rank != rank:
        panic(
            1,
            "binary-input: Expected %i dimensions, but got array with %i dimensions.\n",
            rank,
            bin_rank,
        )

    bin_type_enum = read_bin_read_type(f)
    if expected_type != bin_type_enum:
        panic(
            1,
            "binary-input: Expected %iD-array with element type '%s' but got %iD-array with element type '%s'.\n",
            rank,
            expected_type,
            bin_rank,
            bin_type_enum,
        )

    shape = []
    elem_count = 1
    for i in range(rank):
        bin_size = read_bin_i64(f)
        elem_count *= bin_size
        shape.append(bin_size)

    bin_fmt = FUTHARK_PRIMTYPES[bin_type_enum]["bin_format"]

    # We first read the expected number of types into a bytestring,
    # then use np.frombuffer.  This is because np.fromfile does not
    # work on things that are insufficiently file-like, like a network
    # stream.
    bytes = f.get_chars(elem_count * FUTHARK_PRIMTYPES[expected_type]["size"])
    arr = np.frombuffer(
        bytes, dtype=FUTHARK_PRIMTYPES[bin_type_enum]["numpy_type"]
    )
    arr.shape = shape

    return arr.copy()  # To ensure it is writeable.


if sys.version_info >= (3, 0):
    input_reader = ReaderInput(sys.stdin.buffer)
else:
    input_reader = ReaderInput(sys.stdin)

import re


def read_value(type_desc, reader=input_reader):
    """Read a value of the given type.  The type is a string
    representation of the Futhark type."""
    m = re.match(r"((?:\[\])*)([a-z0-9]+)$", type_desc)
    if m:
        dims = int(len(m.group(1)) / 2)
        basetype = m.group(2)
    assert m and basetype in FUTHARK_PRIMTYPES, "Unknown type: {}".format(
        type_desc
    )
    if dims > 0:
        return read_array(reader, basetype, dims)
    else:
        return read_scalar(reader, basetype)


def end_of_input(entry, f=input_reader):
    skip_spaces(f)
    if f.get_char() != b"":
        panic(1, 'Expected EOF on stdin after reading input for "%s".', entry)


def write_value_text(v, out=sys.stdout):
    if type(v) == np.uint8:
        out.write("%uu8" % v)
    elif type(v) == np.uint16:
        out.write("%uu16" % v)
    elif type(v) == np.uint32:
        out.write("%uu32" % v)
    elif type(v) == np.uint64:
        out.write("%uu64" % v)
    elif type(v) == np.int8:
        out.write("%di8" % v)
    elif type(v) == np.int16:
        out.write("%di16" % v)
    elif type(v) == np.int32:
        out.write("%di32" % v)
    elif type(v) == np.int64:
        out.write("%di64" % v)
    elif type(v) in [bool, np.bool_]:
        if v:
            out.write("true")
        else:
            out.write("false")
    elif type(v) == np.float16:
        if np.isnan(v):
            out.write("f16.nan")
        elif np.isinf(v):
            if v >= 0:
                out.write("f16.inf")
            else:
                out.write("-f16.inf")
        else:
            out.write("%.6ff16" % v)
    elif type(v) == np.float32:
        if np.isnan(v):
            out.write("f32.nan")
        elif np.isinf(v):
            if v >= 0:
                out.write("f32.inf")
            else:
                out.write("-f32.inf")
        else:
            out.write("%.6ff32" % v)
    elif type(v) == np.float64:
        if np.isnan(v):
            out.write("f64.nan")
        elif np.isinf(v):
            if v >= 0:
                out.write("f64.inf")
            else:
                out.write("-f64.inf")
        else:
            out.write("%.6ff64" % v)
    elif type(v) == np.ndarray:
        if np.prod(v.shape) == 0:
            tname = numpy_type_to_type_name(v.dtype)
            out.write(
                "empty({}{})".format(
                    "".join(["[{}]".format(d) for d in v.shape]), tname
                )
            )
        else:
            first = True
            out.write("[")
            for x in v:
                if not first:
                    out.write(", ")
                first = False
                write_value(x, out=out)
            out.write("]")
    else:
        raise Exception("Cannot print value of type {}: {}".format(type(v), v))


type_strs = {
    np.dtype("int8"): b"  i8",
    np.dtype("int16"): b" i16",
    np.dtype("int32"): b" i32",
    np.dtype("int64"): b" i64",
    np.dtype("uint8"): b"  u8",
    np.dtype("uint16"): b" u16",
    np.dtype("uint32"): b" u32",
    np.dtype("uint64"): b" u64",
    np.dtype("float16"): b" f16",
    np.dtype("float32"): b" f32",
    np.dtype("float64"): b" f64",
    np.dtype("bool"): b"bool",
}


def construct_binary_value(v):
    t = v.dtype
    shape = v.shape

    elems = 1
    for d in shape:
        elems *= d

    num_bytes = 1 + 1 + 1 + 4 + len(shape) * 8 + elems * t.itemsize
    bytes = bytearray(num_bytes)
    bytes[0] = np.int8(ord("b"))
    bytes[1] = 2
    bytes[2] = np.int8(len(shape))
    bytes[3:7] = type_strs[t]

    for i in range(len(shape)):
        bytes[7 + i * 8 : 7 + (i + 1) * 8] = np.int64(shape[i]).tobytes()

    bytes[7 + len(shape) * 8 :] = np.ascontiguousarray(v).tobytes()

    return bytes


def write_value_binary(v, out=sys.stdout):
    if sys.version_info >= (3, 0):
        out = out.buffer
    out.write(construct_binary_value(v))


def write_value(v, out=sys.stdout, binary=False):
    if binary:
        return write_value_binary(v, out=out)
    else:
        return write_value_text(v, out=out)


# End of values.py.
# Start of memory.py.

import ctypes as ct


def allocateMem(size):
    return np.empty(size, dtype=np.byte)


# Copy an array if its is not-None.  This is important for treating
# Numpy arrays as flat memory, but has some overhead.
def normaliseArray(x):
    if (x.base is x) or (x.base is None):
        return x
    else:
        return x.copy()


def unwrapArray(x):
    return x.ravel().view(np.byte)


def indexArray(x, offset, bt):
    return x.view(bt)[offset]


def writeScalarArray(x, offset, v):
    x.view(type(v))[offset] = v


# An opaque Futhark value.
class opaque(object):
    def __init__(self, desc, *payload):
        self.data = payload
        self.desc = desc

    def __repr__(self):
        return "<opaque Futhark value of type {}>".format(self.desc)


# LMAD stuff


def lmad_contiguous_search(checked, expected, strides, shape, used):
    for i in range(len(strides)):
        for j in range(len(strides)):
            if not used[j] and strides[j] == expected and strides[j] >= 0:
                used[j] = True
                if checked + 1 == len(strides) or lmad_contiguous_search(
                    checked + 1, expected * shape[j], strides, shape, used
                ):
                    return True
                used[j] = False
    return False


def lmad_contiguous(strides, shape):
    used = len(strides) * [False]
    return lmad_contiguous_search(0, 1, strides, shape, used)


def lmad_memcpyable(dst_strides, src_strides, shape):
    if not lmad_contiguous(dst_strides, shape):
        return False
    for i in range(len(dst_strides)):
        if dst_strides[i] != src_strides[i] and shape[i] != 1:
            return False
    return True


def lmad_is_tr(strides, shape):
    r = len(shape)
    for i in range(1, r):
        n = 1
        m = 1
        ok = True
        expected = 1
        # Check strides before 'i'.
        for j in range(i - 1, -1, -1):
            ok = ok and strides[j] == expected
            expected *= shape[j]
            n *= shape[j]
        # Check strides after 'i'.
        for j in range(r - 1, i - 1, -1):
            ok = ok and strides[j] == expected
            expected *= shape[j]
            m *= shape[j]
        if ok:
            return (n, m)
    return None


def lmad_map_tr(dst_strides, src_strides, shape):
    r = len(dst_strides)
    rowmajor_strides = [0] * r
    rowmajor_strides[r - 1] = 1

    for i in range(r - 2, -1, -1):
        rowmajor_strides[i] = rowmajor_strides[i + 1] * shape[i + 1]

    # map_r will be the number of mapped dimensions on top.
    map_r = 0
    k = 1
    for i in range(r):
        if (
            dst_strides[i] != rowmajor_strides[i]
            or src_strides[i] != rowmajor_strides[i]
        ):
            break
        else:
            k *= shape[i]
            map_r += 1

    if rowmajor_strides[map_r:] == dst_strides[map_r:]:
        r = lmad_is_tr(src_strides[map_r:], shape[map_r:])
        if r is not None:
            (n, m) = r
            return (k, n, m)
    elif rowmajor_strides[map_r:] == src_strides[map_r:]:
        r = lmad_is_tr(dst_strides[map_r:], shape[map_r:])
        if r is not None:
            (n, m) = r
            return (k, m, n)  # Sic!
    return None


def lmad_copy_elements(
    pt, dst, dst_offset, dst_strides, src, src_offset, src_strides, shape
):
    if len(shape) == 1:
        for i in range(shape[0]):
            writeScalarArray(
                dst,
                dst_offset + i * dst_strides[0],
                indexArray(src, src_offset + i * src_strides[0], pt),
            )
    else:
        for i in range(shape[0]):
            lmad_copy_elements(
                pt,
                dst,
                dst_offset + i * dst_strides[0],
                dst_strides[1:],
                src,
                src_offset + i * src_strides[0],
                src_strides[1:],
                shape[1:],
            )


def lmad_copy(
    pt, dst, dst_offset, dst_strides, src, src_offset, src_strides, shape
):
    if lmad_memcpyable(dst_strides, src_strides, shape):
        dst[
            dst_offset * ct.sizeof(pt) : dst_offset * ct.sizeof(pt)
            + np.prod(shape) * ct.sizeof(pt)
        ] = src[
            src_offset * ct.sizeof(pt) : src_offset * ct.sizeof(pt)
            + np.prod(shape) * ct.sizeof(pt)
        ]
    else:
        lmad_copy_elements(
            pt,
            dst,
            dst_offset,
            dst_strides,
            src,
            src_offset,
            src_strides,
            shape,
        )


# End of memory.py.
# Start of panic.py.


def panic(exitcode, fmt, *args):
    sys.stderr.write("%s: " % sys.argv[0])
    sys.stderr.write(fmt % args)
    sys.stderr.write("\n")
    sys.exit(exitcode)


# End of panic.py.
# Start of tuning.py


def read_tuning_file(kvs, f):
    for line in f.read().splitlines():
        size, value = line.split("=")
        kvs[size] = int(value)
    return kvs


# End of tuning.py.
# Start of scalar.py.

import numpy as np
import math
import struct

pi16 = np.float16(np.pi)
pi32 = np.float32(np.pi)
pi64 = np.float64(np.pi)


def intlit(t, x):
    if t == np.int8:
        return np.int8(x)
    elif t == np.int16:
        return np.int16(x)
    elif t == np.int32:
        return np.int32(x)
    else:
        return np.int64(x)


def signed(x):
    if type(x) == np.uint8:
        return np.int8(x)
    elif type(x) == np.uint16:
        return np.int16(x)
    elif type(x) == np.uint32:
        return np.int32(x)
    else:
        return np.int64(x)


def unsigned(x):
    if type(x) == np.int8:
        return np.uint8(x)
    elif type(x) == np.int16:
        return np.uint16(x)
    elif type(x) == np.int32:
        return np.uint32(x)
    else:
        return np.uint64(x)


def shlN(x, y):
    return x << y


def ashrN(x, y):
    return x >> y


# Python is so slow that we just make all the unsafe operations safe,
# always.


def sdivN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return x // y


def sdiv_upN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return (x + y - intlit(type(x), 1)) // y


def smodN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return x % y


def udivN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return signed(unsigned(x) // unsigned(y))


def udiv_upN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return signed(
            (unsigned(x) + unsigned(y) - unsigned(intlit(type(x), 1)))
            // unsigned(y)
        )


def umodN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return signed(unsigned(x) % unsigned(y))


def squotN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return np.floor_divide(np.abs(x), np.abs(y)) * np.sign(x) * np.sign(y)


def sremN(x, y):
    if y == 0:
        return intlit(type(x), 0)
    else:
        return np.remainder(np.abs(x), np.abs(y)) * np.sign(x)


def sminN(x, y):
    return min(x, y)


def smaxN(x, y):
    return max(x, y)


def uminN(x, y):
    return signed(min(unsigned(x), unsigned(y)))


def umaxN(x, y):
    return signed(max(unsigned(x), unsigned(y)))


def fminN(x, y):
    return np.fmin(x, y)


def fmaxN(x, y):
    return np.fmax(x, y)


def powN(x, y):
    return x**y


def fpowN(x, y):
    return x**y


def sleN(x, y):
    return x <= y


def sltN(x, y):
    return x < y


def uleN(x, y):
    return unsigned(x) <= unsigned(y)


def ultN(x, y):
    return unsigned(x) < unsigned(y)


def lshr8(x, y):
    return np.int8(np.uint8(x) >> np.uint8(y))


def lshr16(x, y):
    return np.int16(np.uint16(x) >> np.uint16(y))


def lshr32(x, y):
    return np.int32(np.uint32(x) >> np.uint32(y))


def lshr64(x, y):
    return np.int64(np.uint64(x) >> np.uint64(y))


def sext_T_i8(x):
    return np.int8(x)


def sext_T_i16(x):
    return np.int16(x)


def sext_T_i32(x):
    return np.int32(x)


def sext_T_i64(x):
    return np.int64(x)


def itob_T_bool(x):
    return bool(x)


def btoi_bool_i8(x):
    return np.int8(x)


def btoi_bool_i16(x):
    return np.int16(x)


def btoi_bool_i32(x):
    return np.int32(x)


def btoi_bool_i64(x):
    return np.int64(x)


def ftob_T_bool(x):
    return bool(x)


def btof_bool_f16(x):
    return np.float16(x)


def btof_bool_f32(x):
    return np.float32(x)


def btof_bool_f64(x):
    return np.float64(x)


def zext_i8_i8(x):
    return np.int8(np.uint8(x))


def zext_i8_i16(x):
    return np.int16(np.uint8(x))


def zext_i8_i32(x):
    return np.int32(np.uint8(x))


def zext_i8_i64(x):
    return np.int64(np.uint8(x))


def zext_i16_i8(x):
    return np.int8(np.uint16(x))


def zext_i16_i16(x):
    return np.int16(np.uint16(x))


def zext_i16_i32(x):
    return np.int32(np.uint16(x))


def zext_i16_i64(x):
    return np.int64(np.uint16(x))


def zext_i32_i8(x):
    return np.int8(np.uint32(x))


def zext_i32_i16(x):
    return np.int16(np.uint32(x))


def zext_i32_i32(x):
    return np.int32(np.uint32(x))


def zext_i32_i64(x):
    return np.int64(np.uint32(x))


def zext_i64_i8(x):
    return np.int8(np.uint64(x))


def zext_i64_i16(x):
    return np.int16(np.uint64(x))


def zext_i64_i32(x):
    return np.int32(np.uint64(x))


def zext_i64_i64(x):
    return np.int64(np.uint64(x))


sdiv8 = sdiv16 = sdiv32 = sdiv64 = sdivN
sdiv_up8 = sdiv1_up6 = sdiv_up32 = sdiv_up64 = sdiv_upN
sdiv_safe8 = sdiv1_safe6 = sdiv_safe32 = sdiv_safe64 = sdivN
sdiv_up_safe8 = sdiv_up1_safe6 = sdiv_up_safe32 = sdiv_up_safe64 = sdiv_upN
smod8 = smod16 = smod32 = smod64 = smodN
smod_safe8 = smod_safe16 = smod_safe32 = smod_safe64 = smodN
udiv8 = udiv16 = udiv32 = udiv64 = udivN
udiv_up8 = udiv_up16 = udiv_up32 = udiv_up64 = udivN
udiv_safe8 = udiv_safe16 = udiv_safe32 = udiv_safe64 = udiv_upN
udiv_up_safe8 = udiv_up_safe16 = udiv_up_safe32 = udiv_up_safe64 = udiv_upN
umod8 = umod16 = umod32 = umod64 = umodN
umod_safe8 = umod_safe16 = umod_safe32 = umod_safe64 = umodN
squot8 = squot16 = squot32 = squot64 = squotN
squot_safe8 = squot_safe16 = squot_safe32 = squot_safe64 = squotN
srem8 = srem16 = srem32 = srem64 = sremN
srem_safe8 = srem_safe16 = srem_safe32 = srem_safe64 = sremN

shl8 = shl16 = shl32 = shl64 = shlN
ashr8 = ashr16 = ashr32 = ashr64 = ashrN
smax8 = smax16 = smax32 = smax64 = smaxN
smin8 = smin16 = smin32 = smin64 = sminN
umax8 = umax16 = umax32 = umax64 = umaxN
umin8 = umin16 = umin32 = umin64 = uminN
pow8 = pow16 = pow32 = pow64 = powN
fpow16 = fpow32 = fpow64 = fpowN
fmax16 = fmax32 = fmax64 = fmaxN
fmin16 = fmin32 = fmin64 = fminN
sle8 = sle16 = sle32 = sle64 = sleN
slt8 = slt16 = slt32 = slt64 = sltN
ule8 = ule16 = ule32 = ule64 = uleN
ult8 = ult16 = ult32 = ult64 = ultN
sext_i8_i8 = sext_i16_i8 = sext_i32_i8 = sext_i64_i8 = sext_T_i8
sext_i8_i16 = sext_i16_i16 = sext_i32_i16 = sext_i64_i16 = sext_T_i16
sext_i8_i32 = sext_i16_i32 = sext_i32_i32 = sext_i64_i32 = sext_T_i32
sext_i8_i64 = sext_i16_i64 = sext_i32_i64 = sext_i64_i64 = sext_T_i64
itob_i8_bool = itob_i16_bool = itob_i32_bool = itob_i64_bool = itob_T_bool
ftob_f16_bool = ftob_f32_bool = ftob_f64_bool = ftob_T_bool


def clz_T(x):
    n = np.int32(0)
    bits = x.itemsize * 8
    for i in range(bits):
        if x < 0:
            break
        n += np.int32(1)
        x <<= np.int8(1)
    return n


def ctz_T(x):
    n = np.int32(0)
    bits = x.itemsize * 8
    for i in range(bits):
        if (x & 1) == 1:
            break
        n += np.int32(1)
        x >>= np.int8(1)
    return n


def popc_T(x):
    c = np.int32(0)
    while x != 0:
        x &= x - np.int8(1)
        c += np.int32(1)
    return c


futhark_popc8 = futhark_popc16 = futhark_popc32 = futhark_popc64 = popc_T
futhark_clzz8 = futhark_clzz16 = futhark_clzz32 = futhark_clzz64 = clz_T
futhark_ctzz8 = futhark_ctzz16 = futhark_ctzz32 = futhark_ctzz64 = ctz_T


def ssignum(x):
    return np.sign(x)


def usignum(x):
    if x < 0:
        return ssignum(-x)
    else:
        return ssignum(x)


def sitofp_T_f32(x):
    return np.float32(x)


sitofp_i8_f32 = sitofp_i16_f32 = sitofp_i32_f32 = sitofp_i64_f32 = sitofp_T_f32


def sitofp_T_f64(x):
    return np.float64(x)


sitofp_i8_f64 = sitofp_i16_f64 = sitofp_i32_f64 = sitofp_i64_f64 = sitofp_T_f64


def uitofp_T_f32(x):
    return np.float32(unsigned(x))


uitofp_i8_f32 = uitofp_i16_f32 = uitofp_i32_f32 = uitofp_i64_f32 = uitofp_T_f32


def uitofp_T_f64(x):
    return np.float64(unsigned(x))


uitofp_i8_f64 = uitofp_i16_f64 = uitofp_i32_f64 = uitofp_i64_f64 = uitofp_T_f64


def fptosi_T_i8(x):
    if np.isnan(x) or np.isinf(x):
        return np.int8(0)
    else:
        return np.int8(np.trunc(x))


fptosi_f16_i8 = fptosi_f32_i8 = fptosi_f64_i8 = fptosi_T_i8


def fptosi_T_i16(x):
    if np.isnan(x) or np.isinf(x):
        return np.int16(0)
    else:
        return np.int16(np.trunc(x))


fptosi_f16_i16 = fptosi_f32_i16 = fptosi_f64_i16 = fptosi_T_i16


def fptosi_T_i32(x):
    if np.isnan(x) or np.isinf(x):
        return np.int32(0)
    else:
        return np.int32(np.trunc(x))


fptosi_f16_i32 = fptosi_f32_i32 = fptosi_f64_i32 = fptosi_T_i32


def fptosi_T_i64(x):
    if np.isnan(x) or np.isinf(x):
        return np.int64(0)
    else:
        return np.int64(np.trunc(x))


fptosi_f16_i64 = fptosi_f32_i64 = fptosi_f64_i64 = fptosi_T_i64


def fptoui_T_i8(x):
    if np.isnan(x) or np.isinf(x):
        return np.int8(0)
    else:
        return np.int8(np.trunc(x))


fptoui_f16_i8 = fptoui_f32_i8 = fptoui_f64_i8 = fptoui_T_i8


def fptoui_T_i16(x):
    if np.isnan(x) or np.isinf(x):
        return np.int16(0)
    else:
        return np.int16(np.trunc(x))


fptoui_f16_i16 = fptoui_f32_i16 = fptoui_f64_i16 = fptoui_T_i16


def fptoui_T_i32(x):
    if np.isnan(x) or np.isinf(x):
        return np.int32(0)
    else:
        return np.int32(np.trunc(x))


fptoui_f16_i32 = fptoui_f32_i32 = fptoui_f64_i32 = fptoui_T_i32


def fptoui_T_i64(x):
    if np.isnan(x) or np.isinf(x):
        return np.int64(0)
    else:
        return np.int64(np.trunc(x))


fptoui_f16_i64 = fptoui_f32_i64 = fptoui_f64_i64 = fptoui_T_i64


def fpconv_f16_f32(x):
    return np.float32(x)


def fpconv_f16_f64(x):
    return np.float64(x)


def fpconv_f32_f16(x):
    return np.float16(x)


def fpconv_f32_f64(x):
    return np.float64(x)


def fpconv_f64_f16(x):
    return np.float16(x)


def fpconv_f64_f32(x):
    return np.float32(x)


def futhark_umul_hi8(a, b):
    return np.int8(
        (np.uint64(np.uint8(a)) * np.uint64(np.uint8(b))) >> np.uint64(8)
    )


def futhark_umul_hi16(a, b):
    return np.int16(
        (np.uint64(np.uint16(a)) * np.uint64(np.uint16(b))) >> np.uint64(16)
    )


def futhark_umul_hi32(a, b):
    return np.int32(
        (np.uint64(np.uint32(a)) * np.uint64(np.uint32(b))) >> np.uint64(32)
    )


def futhark_umul_hi64(a, b):
    return np.int64(np.uint64(int(np.uint64(a)) * int(np.uint64(b)) >> 64))


def futhark_smul_hi8(a, b):
    return np.int8((np.int64(a) * np.int64(b)) >> np.int64(8))


def futhark_smul_hi16(a, b):
    return np.int16((np.int64(a) * np.int64(b)) >> np.int64(16))


def futhark_smul_hi32(a, b):
    return np.int32((np.int64(a) * np.int64(b)) >> np.int64(32))


def futhark_smul_hi64(a, b):
    return np.int64(int(a) * int(b) >> 64)


def futhark_umad_hi8(a, b, c):
    return futhark_umul_hi8(a, b) + c


def futhark_umad_hi16(a, b, c):
    return futhark_umul_hi16(a, b) + c


def futhark_umad_hi32(a, b, c):
    return futhark_umul_hi32(a, b) + c


def futhark_umad_hi64(a, b, c):
    return futhark_umul_hi64(a, b) + c


def futhark_smad_hi8(a, b, c):
    return futhark_smul_hi8(a, b) + c


def futhark_smad_hi16(a, b, c):
    return futhark_smul_hi16(a, b) + c


def futhark_smad_hi32(a, b, c):
    return futhark_smul_hi32(a, b) + c


def futhark_smad_hi64(a, b, c):
    return futhark_smul_hi64(a, b) + c


def futhark_log64(x):
    return np.float64(np.log(x))


def futhark_log2_64(x):
    return np.float64(np.log2(x))


def futhark_log10_64(x):
    return np.float64(np.log10(x))


def futhark_log1p_64(x):
    return np.float64(np.log1p(x))


def futhark_sqrt64(x):
    return np.sqrt(x)


def futhark_rsqrt64(x):
    return 1 / np.sqrt(x)


def futhark_cbrt64(x):
    return np.cbrt(x)


def futhark_exp64(x):
    return np.exp(x)


def futhark_cos64(x):
    return np.cos(x)


def futhark_cospi64(x):
    return np.cos(pi64 * x)


def futhark_sin64(x):
    return np.sin(x)


def futhark_sinpi64(x):
    return np.sin(pi64 * x)


def futhark_tan64(x):
    return np.tan(x)


def futhark_tanpi64(x):
    return np.tan(pi64 * x)


def futhark_acos64(x):
    return np.arccos(x)


def futhark_acospi64(x):
    return np.arccos(x) / pi64


def futhark_asin64(x):
    return np.arcsin(x)


def futhark_asinpi64(x):
    return np.arcsin(x) / pi64


def futhark_atan64(x):
    return np.arctan(x)


def futhark_atanpi64(x):
    return np.arctan(x) / pi64


def futhark_cosh64(x):
    return np.cosh(x)


def futhark_sinh64(x):
    return np.sinh(x)


def futhark_tanh64(x):
    return np.tanh(x)


def futhark_acosh64(x):
    return np.arccosh(x)


def futhark_asinh64(x):
    return np.arcsinh(x)


def futhark_atanh64(x):
    return np.arctanh(x)


def futhark_atan2_64(x, y):
    return np.arctan2(x, y)


def futhark_atan2pi_64(x, y):
    return np.arctan2(x, y) / pi64


def futhark_hypot64(x, y):
    return np.hypot(x, y)


def futhark_gamma64(x):
    return np.float64(math.gamma(x))


def futhark_lgamma64(x):
    return np.float64(math.lgamma(x))


def futhark_erf64(x):
    return np.float64(math.erf(x))


def futhark_erfc64(x):
    return np.float64(math.erfc(x))


def futhark_round64(x):
    return np.round(x)


def futhark_ceil64(x):
    return np.ceil(x)


def futhark_floor64(x):
    return np.floor(x)


def futhark_nextafter64(x, y):
    return np.nextafter(x, y)


def futhark_isnan64(x):
    return np.isnan(x)


def futhark_isinf64(x):
    return np.isinf(x)


def fptobits_f64_i64(x):
    return x.view(np.int64)


def bitstofp_i64_f64(x):
    return x.view(np.float64)


def futhark_log32(x):
    return np.float32(np.log(x))


def futhark_log2_32(x):
    return np.float32(np.log2(x))


def futhark_log10_32(x):
    return np.float32(np.log10(x))


def futhark_log1p_32(x):
    return np.float32(np.log1p(x))


def futhark_sqrt32(x):
    return np.float32(np.sqrt(x))


def futhark_rsqrt32(x):
    return np.float32(1 / np.sqrt(x))


def futhark_cbrt32(x):
    return np.float32(np.cbrt(x))


def futhark_exp32(x):
    return np.exp(x)


def futhark_cos32(x):
    return np.cos(x)


def futhark_cospi32(x):
    return np.cos(pi32 * x)


def futhark_sin32(x):
    return np.sin(x)


def futhark_sinpi32(x):
    return np.sin(pi32 * x)


def futhark_tan32(x):
    return np.tan(x)


def futhark_tanpi32(x):
    return np.tan(pi32 * x)


def futhark_acos32(x):
    return np.arccos(x)


def futhark_acospi32(x):
    return np.arccos(x) / pi32


def futhark_asin32(x):
    return np.arcsin(x)


def futhark_asinpi32(x):
    return np.arcsin(x) / pi32


def futhark_atan32(x):
    return np.arctan(x)


def futhark_atanpi32(x):
    return np.arctan(x) / pi32


def futhark_cosh32(x):
    return np.cosh(x)


def futhark_sinh32(x):
    return np.sinh(x)


def futhark_tanh32(x):
    return np.tanh(x)


def futhark_acosh32(x):
    return np.arccosh(x)


def futhark_asinh32(x):
    return np.arcsinh(x)


def futhark_atanh32(x):
    return np.arctanh(x)


def futhark_atan2_32(x, y):
    return np.arctan2(x, y)


def futhark_atan2pi_32(x, y):
    return np.arctan2(x, y) / pi32


def futhark_hypot32(x, y):
    return np.hypot(x, y)


def futhark_gamma32(x):
    return np.float32(math.gamma(x))


def futhark_lgamma32(x):
    return np.float32(math.lgamma(x))


def futhark_erf32(x):
    return np.float32(math.erf(x))


def futhark_erfc32(x):
    return np.float32(math.erfc(x))


def futhark_round32(x):
    return np.round(x)


def futhark_ceil32(x):
    return np.ceil(x)


def futhark_floor32(x):
    return np.floor(x)


def futhark_nextafter32(x, y):
    return np.nextafter(x, y)


def futhark_isnan32(x):
    return np.isnan(x)


def futhark_isinf32(x):
    return np.isinf(x)


def fptobits_f32_i32(x):
    return x.view(np.int32)


def bitstofp_i32_f32(x):
    return x.view(np.float32)


def futhark_log16(x):
    return np.float16(np.log(x))


def futhark_log2_16(x):
    return np.float16(np.log2(x))


def futhark_log10_16(x):
    return np.float16(np.log10(x))


def futhark_log1p_16(x):
    return np.float16(np.log1p(x))


def futhark_sqrt16(x):
    return np.float16(np.sqrt(x))


def futhark_rsqrt16(x):
    return np.float16(1 / np.sqrt(x))


def futhark_cbrt16(x):
    return np.float16(np.cbrt(x))


def futhark_exp16(x):
    return np.exp(x)


def futhark_cos16(x):
    return np.cos(x)


def futhark_cospi16(x):
    return np.cos(pi16 * x)


def futhark_sin16(x):
    return np.sin(x)


def futhark_sinpi16(x):
    return np.sin(pi16 * x)


def futhark_tan16(x):
    return np.tan(x)


def futhark_tanpi16(x):
    return np.tan(pi16 * x)


def futhark_acos16(x):
    return np.arccos(x)


def futhark_acospi16(x):
    return np.arccos(x) / pi16


def futhark_asin16(x):
    return np.arcsin(x)


def futhark_asinpi16(x):
    return np.arcsin(x) / pi16


def futhark_atan16(x):
    return np.arctan(x)


def futhark_atanpi16(x):
    return np.arctan(x) / pi16


def futhark_cosh16(x):
    return np.cosh(x)


def futhark_sinh16(x):
    return np.sinh(x)


def futhark_tanh16(x):
    return np.tanh(x)


def futhark_acosh16(x):
    return np.arccosh(x)


def futhark_asinh16(x):
    return np.arcsinh(x)


def futhark_atanh16(x):
    return np.arctanh(x)


def futhark_atan2_16(x, y):
    return np.arctan2(x, y)


def futhark_atan2pi_16(x, y):
    return np.arctan2(x, y) / pi16


def futhark_hypot16(x, y):
    return np.hypot(x, y)


def futhark_gamma16(x):
    return np.float16(math.gamma(x))


def futhark_lgamma16(x):
    return np.float16(math.lgamma(x))


def futhark_erf16(x):
    return np.float16(math.erf(x))


def futhark_erfc16(x):
    return np.float16(math.erfc(x))


def futhark_round16(x):
    return np.round(x)


def futhark_ceil16(x):
    return np.ceil(x)


def futhark_floor16(x):
    return np.floor(x)


def futhark_nextafter16(x, y):
    return np.nextafter(x, y)


def futhark_isnan16(x):
    return np.isnan(x)


def futhark_isinf16(x):
    return np.isinf(x)


def fptobits_f16_i16(x):
    return x.view(np.int16)


def bitstofp_i16_f16(x):
    return x.view(np.float16)


def futhark_lerp16(v0, v1, t):
    return v0 + (v1 - v0) * t


def futhark_lerp32(v0, v1, t):
    return v0 + (v1 - v0) * t


def futhark_lerp64(v0, v1, t):
    return v0 + (v1 - v0) * t


def futhark_ldexp16(x, y):
    return np.ldexp(x, y)


def futhark_ldexp32(x, y):
    return np.ldexp(x, y)


def futhark_ldexp64(x, y):
    return np.ldexp(x, y)


def futhark_mad16(a, b, c):
    return a * b + c


def futhark_mad32(a, b, c):
    return a * b + c


def futhark_mad64(a, b, c):
    return a * b + c


def futhark_fma16(a, b, c):
    return a * b + c


def futhark_fma32(a, b, c):
    return a * b + c


def futhark_fma64(a, b, c):
    return a * b + c


futhark_copysign16 = futhark_copysign32 = futhark_copysign64 = np.copysign


def futhark_cond(x, y, z):
    return y if x else z


futhark_cond_f16 = futhark_cond_f32 = futhark_cond_f64 = futhark_cond
futhark_cond_i18 = futhark_cond_i16 = futhark_cond_i32 = futhark_cond_i64 = (
    futhark_cond
)
futhark_cond_bool = futhark_cond_unit = futhark_cond


# End of scalar.py.
# Start of server.py

import sys
import time
import shlex  # For string splitting


class Server:
    def __init__(self, ctx):
        self._ctx = ctx
        self._vars = {}

    class Failure(BaseException):
        def __init__(self, msg):
            self.msg = msg

    def _get_arg(self, args, i):
        if i < len(args):
            return args[i]
        else:
            raise self.Failure("Insufficient command args")

    def _get_entry_point(self, entry):
        if entry in self._ctx.entry_points:
            return self._ctx.entry_points[entry]
        else:
            raise self.Failure("Unknown entry point: %s" % entry)

    def _check_var(self, vname):
        if not vname in self._vars:
            raise self.Failure("Unknown variable: %s" % vname)

    def _check_new_var(self, vname):
        if vname in self._vars:
            raise self.Failure("Variable already exists: %s" % vname)

    def _get_var(self, vname):
        self._check_var(vname)
        return self._vars[vname]

    def _cmd_inputs(self, args):
        entry = self._get_arg(args, 0)
        for t in self._get_entry_point(entry)[1]:
            print(t)

    def _cmd_outputs(self, args):
        entry = self._get_arg(args, 0)
        for t in self._get_entry_point(entry)[2]:
            print(t)

    def _cmd_dummy(self, args):
        pass

    def _cmd_free(self, args):
        for vname in args:
            self._check_var(vname)
            del self._vars[vname]

    def _cmd_rename(self, args):
        oldname = self._get_arg(args, 0)
        newname = self._get_arg(args, 1)
        self._check_var(oldname)
        self._check_new_var(newname)
        self._vars[newname] = self._vars[oldname]
        del self._vars[oldname]

    def _cmd_call(self, args):
        entry = self._get_entry_point(self._get_arg(args, 0))
        entry_fname = entry[0]
        num_ins = len(entry[1])
        num_outs = len(entry[2])
        exp_len = 1 + num_outs + num_ins

        if len(args) != exp_len:
            raise self.Failure("Invalid argument count, expected %d" % exp_len)

        out_vnames = args[1 : num_outs + 1]

        for out_vname in out_vnames:
            self._check_new_var(out_vname)

        in_vnames = args[1 + num_outs :]
        ins = [self._get_var(in_vname) for in_vname in in_vnames]

        try:
            (runtime, vals) = getattr(self._ctx, entry_fname)(*ins)
        except Exception as e:
            raise self.Failure(str(e))

        print("runtime: %d" % runtime)

        if num_outs == 1:
            self._vars[out_vnames[0]] = vals
        else:
            for out_vname, val in zip(out_vnames, vals):
                self._vars[out_vname] = val

    def _store_val(self, f, value):
        # In case we are using the PyOpenCL backend, we first
        # need to convert OpenCL arrays to ordinary NumPy
        # arrays.  We do this in a nasty way.
        if isinstance(value, opaque):
            for component in value.data:
                self._store_val(f, component)
        elif (
            isinstance(value, np.number)
            or isinstance(value, bool)
            or isinstance(value, np.bool_)
            or isinstance(value, np.ndarray)
        ):
            # Ordinary NumPy value.
            f.write(construct_binary_value(value))
        else:
            # Assuming PyOpenCL array.
            f.write(construct_binary_value(value.get()))

    def _cmd_store(self, args):
        fname = self._get_arg(args, 0)

        with open(fname, "wb") as f:
            for i in range(1, len(args)):
                self._store_val(f, self._get_var(args[i]))

    def _restore_val(self, reader, typename):
        if typename in self._ctx.opaques:
            vs = []
            for t in self._ctx.opaques[typename]:
                vs += [read_value(t, reader)]
            return opaque(typename, *vs)
        else:
            return read_value(typename, reader)

    def _cmd_restore(self, args):
        if len(args) % 2 == 0:
            raise self.Failure("Invalid argument count")

        fname = args[0]
        args = args[1:]

        with open(fname, "rb") as f:
            reader = ReaderInput(f)
            while args != []:
                vname = args[0]
                typename = args[1]
                args = args[2:]

                if vname in self._vars:
                    raise self.Failure("Variable already exists: %s" % vname)

                try:
                    self._vars[vname] = self._restore_val(reader, typename)
                except ValueError:
                    raise self.Failure(
                        "Failed to restore variable %s.\n"
                        "Possibly malformed data in %s.\n" % (vname, fname)
                    )

            skip_spaces(reader)
            if reader.get_char() != b"":
                raise self.Failure("Expected EOF after reading values")

    def _cmd_types(self, args):
        for k in self._ctx.opaques.keys():
            print(k)

    def _cmd_entry_points(self, args):
        for k in self._ctx.entry_points.keys():
            print(k)

    _commands = {
        "inputs": _cmd_inputs,
        "outputs": _cmd_outputs,
        "call": _cmd_call,
        "restore": _cmd_restore,
        "store": _cmd_store,
        "free": _cmd_free,
        "rename": _cmd_rename,
        "clear": _cmd_dummy,
        "pause_profiling": _cmd_dummy,
        "unpause_profiling": _cmd_dummy,
        "report": _cmd_dummy,
        "types": _cmd_types,
        "entry_points": _cmd_entry_points,
    }

    def _process_line(self, line):
        lex = shlex.shlex(line)
        lex.quotes = '"'
        lex.whitespace_split = True
        lex.commenters = ""
        words = list(lex)
        if words == []:
            raise self.Failure("Empty line")
        else:
            cmd = words[0]
            args = words[1:]
            if cmd in self._commands:
                self._commands[cmd](self, args)
            else:
                raise self.Failure("Unknown command: %s" % cmd)

    def run(self):
        while True:
            print("%%% OK", flush=True)
            line = sys.stdin.readline()
            if line == "":
                return
            try:
                self._process_line(line)
            except self.Failure as e:
                print("%%% FAILURE")
                print(e.msg)


# End of server.py
class sample_kernel:
  entry_points = {"matmul_bias_relu_sum": ("matmul_bias_relu_sum", ["[][]f64", "[][]f64", "[]f64"], ["f64"]), "sum_squares": ("sum_squares", ["[]f64"], ["f64"])}
  opaques = {}
  def __init__(self, build_options=build_options, command_queue=None, interactive=False, platform_pref=preferred_platform, device_pref=preferred_device, default_group_size=default_group_size, default_num_groups=default_num_groups, default_tile_size=default_tile_size, default_reg_tile_size=default_reg_tile_size, default_threshold=default_threshold, sizes=sizes):
    size_heuristics=[("NVIDIA CUDA", cl.device_type.GPU, "lockstep_width", lambda device: np.int32(32)), ("AMD Accelerated Parallel Processing", cl.device_type.GPU, "lockstep_width", lambda device: np.int32(32)), ("rusticl", cl.device_type.GPU, "lockstep_width", lambda device: np.int32(32)), ("", cl.device_type.GPU, "lockstep_width", lambda device: np.int32(1)), ("", cl.device_type.GPU, "num_groups", lambda device: (np.int32(4) * device.get_info(getattr(cl.device_info, "MAX_COMPUTE_UNITS")))), ("", cl.device_type.GPU, "group_size", lambda device: np.int32(256)), ("", cl.device_type.GPU, "tile_size", lambda device: np.int32(32)), ("", cl.device_type.GPU, "reg_tile_size", lambda device: np.int32(2)), ("", cl.device_type.GPU, "threshold", lambda device: np.int32(32768)), ("", cl.device_type.CPU, "lockstep_width", lambda device: np.int32(1)), ("", cl.device_type.CPU, "num_groups", lambda device: device.get_info(getattr(cl.device_info, "MAX_COMPUTE_UNITS"))), ("", cl.device_type.CPU, "group_size", lambda device: np.int32(32)), ("", cl.device_type.CPU, "tile_size", lambda device: np.int32(4)), ("", cl.device_type.CPU, "reg_tile_size", lambda device: np.int32(1)), ("", cl.device_type.CPU, "threshold", lambda device: device.get_info(getattr(cl.device_info, "MAX_COMPUTE_UNITS")))]
    self.global_failure_args_max = 0
    self.failure_msgs=[]
    constants = [("sum_squareszisegred_nonseg_6524_dim1", lambda : self.sizes["sum_squares.segred_tblock_size_6516"]), ("sum_squareszisegred_nonseg_6524zisegred_tblock_sizze_6517", lambda : self.sizes["sum_squares.segred_tblock_size_6516"]), ("sum_squareszisegred_nonseg_6524zichunk_sizze_7689", lambda : np.int64(1)), ("matmul_bias_relu_sumzigpuseq_8028_dim1", lambda : np.int64(1)), ("matmul_bias_relu_sumzisegred_nonseg_6998_dim1", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6852"]), ("matmul_bias_relu_sumzisegred_nonseg_6998zisegred_tblock_sizze_6992", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6852"]), ("matmul_bias_relu_sumzisegred_nonseg_6998zichunk_sizze_7994", lambda : np.int64(1)), ("matmul_bias_relu_sumzisegred_large_6984_dim1", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"]), ("matmul_bias_relu_sumzisegred_large_6984zisegred_tblock_sizze_6978", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"]), ("matmul_bias_relu_sumzisegred_large_6984zichunk_sizze_7920", lambda : np.int64(1)), ("matmul_bias_relu_sumzisegred_small_6984_dim1", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"]), ("matmul_bias_relu_sumzisegred_small_6984zisegred_tblock_sizze_6978", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"]), ("matmul_bias_relu_sumzisegmap_6969_dim1", lambda : self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6709"]), ("matmul_bias_relu_sumzisegmap_6969zisegmap_tblock_sizze_6964", lambda : self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6709"]), ("matmul_bias_relu_sumzisegred_large_6953_dim1", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"]), ("matmul_bias_relu_sumzisegred_large_6953zisegred_tblock_sizze_6946", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"]), ("matmul_bias_relu_sumzisegred_large_6953zichunk_sizze_7832", lambda : np.int64(1)), ("matmul_bias_relu_sumzisegred_small_6953_dim1", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"]), ("matmul_bias_relu_sumzisegred_small_6953zisegred_tblock_sizze_6946", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"]), ("matmul_bias_relu_sumzisegmap_intrablock_7025_dim1", lambda : (self.sizes["matmul_bias_relu_sum.Ty_7006"] * self.sizes["matmul_bias_relu_sum.Ty_7006"])), ("matmul_bias_relu_sumzisegmap_intrablock_7025ziTy_7008", lambda : self.sizes["matmul_bias_relu_sum.Ty_7006"]), ("matmul_bias_relu_sumzisegmap_intrablock_7025ziRy_7009", lambda : self.sizes["matmul_bias_relu_sum.Ry_7007"]), ("matmul_bias_relu_sumzisegmap_intrablock_7025ziTk_7010", lambda : self.sizes["matmul_bias_relu_sum.Tk_7005"]), ("matmul_bias_relu_sumzisegmap_intrablock_7025zitk_div_tx_7011", lambda : sdiv_up64(self.sizes["matmul_bias_relu_sum.Tk_7005"], self.sizes["matmul_bias_relu_sum.Ty_7006"])), ("matmul_bias_relu_sumzisegmap_intrablock_7025ziTxRx_7013", lambda : (self.sizes["matmul_bias_relu_sum.Ty_7006"] * self.sizes["matmul_bias_relu_sum.Ry_7007"])), ("matmul_bias_relu_sumzisegmap_intrablock_7025zia_loc_szz_7016", lambda : (self.sizes["matmul_bias_relu_sum.Tk_7005"] * (self.sizes["matmul_bias_relu_sum.Ty_7006"] * self.sizes["matmul_bias_relu_sum.Ry_7007"]))), ("matmul_bias_relu_sumzisegmap_intrablock_7025ziloop_nonempty_7448", lambda : slt64(np.int64(0), self.sizes["matmul_bias_relu_sum.Ry_7007"])), ("matmul_bias_relu_sumzisegmap_intrablock_7025zibytes_7524", lambda : (np.int64(8) * (self.sizes["matmul_bias_relu_sum.Tk_7005"] * (self.sizes["matmul_bias_relu_sum.Ty_7006"] * self.sizes["matmul_bias_relu_sum.Ry_7007"])))), ("matmul_bias_relu_sumzisegmap_6876_dim1", lambda : self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6580"]), ("matmul_bias_relu_sumzisegmap_6876zisegmap_tblock_sizze_6872", lambda : self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6580"]), ("matmul_bias_relu_sumzigpuseq_7748_dim1", lambda : np.int64(1)), ("matmul_bias_relu_sumzisegred_nonseg_6544_dim1", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6528"]), ("matmul_bias_relu_sumzisegred_nonseg_6544zisegred_tblock_sizze_6538", lambda : self.sizes["matmul_bias_relu_sum.segred_tblock_size_6528"]), ("matmul_bias_relu_sumzisegred_nonseg_6544zichunk_sizze_7692", lambda : np.int64(1))]
    program = initialise_opencl_object(self,
                                       program_src=fut_opencl_src,
                                       build_options=build_options,
                                       command_queue=command_queue,
                                       interactive=interactive,
                                       platform_pref=platform_pref,
                                       device_pref=device_pref,
                                       default_group_size=default_group_size,
                                       default_num_groups=default_num_groups,
                                       default_tile_size=default_tile_size,
                                       default_reg_tile_size=default_reg_tile_size,
                                       default_threshold=default_threshold,
                                       size_heuristics=size_heuristics,
                                       required_types=["i32", "i64", "f64", "bool", "unit"],
                                       user_sizes=sizes,
                                       all_sizes={"builtin#replicate_i32.tblock_size_7704": {"class": "thread_block_size", "value": None}, "matmul_bias_relu_sum.Ry_7007": {"class": "reg_tile_size", "value": None}, "matmul_bias_relu_sum.Tk_7005": {"class": "tile_size", "value": None}, "matmul_bias_relu_sum.Ty_7006": {"class": "tile_size", "value": None}, "matmul_bias_relu_sum.segmap_tblock_size_6580": {"class": "thread_block_size", "value": None}, "matmul_bias_relu_sum.segmap_tblock_size_6709": {"class": "thread_block_size", "value": None}, "matmul_bias_relu_sum.segred_num_tblocks_6530": {"class": "grid_size", "value": None}, "matmul_bias_relu_sum.segred_num_tblocks_6661": {"class": "grid_size", "value": None}, "matmul_bias_relu_sum.segred_num_tblocks_6727": {"class": "grid_size", "value": None}, "matmul_bias_relu_sum.segred_num_tblocks_6854": {"class": "grid_size", "value": None}, "matmul_bias_relu_sum.segred_tblock_size_6528": {"class": "thread_block_size", "value": None}, "matmul_bias_relu_sum.segred_tblock_size_6659": {"class": "thread_block_size", "value": None}, "matmul_bias_relu_sum.segred_tblock_size_6725": {"class": "thread_block_size", "value": None}, "matmul_bias_relu_sum.segred_tblock_size_6852": {"class": "thread_block_size", "value": None}, "matmul_bias_relu_sum.suff_intra_par_2": {"class": "threshold(32, !matmul_bias_relu_sum.suff_outer_par_1 !matmul_bias_relu_sum.suff_outer_screma_0)", "value": 32}, "matmul_bias_relu_sum.suff_outer_par_1": {"class": "threshold(def, !matmul_bias_relu_sum.suff_outer_screma_0)", "value": None}, "matmul_bias_relu_sum.suff_outer_par_3": {"class": "threshold(def, !matmul_bias_relu_sum.suff_outer_par_1 !matmul_bias_relu_sum.suff_intra_par_2 !matmul_bias_relu_sum.suff_outer_screma_0)", "value": None}, "matmul_bias_relu_sum.suff_outer_screma_0": {"class": "threshold(def, )", "value": None}, "sum_squares.segred_num_tblocks_6518": {"class": "grid_size", "value": None}, "sum_squares.segred_tblock_size_6516": {"class": "thread_block_size", "value": None}},
                                       constants=constants)
    self.builtinzhreplicate_i32zireplicate_7700_var = program.builtinzhreplicate_i32zireplicate_7700
    self.matmul_bias_relu_sumzigpuseq_7748_var = program.matmul_bias_relu_sumzigpuseq_7748
    self.matmul_bias_relu_sumzigpuseq_8028_var = program.matmul_bias_relu_sumzigpuseq_8028
    self.matmul_bias_relu_sumzisegmap_6876_var = program.matmul_bias_relu_sumzisegmap_6876
    self.matmul_bias_relu_sumzisegmap_6969_var = program.matmul_bias_relu_sumzisegmap_6969
    self.matmul_bias_relu_sumzisegmap_intrablock_6897_var = program.matmul_bias_relu_sumzisegmap_intrablock_6897
    self.matmul_bias_relu_sumzisegmap_intrablock_7025_var = program.matmul_bias_relu_sumzisegmap_intrablock_7025
    self.matmul_bias_relu_sumzisegred_large_6953_var = program.matmul_bias_relu_sumzisegred_large_6953
    self.matmul_bias_relu_sumzisegred_large_6984_var = program.matmul_bias_relu_sumzisegred_large_6984
    self.matmul_bias_relu_sumzisegred_nonseg_6544_var = program.matmul_bias_relu_sumzisegred_nonseg_6544
    self.matmul_bias_relu_sumzisegred_nonseg_6998_var = program.matmul_bias_relu_sumzisegred_nonseg_6998
    self.matmul_bias_relu_sumzisegred_small_6953_var = program.matmul_bias_relu_sumzisegred_small_6953
    self.matmul_bias_relu_sumzisegred_small_6984_var = program.matmul_bias_relu_sumzisegred_small_6984
    self.sum_squareszisegred_nonseg_6524_var = program.sum_squareszisegred_nonseg_6524
    self.constants = {}
    self.constants["counters_mem_7690"] = opencl_alloc(self, np.int64(80), "self.constants[\"counters_mem_7690\"]")
    self.futhark_builtinzhreplicate_i32(self.constants["counters_mem_7690"], np.int64(20), np.int32(0))
    self.constants["counters_mem_7693"] = opencl_alloc(self, np.int64(80), "self.constants[\"counters_mem_7693\"]")
    self.futhark_builtinzhreplicate_i32(self.constants["counters_mem_7693"], np.int64(20), np.int32(0))
    self.constants["counters_mem_7869"] = opencl_alloc(self, np.int64(81920), "self.constants[\"counters_mem_7869\"]")
    self.futhark_builtinzhreplicate_i32(self.constants["counters_mem_7869"], np.int64(20480), np.int32(0))
    self.constants["counters_mem_7955"] = opencl_alloc(self, np.int64(81920), "self.constants[\"counters_mem_7955\"]")
    self.futhark_builtinzhreplicate_i32(self.constants["counters_mem_7955"], np.int64(20480), np.int32(0))
    self.constants["counters_mem_7995"] = opencl_alloc(self, np.int64(80), "self.constants[\"counters_mem_7995\"]")
    self.futhark_builtinzhreplicate_i32(self.constants["counters_mem_7995"], np.int64(20), np.int32(0))
  def futhark_builtinzhreplicate_i32(self, mem_7695, num_elems_7696, val_7697):
    replicate_n_7699 = num_elems_7696
    tblock_sizze_7704 = self.sizes["builtin#replicate_i32.tblock_size_7704"]
    virt_num_tblocks_7705 = sdiv_up64(replicate_n_7699, tblock_sizze_7704)
    num_tblocks_7706 = smin64(virt_num_tblocks_7705, np.int64(1048576))
    if ((1 * (np.int64(num_tblocks_7706) * np.int64(tblock_sizze_7704))) != 0):
      self.builtinzhreplicate_i32zireplicate_7700_var.set_args(cl.LocalMemory(max(np.int64(0), 1)), ct.c_int64(num_elems_7696), ct.c_int32(val_7697), ct.c_int64(replicate_n_7699), ct.c_int64(virt_num_tblocks_7705), ct.c_int64(num_tblocks_7706), mem_7695)
      cl.enqueue_nd_range_kernel(self.queue, self.builtinzhreplicate_i32zireplicate_7700_var, ((np.int64(num_tblocks_7706) * np.int64(tblock_sizze_7704)),), (np.int64(tblock_sizze_7704),))
      if synchronous:
        sync(self)
    return ()
  def futhark_entry_matmul_bias_relu_sum(self, a_mem_7471, b_mem_7472, bias_mem_7473, n_6137, k_6138, m_6139):
    suff_outer_screma_6526 = (self.sizes["matmul_bias_relu_sum.suff_outer_screma_0"] <= n_6137)
    suff_outer_par_6862 = (self.sizes["matmul_bias_relu_sum.suff_outer_par_1"] <= n_6137)
    max_tblock_sizze_6865 = self.max_thread_block_size
    fits_6866 = sle64(m_6139, max_tblock_sizze_6865)
    suff_intra_par_6868 = (self.sizes["matmul_bias_relu_sum.suff_intra_par_2"] <= m_6139)
    intra_suff_and_fits_6869 = (fits_6866 and suff_intra_par_6868)
    comparatee_6923 = (n_6137 * m_6139)
    suff_outer_par_6924 = (self.sizes["matmul_bias_relu_sum.suff_outer_par_3"] <= comparatee_6923)
    nest_sizze_6945 = (k_6138 * comparatee_6923)
    segred_tblock_sizze_6946 = self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"]
    max_num_tblocks_7688 = self.sizes["matmul_bias_relu_sum.segred_num_tblocks_6727"]
    num_tblocks_6947 = sext_i64_i32(smax64(np.int64(1), smin64(sdiv_up64(nest_sizze_6945, segred_tblock_sizze_6946), max_num_tblocks_7688)))
    segmap_tblock_sizze_6964 = self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6709"]
    segred_tblock_sizze_6978 = self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"]
    max_num_tblocks_7689 = self.sizes["matmul_bias_relu_sum.segred_num_tblocks_6661"]
    num_tblocks_6979 = sext_i64_i32(smax64(np.int64(1), smin64(sdiv_up64(comparatee_6923, segred_tblock_sizze_6978), max_num_tblocks_7689)))
    segmap_tblock_sizze_6872 = self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6580"]
    segred_tblock_sizze_6992 = self.sizes["matmul_bias_relu_sum.segred_tblock_size_6852"]
    max_num_tblocks_7690 = self.sizes["matmul_bias_relu_sum.segred_num_tblocks_6854"]
    num_tblocks_6993 = sext_i64_i32(smax64(np.int64(1), smin64(sdiv_up64(n_6137, segred_tblock_sizze_6992), max_num_tblocks_7690)))
    segred_tblock_sizze_6538 = self.sizes["matmul_bias_relu_sum.segred_tblock_size_6528"]
    max_num_tblocks_7691 = self.sizes["matmul_bias_relu_sum.segred_num_tblocks_6530"]
    num_tblocks_6539 = sext_i64_i32(smax64(np.int64(1), smin64(sdiv_up64(n_6137, segred_tblock_sizze_6538), max_num_tblocks_7691)))
    Ty_7008 = self.sizes["matmul_bias_relu_sum.Ty_7006"]
    Ry_7009 = self.sizes["matmul_bias_relu_sum.Ry_7007"]
    Tk_7010 = self.sizes["matmul_bias_relu_sum.Tk_7005"]
    TxRx_7013 = (Ty_7008 * Ry_7009)
    a_loc_szz_7016 = (Tk_7010 * TxRx_7013)
    tblock_sizze_7022 = (Ty_7008 * Ty_7008)
    loop_nonempty_7448 = slt64(np.int64(0), Ry_7009)
    binop_y_7474 = (k_6138 - np.int64(1))
    binop_x_7476 = smax64(np.int64(0), binop_y_7474)
    binop_y_7477 = (m_6139 - np.int64(1))
    binop_x_7478 = smax64(np.int64(0), binop_y_7477)
    binop_y_7479 = (k_6138 * binop_x_7478)
    binop_y_7480 = smax64(np.int64(0), binop_y_7479)
    binop_y_7481 = (binop_x_7476 + binop_y_7480)
    binop_y_7482 = (np.int64(1) + binop_y_7481)
    bytes_7483 = (np.int64(8) * binop_y_7482)
    binop_x_7486 = (np.int64(8) * n_6137)
    bytes_7487 = (m_6139 * binop_x_7486)
    binop_y_7512 = (Ry_7009 - np.int64(1))
    binop_x_7513 = smax64(np.int64(0), binop_y_7512)
    binop_y_7514 = (Ry_7009 * binop_x_7513)
    binop_y_7515 = smax64(np.int64(0), binop_y_7514)
    binop_y_7520 = (binop_x_7513 + binop_y_7515)
    binop_y_7521 = (np.int64(1) + binop_y_7520)
    bytes_7522 = (np.int64(8) * binop_y_7521)
    bytes_7524 = (np.int64(8) * a_loc_szz_7016)
    binop_y_7626 = (n_6137 - np.int64(1))
    binop_x_7628 = smax64(np.int64(0), binop_y_7626)
    binop_y_7631 = (n_6137 * binop_x_7476)
    binop_y_7632 = smax64(np.int64(0), binop_y_7631)
    binop_y_7633 = (binop_x_7628 + binop_y_7632)
    binop_y_7634 = (np.int64(1) + binop_y_7633)
    bytes_7635 = (np.int64(8) * binop_y_7634)
    shared_memory_capacity_8034 = self.max_shared_memory
    if (suff_outer_screma_6526 and (sle64(((sdiv_up64((np.int64(8) * segred_tblock_sizze_6538), np.int64(8)) * np.int64(8)) + np.int64(8)), shared_memory_capacity_8034) and sle64(np.int64(0), shared_memory_capacity_8034))):
      mem_7665 = opencl_alloc(self, bytes_7635, "mem_7665")
      lmad_copy_gpu2gpu(self, ct.c_double, mem_7665, np.int64(0), [np.int64(1), n_6137], a_mem_7471, np.int64(0), [k_6138, np.int64(1)], [n_6137, k_6138])
      mem_7676 = opencl_alloc(self, bytes_7483, "mem_7676")
      lmad_copy_gpu2gpu(self, ct.c_double, mem_7676, np.int64(0), [np.int64(1), k_6138], b_mem_7472, np.int64(0), [m_6139, np.int64(1)], [k_6138, m_6139])
      mem_7678 = opencl_alloc(self, np.int64(8), "mem_7678")
      chunk_sizze_7692 = np.int64(1)
      segred_tmp_mem_7715 = opencl_alloc(self, (np.int64(8) * num_tblocks_6539), "segred_tmp_mem_7715")
      num_threads_7717 = (num_tblocks_6539 * segred_tblock_sizze_6538)
      if ((1 * (np.int64(num_tblocks_6539) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6528"])) != 0):
        self.matmul_bias_relu_sumzisegred_nonseg_6544_var.set_args(cl.LocalMemory(max((np.int32(8) + ((np.int64(8) * segred_tblock_sizze_6538) + srem64((np.int64(8) - srem64((np.int64(8) * segred_tblock_sizze_6538), np.int64(8))), np.int64(8)))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(k_6138), ct.c_int64(m_6139), ct.c_int64(num_tblocks_6539), ct.c_int64(num_threads_7717), bias_mem_7473, mem_7665, mem_7676, mem_7678, self.constants["counters_mem_7693"], segred_tmp_mem_7715)
        cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegred_nonseg_6544_var, ((np.int64(num_tblocks_6539) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6528"]),), (self.sizes["matmul_bias_relu_sum.segred_tblock_size_6528"],))
        if synchronous:
          sync(self)
      mem_7665 = None
      mem_7676 = None
      mem_7679 = opencl_alloc(self, np.int64(8), "mem_7679")
      if ((1 * (np.int64(np.int64(1)) * np.int64(1))) != 0):
        self.matmul_bias_relu_sumzigpuseq_7748_var.set_args(cl.LocalMemory(max(np.int64(0), 1)), self.global_failure, mem_7678, mem_7679)
        cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzigpuseq_7748_var, ((np.int64(np.int64(1)) * np.int64(1)),), (np.int64(1),))
        if synchronous:
          sync(self)
      mem_7678 = None
      ext_mem_7680 = mem_7679
    else:
      shared_memory_capacity_7993 = self.max_shared_memory
      if (suff_outer_par_6862 and sle64(np.int64(0), shared_memory_capacity_7993)):
        segmap_usable_groups_6873 = sdiv_up64(n_6137, segmap_tblock_sizze_6872)
        mem_7636 = opencl_alloc(self, bytes_7635, "mem_7636")
        lmad_copy_gpu2gpu(self, ct.c_double, mem_7636, np.int64(0), [np.int64(1), n_6137], a_mem_7471, np.int64(0), [k_6138, np.int64(1)], [n_6137, k_6138])
        mem_7647 = opencl_alloc(self, bytes_7483, "mem_7647")
        lmad_copy_gpu2gpu(self, ct.c_double, mem_7647, np.int64(0), [np.int64(1), k_6138], b_mem_7472, np.int64(0), [m_6139, np.int64(1)], [k_6138, m_6139])
        mem_7650 = opencl_alloc(self, binop_x_7486, "mem_7650")
        virt_num_tblocks_7754 = sext_i64_i32(sdiv_up64(n_6137, segmap_tblock_sizze_6872))
        if ((1 * (np.int64(segmap_usable_groups_6873) * self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6580"])) != 0):
          self.matmul_bias_relu_sumzisegmap_6876_var.set_args(cl.LocalMemory(max(np.int64(0), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(k_6138), ct.c_int64(m_6139), bias_mem_7473, mem_7636, mem_7647, mem_7650)
          cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegmap_6876_var, ((np.int64(segmap_usable_groups_6873) * self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6580"]),), (self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6580"],))
          if synchronous:
            sync(self)
        mem_7636 = None
        mem_7647 = None
        ext_mem_7651 = mem_7650
      else:
        shared_memory_capacity_7992 = self.max_shared_memory
        if (intra_suff_and_fits_6869 and sle64((sdiv_up64((np.int64(8) * m_6139), np.int64(8)) * np.int64(8)), shared_memory_capacity_7992)):
          mem_7624 = opencl_alloc(self, binop_x_7486, "mem_7624")
          num_chunks_7765 = sext_i64_i32(sdiv_up64(m_6139, m_6139))
          virt_num_tblocks_7766 = sext_i64_i32(n_6137)
          if ((1 * (np.int64(n_6137) * np.int64(m_6139))) != 0):
            self.matmul_bias_relu_sumzisegmap_intrablock_6897_var.set_args(cl.LocalMemory(max(((np.int64(8) * m_6139) + srem64((np.int64(8) - srem64((np.int64(8) * m_6139), np.int64(8))), np.int64(8))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(k_6138), ct.c_int64(m_6139), a_mem_7471, b_mem_7472, bias_mem_7473, mem_7624)
            cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegmap_intrablock_6897_var, ((np.int64(n_6137) * np.int64(m_6139)),), (np.int64(m_6139),))
            if synchronous:
              sync(self)
          ext_mem_7625 = mem_7624
        else:
          shared_memory_capacity_7919 = self.max_shared_memory
          if (suff_outer_par_6924 and sle64(((sdiv_up64(bytes_7524, np.int64(8)) * np.int64(8)) + (sdiv_up64(bytes_7524, np.int64(8)) * np.int64(8))), shared_memory_capacity_7919)):
            tk_div_tx_7011 = sdiv_up64(Tk_7010, Ty_7008)
            gridDim_x_7019 = sdiv_up64(m_6139, TxRx_7013)
            gridDim_y_7020 = sdiv_up64(n_6137, TxRx_7013)
            grid_sizze_7021 = (gridDim_x_7019 * gridDim_y_7020)
            full_tiles_7053 = squot64(k_6138, Tk_7010)
            kk_7221 = (Tk_7010 * full_tiles_7053)
            mem_7617 = opencl_alloc(self, bytes_7487, "mem_7617")
            num_chunks_7782 = sext_i64_i32(sdiv_up64((Ty_7008 * Ty_7008), tblock_sizze_7022))
            virt_num_tblocks_7783 = sext_i64_i32((gridDim_y_7020 * gridDim_x_7019))
            if ((1 * (np.int64(grid_sizze_7021) * (self.sizes["matmul_bias_relu_sum.Ty_7006"] * self.sizes["matmul_bias_relu_sum.Ty_7006"]))) != 0):
              self.matmul_bias_relu_sumzisegmap_intrablock_7025_var.set_args(cl.LocalMemory(max(((bytes_7524 + srem64((np.int64(8) - srem64(bytes_7524, np.int64(8))), np.int64(8))) + (bytes_7524 + srem64((np.int64(8) - srem64(bytes_7524, np.int64(8))), np.int64(8)))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(k_6138), ct.c_int64(m_6139), ct.c_int64(gridDim_x_7019), ct.c_int64(gridDim_y_7020), ct.c_int64(full_tiles_7053), ct.c_int64(kk_7221), a_mem_7471, b_mem_7472, bias_mem_7473, mem_7617)
              cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegmap_intrablock_7025_var, ((np.int64(grid_sizze_7021) * (self.sizes["matmul_bias_relu_sum.Ty_7006"] * self.sizes["matmul_bias_relu_sum.Ty_7006"])),), ((self.sizes["matmul_bias_relu_sum.Ty_7006"] * self.sizes["matmul_bias_relu_sum.Ty_7006"]),))
              if synchronous:
                sync(self)
            ext_mem_7618 = mem_7617
          else:
            mem_7484 = opencl_alloc(self, bytes_7483, "mem_7484")
            lmad_copy_gpu2gpu(self, ct.c_double, mem_7484, np.int64(0), [np.int64(1), k_6138], b_mem_7472, np.int64(0), [m_6139, np.int64(1)], [k_6138, m_6139])
            mem_7492 = opencl_alloc(self, bytes_7487, "mem_7492")
            chunk_sizze_7832 = np.int64(1)
            if slt64((k_6138 * np.int64(2)), (segred_tblock_sizze_6946 * chunk_sizze_7832)):
              segment_sizze_nonzzero_7833 = smax64(np.int64(1), k_6138)
              num_threads_7834 = (num_tblocks_6947 * segred_tblock_sizze_6946)
              if ((1 * (np.int64(num_tblocks_6947) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"])) != 0):
                self.matmul_bias_relu_sumzisegred_small_6953_var.set_args(cl.LocalMemory(max(((np.int64(8) * segred_tblock_sizze_6946) + srem64((np.int64(8) - srem64((np.int64(8) * segred_tblock_sizze_6946), np.int64(8))), np.int64(8))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(k_6138), ct.c_int64(m_6139), ct.c_int64(num_tblocks_6947), ct.c_int64(segment_sizze_nonzzero_7833), a_mem_7471, mem_7484, mem_7492)
                cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegred_small_6953_var, ((np.int64(num_tblocks_6947) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"]),), (self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"],))
                if synchronous:
                  sync(self)
            else:
              blocks_per_segment_7863 = sdiv_up64(num_tblocks_6947, smax64(np.int64(1), (n_6137 * m_6139)))
              q_7864 = sdiv_up64(k_6138, ((segred_tblock_sizze_6946 * blocks_per_segment_7863) * chunk_sizze_7832))
              num_virtblocks_7865 = (blocks_per_segment_7863 * (n_6137 * m_6139))
              threads_per_segment_7866 = (blocks_per_segment_7863 * segred_tblock_sizze_6946)
              segred_tmp_mem_7867 = opencl_alloc(self, (np.int64(8) * num_virtblocks_7865), "segred_tmp_mem_7867")
              if ((1 * (np.int64(num_tblocks_6947) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"])) != 0):
                self.matmul_bias_relu_sumzisegred_large_6953_var.set_args(cl.LocalMemory(max((np.int32(8) + ((np.int64(8) * segred_tblock_sizze_6946) + srem64((np.int64(8) - srem64((np.int64(8) * segred_tblock_sizze_6946), np.int64(8))), np.int64(8)))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(k_6138), ct.c_int64(m_6139), ct.c_int64(num_tblocks_6947), ct.c_int64(blocks_per_segment_7863), ct.c_int64(q_7864), ct.c_int64(num_virtblocks_7865), ct.c_int64(threads_per_segment_7866), a_mem_7471, mem_7484, mem_7492, segred_tmp_mem_7867, self.constants["counters_mem_7869"])
                cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegred_large_6953_var, ((np.int64(num_tblocks_6947) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"]),), (self.sizes["matmul_bias_relu_sum.segred_tblock_size_6725"],))
                if synchronous:
                  sync(self)
            mem_7484 = None
            segmap_usable_groups_6965 = sdiv_up64(comparatee_6923, segmap_tblock_sizze_6964)
            virt_num_tblocks_7908 = sext_i64_i32(sdiv_up64((n_6137 * m_6139), segmap_tblock_sizze_6964))
            if ((1 * (np.int64(segmap_usable_groups_6965) * self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6709"])) != 0):
              self.matmul_bias_relu_sumzisegmap_6969_var.set_args(cl.LocalMemory(max(np.int64(0), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(m_6139), bias_mem_7473, mem_7492)
              cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegmap_6969_var, ((np.int64(segmap_usable_groups_6965) * self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6709"]),), (self.sizes["matmul_bias_relu_sum.segmap_tblock_size_6709"],))
              if synchronous:
                sync(self)
            ext_mem_7618 = mem_7492
          mem_7621 = opencl_alloc(self, binop_x_7486, "mem_7621")
          chunk_sizze_7920 = np.int64(1)
          if slt64((m_6139 * np.int64(2)), (segred_tblock_sizze_6978 * chunk_sizze_7920)):
            segment_sizze_nonzzero_7921 = smax64(np.int64(1), m_6139)
            num_threads_7922 = (num_tblocks_6979 * segred_tblock_sizze_6978)
            if ((1 * (np.int64(num_tblocks_6979) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"])) != 0):
              self.matmul_bias_relu_sumzisegred_small_6984_var.set_args(cl.LocalMemory(max(((np.int64(8) * segred_tblock_sizze_6978) + srem64((np.int64(8) - srem64((np.int64(8) * segred_tblock_sizze_6978), np.int64(8))), np.int64(8))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(m_6139), ct.c_int64(num_tblocks_6979), ct.c_int64(segment_sizze_nonzzero_7921), ext_mem_7618, mem_7621)
              cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegred_small_6984_var, ((np.int64(num_tblocks_6979) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"]),), (self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"],))
              if synchronous:
                sync(self)
          else:
            blocks_per_segment_7949 = sdiv_up64(num_tblocks_6979, smax64(np.int64(1), n_6137))
            q_7950 = sdiv_up64(m_6139, ((segred_tblock_sizze_6978 * blocks_per_segment_7949) * chunk_sizze_7920))
            num_virtblocks_7951 = (blocks_per_segment_7949 * n_6137)
            threads_per_segment_7952 = (blocks_per_segment_7949 * segred_tblock_sizze_6978)
            segred_tmp_mem_7953 = opencl_alloc(self, (np.int64(8) * num_virtblocks_7951), "segred_tmp_mem_7953")
            if ((1 * (np.int64(num_tblocks_6979) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"])) != 0):
              self.matmul_bias_relu_sumzisegred_large_6984_var.set_args(cl.LocalMemory(max((np.int32(8) + ((np.int64(8) * segred_tblock_sizze_6978) + srem64((np.int64(8) - srem64((np.int64(8) * segred_tblock_sizze_6978), np.int64(8))), np.int64(8)))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(m_6139), ct.c_int64(num_tblocks_6979), ct.c_int64(blocks_per_segment_7949), ct.c_int64(q_7950), ct.c_int64(num_virtblocks_7951), ct.c_int64(threads_per_segment_7952), ext_mem_7618, mem_7621, segred_tmp_mem_7953, self.constants["counters_mem_7955"])
              cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegred_large_6984_var, ((np.int64(num_tblocks_6979) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"]),), (self.sizes["matmul_bias_relu_sum.segred_tblock_size_6659"],))
              if synchronous:
                sync(self)
          ext_mem_7618 = None
          ext_mem_7625 = mem_7621
        ext_mem_7651 = ext_mem_7625
      mem_7653 = opencl_alloc(self, np.int64(8), "mem_7653")
      chunk_sizze_7994 = np.int64(1)
      segred_tmp_mem_7997 = opencl_alloc(self, (np.int64(8) * num_tblocks_6993), "segred_tmp_mem_7997")
      num_threads_7999 = (num_tblocks_6993 * segred_tblock_sizze_6992)
      if ((1 * (np.int64(num_tblocks_6993) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6852"])) != 0):
        self.matmul_bias_relu_sumzisegred_nonseg_6998_var.set_args(cl.LocalMemory(max((np.int32(8) + ((np.int64(8) * segred_tblock_sizze_6992) + srem64((np.int64(8) - srem64((np.int64(8) * segred_tblock_sizze_6992), np.int64(8))), np.int64(8)))), 1)), self.global_failure, ct.c_int64(n_6137), ct.c_int64(num_tblocks_6993), ct.c_int64(num_threads_7999), ext_mem_7651, mem_7653, self.constants["counters_mem_7995"], segred_tmp_mem_7997)
        cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzisegred_nonseg_6998_var, ((np.int64(num_tblocks_6993) * self.sizes["matmul_bias_relu_sum.segred_tblock_size_6852"]),), (self.sizes["matmul_bias_relu_sum.segred_tblock_size_6852"],))
        if synchronous:
          sync(self)
      ext_mem_7651 = None
      mem_7654 = opencl_alloc(self, np.int64(8), "mem_7654")
      if ((1 * (np.int64(np.int64(1)) * np.int64(1))) != 0):
        self.matmul_bias_relu_sumzigpuseq_8028_var.set_args(cl.LocalMemory(max(np.int64(0), 1)), self.global_failure, mem_7653, mem_7654)
        cl.enqueue_nd_range_kernel(self.queue, self.matmul_bias_relu_sumzigpuseq_8028_var, ((np.int64(np.int64(1)) * np.int64(1)),), (np.int64(1),))
        if synchronous:
          sync(self)
      mem_7653 = None
      ext_mem_7680 = mem_7654
    read_res_8035 = np.empty(1, dtype=ct.c_double)
    cl.enqueue_copy(self.queue, read_res_8035, ext_mem_7680, src_offset=(np.int64(np.int64(0)) * 8), is_blocking=synchronous)
    sync(self)
    defunc_0_reduce_res_6514 = read_res_8035[0]
    ext_mem_7680 = None
    prim_out_7687 = defunc_0_reduce_res_6514
    return prim_out_7687
  def futhark_entry_sum_squares(self, xs_mem_7471, n_5766):
    segred_tblock_sizze_6517 = self.sizes["sum_squares.segred_tblock_size_6516"]
    max_num_tblocks_7688 = self.sizes["sum_squares.segred_num_tblocks_6518"]
    num_tblocks_6519 = sext_i64_i32(smax64(np.int64(1), smin64(sdiv_up64(n_5766, segred_tblock_sizze_6517), max_num_tblocks_7688)))
    mem_7473 = opencl_alloc(self, np.int64(8), "mem_7473")
    chunk_sizze_7689 = np.int64(1)
    segred_tmp_mem_7712 = opencl_alloc(self, (np.int64(8) * num_tblocks_6519), "segred_tmp_mem_7712")
    num_threads_7714 = (num_tblocks_6519 * segred_tblock_sizze_6517)
    if ((1 * (np.int64(num_tblocks_6519) * self.sizes["sum_squares.segred_tblock_size_6516"])) != 0):
      self.sum_squareszisegred_nonseg_6524_var.set_args(cl.LocalMemory(max((np.int32(8) + ((np.int64(8) * segred_tblock_sizze_6517) + srem64((np.int64(8) - srem64((np.int64(8) * segred_tblock_sizze_6517), np.int64(8))), np.int64(8)))), 1)), self.global_failure, ct.c_int64(n_5766), ct.c_int64(num_tblocks_6519), ct.c_int64(num_threads_7714), xs_mem_7471, mem_7473, self.constants["counters_mem_7690"], segred_tmp_mem_7712)
      cl.enqueue_nd_range_kernel(self.queue, self.sum_squareszisegred_nonseg_6524_var, ((np.int64(num_tblocks_6519) * self.sizes["sum_squares.segred_tblock_size_6516"]),), (self.sizes["sum_squares.segred_tblock_size_6516"],))
      if synchronous:
        sync(self)
    read_res_8036 = np.empty(1, dtype=ct.c_double)
    cl.enqueue_copy(self.queue, read_res_8036, mem_7473, src_offset=(np.int64(np.int64(0)) * 8), is_blocking=synchronous)
    sync(self)
    defunc_0_reduce_res_6408 = read_res_8036[0]
    mem_7473 = None
    prim_out_7687 = defunc_0_reduce_res_6408
    return prim_out_7687
  def matmul_bias_relu_sum(self, a_mem_7471_ext, b_mem_7472_ext, bias_mem_7473_ext):
    n_6137 = None
    k_6138 = None
    k_6138 = None
    m_6139 = None
    m_6139 = None
    try:
      assert ((type(a_mem_7471_ext) in [np.ndarray, cl.array.Array]) and (a_mem_7471_ext.dtype == np.float64)), "Parameter has unexpected type"
      if (n_6137 == None):
        n_6137 = np.int64(a_mem_7471_ext.shape[0])
      else:
        assert (n_6137 == a_mem_7471_ext.shape[0]), "Error: entry point arguments have invalid sizes."
      if (k_6138 == None):
        k_6138 = np.int64(a_mem_7471_ext.shape[1])
      else:
        assert (k_6138 == a_mem_7471_ext.shape[1]), "Error: entry point arguments have invalid sizes."
      if (type(a_mem_7471_ext) == cl.array.Array):
        a_mem_7471 = a_mem_7471_ext.data
      else:
        a_mem_7471 = opencl_alloc(self, np.int64(a_mem_7471_ext.nbytes), "a_mem_7471")
        if (np.int64(a_mem_7471_ext.nbytes) != 0):
          cl.enqueue_copy(self.queue, a_mem_7471, normaliseArray(a_mem_7471_ext), is_blocking=synchronous)
    except (TypeError, AssertionError) as e:
      raise TypeError("Argument #0 has invalid value\nFuthark type: {}\nArgument has Python type {} and value: {}\n".format("[][]f64", type(a_mem_7471_ext), a_mem_7471_ext))
    try:
      assert ((type(b_mem_7472_ext) in [np.ndarray, cl.array.Array]) and (b_mem_7472_ext.dtype == np.float64)), "Parameter has unexpected type"
      if (k_6138 == None):
        k_6138 = np.int64(b_mem_7472_ext.shape[0])
      else:
        assert (k_6138 == b_mem_7472_ext.shape[0]), "Error: entry point arguments have invalid sizes."
      if (m_6139 == None):
        m_6139 = np.int64(b_mem_7472_ext.shape[1])
      else:
        assert (m_6139 == b_mem_7472_ext.shape[1]), "Error: entry point arguments have invalid sizes."
      if (type(b_mem_7472_ext) == cl.array.Array):
        b_mem_7472 = b_mem_7472_ext.data
      else:
        b_mem_7472 = opencl_alloc(self, np.int64(b_mem_7472_ext.nbytes), "b_mem_7472")
        if (np.int64(b_mem_7472_ext.nbytes) != 0):
          cl.enqueue_copy(self.queue, b_mem_7472, normaliseArray(b_mem_7472_ext), is_blocking=synchronous)
    except (TypeError, AssertionError) as e:
      raise TypeError("Argument #1 has invalid value\nFuthark type: {}\nArgument has Python type {} and value: {}\n".format("[][]f64", type(b_mem_7472_ext), b_mem_7472_ext))
    try:
      assert ((type(bias_mem_7473_ext) in [np.ndarray, cl.array.Array]) and (bias_mem_7473_ext.dtype == np.float64)), "Parameter has unexpected type"
      if (m_6139 == None):
        m_6139 = np.int64(bias_mem_7473_ext.shape[0])
      else:
        assert (m_6139 == bias_mem_7473_ext.shape[0]), "Error: entry point arguments have invalid sizes."
      if (type(bias_mem_7473_ext) == cl.array.Array):
        bias_mem_7473 = bias_mem_7473_ext.data
      else:
        bias_mem_7473 = opencl_alloc(self, np.int64(bias_mem_7473_ext.nbytes), "bias_mem_7473")
        if (np.int64(bias_mem_7473_ext.nbytes) != 0):
          cl.enqueue_copy(self.queue, bias_mem_7473, normaliseArray(bias_mem_7473_ext), is_blocking=synchronous)
    except (TypeError, AssertionError) as e:
      raise TypeError("Argument #2 has invalid value\nFuthark type: {}\nArgument has Python type {} and value: {}\n".format("[]f64", type(bias_mem_7473_ext), bias_mem_7473_ext))
    time_start = time.time()
    with np.errstate(divide="ignore", over="ignore", under="ignore", invalid="ignore"):
      prim_out_7687 = self.futhark_entry_matmul_bias_relu_sum(a_mem_7471, b_mem_7472, bias_mem_7473, n_6137, k_6138, m_6139)
    runtime = (int((time.time() * 1000000)) - int((time_start * 1000000)))
    sync(self)
    return np.float64(prim_out_7687)
  def sum_squares(self, xs_mem_7471_ext):
    n_5766 = None
    try:
      assert ((type(xs_mem_7471_ext) in [np.ndarray, cl.array.Array]) and (xs_mem_7471_ext.dtype == np.float64)), "Parameter has unexpected type"
      if (n_5766 == None):
        n_5766 = np.int64(xs_mem_7471_ext.shape[0])
      else:
        assert (n_5766 == xs_mem_7471_ext.shape[0]), "Error: entry point arguments have invalid sizes."
      if (type(xs_mem_7471_ext) == cl.array.Array):
        xs_mem_7471 = xs_mem_7471_ext.data
      else:
        xs_mem_7471 = opencl_alloc(self, np.int64(xs_mem_7471_ext.nbytes), "xs_mem_7471")
        if (np.int64(xs_mem_7471_ext.nbytes) != 0):
          cl.enqueue_copy(self.queue, xs_mem_7471, normaliseArray(xs_mem_7471_ext), is_blocking=synchronous)
    except (TypeError, AssertionError) as e:
      raise TypeError("Argument #0 has invalid value\nFuthark type: {}\nArgument has Python type {} and value: {}\n".format("[]f64", type(xs_mem_7471_ext), xs_mem_7471_ext))
    time_start = time.time()
    with np.errstate(divide="ignore", over="ignore", under="ignore", invalid="ignore"):
      prim_out_7687 = self.futhark_entry_sum_squares(xs_mem_7471, n_5766)
    runtime = (int((time.time() * 1000000)) - int((time_start * 1000000)))
    sync(self)
    return np.float64(prim_out_7687)